[[{"l":"MoAI Platform Documents","p":["MoAI(Moreh AI appliance for AI accelerators) Platform is a scalable AI platform that enables easy control of thousands of Graphics Processing Units(GPUs) essential for developing large-scale deep learning models.","Virtual GPU Clusters- Access and utilize virtualized GPU clusters to scale your computational resources seamlessly.","Fine-Tuning- Fine-tune with a few commands and deploy your fine-tuned model for inference.","Advanced Parallelism- Employ advanced parallelism techniques effortlessly to optimize model training."]},{"l":"Getting Started","p":["Get started with fine-tuning MoAI Platform Beginner's Guide for Finetuning","AP Guide Advanced Parallelization (AP) Feature Instructions","Moreh Toolkit Guide Command Line Usage","MoAI Platform Features The virtualization and parallelization features of the MoAI Platform"]},{"i":"what-is-moai-platform","l":"What is MoAI Platform?","p":["MoAI(Moreh AI appliance for AI accelerators) Platform is a scalable AI platform that enables easy control of thousands of Graphics Processing Units(GPUs) essential for developing large-scale deep learning models."]},{"l":"Core Technologies of MoAI Platform","p":["As deep learning models evolve, they become increasingly complex and require substantial computational resources, with parameters expanding from billions to trillions. Developing large-scale models involves managing and processing an immense number of parameters, which is very challenging and time-consuming.","The MoAI Platform's automatic parallelization addresses these challenges by simultaneously processing multiple tasks, determining the optimal calculation method for large models. This allows users to focus solely on their core AI challenges, regardless of their application scale or processor type. Furthermore, it efficiently utilizes GPU computational resources at a reasonable cost by allocating them only during calculation execution.","Various Accelerators, Multi-GPU Support","GPU Virtualization","Dynamic GPU Allocation","AI Compiler Automatic Parallelization"]}],[{"l":"MoAI Platform Overview"},{"i":"what-is-moai-platform","l":"What is MoAI Platform?","p":["The MoAI Platform is a scalable AI platform designed for developing large-scale deep learning models. It allows easy control of thousands of different GPUs to facilitate AI model training and inference."]},{"l":"Core Technologies of MoAI Platform","p":["As deep learning models evolve, they become increasingly complex and require substantial computational resources, with parameters expanding from billions to trillions. Developing large-scale models involves managing and processing an immense number of parameters, which is very challenging and time-consuming.","Additionally, issues such as GPU node failures, memory limitations, and bottlenecks frequently arise during the training and inference of large-scale models, making the resolution of these problems particularly challenging. The MoAI Platform addresses these limitations and challenges with its GPU virtualization and automatic parallelization features, providing an efficient infrastructure for the era of large-scale AI.","Various Accelerators, Multi-GPU Support","GPU Virtualization","Dynamic GPU Allocation","AI Compiler Automatic Parallelization"]},{"i":"1-various-accelerators-multi-gpu-support","l":"1. Various Accelerators, Multi-GPU Support","p":["MoAI Platform supports various accelerators capable of executing various types of operations regardless of types of GPUs.","Users can utilize different accelerators like Intel, AMD, and others alongside NVIDIA without needing to modify their code for deep learning development and model training.","This compatibility allows for flexible development and training of deep learning models, accommodating multiple and diverse types of GPUs."]},{"l":"2. GPU Virtualization","p":["The MoAI Platform’s virtualization feature enables the use of thousands of GPUs as if they were a single GPU. This simplifies the modeling and optimization processes, providing AI engineers with a smooth and efficient experience.","Flexible Scalability: You can scale GPU resources up or down as needed, enhancing the scalability of your services.","Simplified Management and Deployment: By abstracting the complexity of using multiple GPUs, the platform makes it easier to manage and deploy resources to improve performance in deep learning tasks.","Cost Savings: GPU infrastructure managers can reduce hardware costs by efficiently utilizing virtualized GPUs.","The MoAI Platform is designed with a fully usage-based billing model, where charges are applied per minute of GPU usage only during actual computations. This approach allows for significant cost savings compared to traditional cloud services, which often tie GPUs to specific virtual machines(VMs) using a passthrough method.","Comparison Examples:","Google Cloud: Google Cloud charges on a per-second basis for each machine. However, resources are fixed, making it difficult to change virtual machines once selected.","Azure: Azure charges for computing capacity on a per-second basis and allows for adjustments in consumption as needed, but does not offer flexible instance changes based on training requirements.","MoAI Platform: The MoAI Platform offers precise billing based on AI accelerator size and GPU computation time, providing more efficient cost management."]},{"l":"3. Dynamic GPU Allocation","p":["On the MoAI Platform, AI engineers can start deep learning training and inference with only the necessary amount of GPU resources.","Efficient GPU Resource Utilization: GPU resources are allocated only during computation, ensuring efficient use. This approach helps reduce software and infrastructure development costs and shortens development and deployment times.","Easy Cluster Setup: With the MoAI Platform's dynamic allocation feature, AI engineers can easily set up GPU clusters. Typically, deep learning developers need to configure their development environment by connecting PyTorch or TensorFlow to the back nodes of GPU cluster devices, ensuring that each process can communicate with others."]},{"l":"4. AI Compiler Automatic Parallelization","p":["Deep learning models consist of multiple layers, each containing numerous computations. These layers can be trained independently, allowing for parallel processing. GPU automatic parallelization is a technology that automates the parallelization of training and inference in deep learning models.","The MoAI Platform's Advanced Parallelization technology optimally distributes and parallelizes these computations, maximizing hardware resource utilization.","In the era of artificial intelligence, large-scale models like Large Language Models (LLMs) and Large Multimodal Models (LMMs) require substantial GPU clusters and effective GPU parallelization for training and inference.","Currently, common AI frameworks used with NVIDIA require AI engineers to manually adjust parallelization based on the model's size, complexity, and the available GPU size or cluster. This process is time-consuming, often taking several weeks.","The MoAI Platform provides Advanced Parallelization through the Moreh AI compiler, which optimally utilizes GPU resources based on the specific AI model and GPU cluster size.","With MoAI Platform’s Advanced Parallelization, the setup and deployment time for AI models, typically weeks long, can be dramatically reduced to approximately 2-3 days."]}],[{"l":"MoAI Platform Features","p":["The MoAI Platform handles hundreds of GPUs as a single accelerator through virtualization and reduces the burden of managing complex GPU clusters through parallelization.","Parallelization","GPU Virtualization"]}],[{"l":"GPU Virtualization","p":["MoAI Platform의 가상화 기능을 통해 사용자는 수백 개의 GPU를 단일 가속기처럼 사용할 수 있습니다.","이를 통해 수백 대의 GPU에 해당하는 계산 리소스를 활용하면서도 복잡한 GPU 클러스터 관리의 부담을 줄일 수 있습니다.","moreh-smi 명령어를 통해 현재 환경에서 사용 가능한 가상화된 가속기를 확인할 수 있습니다.","예를 들어, ** moreh-smi** 의 출력 결과는 사용자가 마치 2048GB의 메모리를 가진 단일 가속기(MoAI Accelerator)를 사용하는 것처럼 보입니다. MoAI Platform은 수십에서 수백 개의 물리적 GPU로 이루어진 클러스터를 별도의 작업 없이 단일 가속기처럼 사용할 수 있도록 가상화해줍니다.","MoAI Platform에서 PyTorch나 TensorFlow 같은 딥러닝 프레임워크를 사용하는 경우에도 사용자는 단일 가속기를 사용하는 것처럼 작업할 수 있습니다. 사용자는 PyTorch의 cuda() 같은 API를 그대로 사용하여 가상화된 가속기를 활용할 수 있습니다.","예를 들어 위와 같이, PyTorch에서 기존 cuda API를 사용하여 현재 사용 가능한 가속기 수를 확인하면, 1이라는 결과가 나옵니다.","여기서 알아두셔야 할 것은, 실제 유저가 사용하는 노드(front node)에는 물리적인 GPU가 없다는 사실입니다. 사용자가 PyTorch와 같은 딥러닝 프레임워크에서 .cuda() 같은 API를 통해 GPU 가속기를 사용하려고 하면 MoAI Platform은 백노드(backnode)에 있는 GPU 클러스터 자원을 자동으로 할당합니다.","딥러닝 프레임워크를 통해 본격적인 모델 학습 및 추론을 진행할 때, MoAI Platform은 모델 병렬화와 같은 기법을 자동으로 적용하여 사용자가 GPU 클러스터를 하나의 가속기로 간주하여 사용할 수 있도록 합니다.","아래 그림은 사용자가 MoAI Accelerator를 사용할 때 사용 중인 노드와 실제로 할당된 GPU 클러스터 간의 관계를 보여줍니다.","MoAI Accelerator를 구성하는 물리적인 GPU의 사용 현황은 moreh-smi -p 명령어를 통해 확인할 수 있습니다."]}],[{"l":"Automatic Parallelization","p":["MoAI Platform에서 사용자는 가상의 하나의 GPU만 사용하게 됩니다. 따라서 사용자는 하나의 GPU를 사용하는 코드를 작성하게 됩니다. 그렇다면 다수의 GPU를 어떠한 방식으로 사용하게 될까요?","사용자가 다수의 GPU를 사용하는 flavor를 선택하게 된다면 MoAI Platform은 자동화된 데이터 병렬화를 제공합니다. 예를 들어 사용자가 8 device를 포함하는 flavor를 선택한다면, 전체 batch size를 8등분하여 모든 device에 나누고, 이를 동시에 처리하여 훨씬 빠른 학습 속도를 보일 수 있습니다.","예시를 들어서 만약 사용자가 llama3-8b 모델을 fine-tuning할 때, gpu 4개 사용하는 flavor를 선택하고, batch size를 16로 설정한다면 각 gpu당 4개로 아래와 같은 throughput이 나올 것입니다.","그리고 만약 사용자가 gpu 16개를 사용하는 flavor를 선택하고 batchsize를 64로 설정한다면, 각 gpu당 동일하게 4개로 자동으로 병렬처리가 되어 throughput은 아래와 같이 gpu 4개에 비해 약 4배가 될 것 입니다.","사용자가 다수의 GPU를 사용하는 목적이 큰 메모리를 사용하기 때문일 경우 는 어떨까요? MoAI Platform은 모델 병렬화와 최적화를 자동으로 지원합니다.","사용자가 Llama3-8b 모델을 GPU 16개로 batch size 512를 설정하여 돌린다면, 모델 병렬화와 데이터 병렬화가 동시에 이루어져 학습이 진행됩니다.","그 외에도 70B처럼 큰 모델 또한 자동으로 병렬화가 이루어져 간단하게 학습이 이루어집니다. 이처럼 MoAI Platform은 사용자가 사용하는 모델, 배치 크기 등에 따라 자동적으로 최적화와 병렬화를 제공 함으로써, 다중 GPU를 편리하고 효율적으로 사용할 수 있도록 합니다."]}],[{"l":"Fine-tuning Tutorials","p":["This tutorial is for anyone who wants to fine-tune powerful large language models such as Llama2, Mistral, and etc for their own projects. We will walk through the steps to finetune these large language models (LLMs) with MoAI Platform.","Llama2","Llama3 8B","Mistral","GPT","Qwen","Baichuan2","Fine-tuning in machine learning involves adjusting a pre-trained machine learning model's weight on new data to enhance task-specific performance. Essentially, when you want to apply an AI model to a new task, you take an existing model and optimize it with new datasets. This allows you to customize the model to meet your specific needs and domain requirements.","A pre-trained model has a large number of parameters designed for general-purpose use, and effectively fine-tuning such a large model requires a sufficient amount of training data.","With the MoAI Platform, you can easily apply optimized parallelization techniques that consider the GPU's memory size, significantly reducing the time and effort needed before starting training."]},{"i":"what-you-will-learn-here","l":"What you will learn here:","p":["Loading datasets, models, and tokenizers","Running training and checking results","Applying automatic parallelization","Choosing the right training environment and AI accelerators"]}],[{"l":"Llama2 Fine-tuning","p":["This tutorial introduces an example of fine-tuning the open-source Llama2 13B model on the MoAI Platform. Through this tutorial, you'll learn how to leverage the AMD GPU cluster using the MoAI Platform and explore the benefits of performance and automatic parallelization."]},{"l":"Overview","p":["The Llama2 model, released by Meta in July 2023, is an open-source model based on the Decoder-only Transformer. It follows the structure of the existing Llama model but has been trained with 40% more data to understand more diverse and complex information.","Llama2 excels particularly in language understanding and generation tasks, achieving state-of-the-art performance in various natural language processing tasks. This model supports multilingual capabilities, enabling processing of text in various languages worldwide, and is publicly accessible for research and development purposes.","In this tutorial, we will fine-tune the Llama2 model on the MoAI Platform using the CNN Daily Mail dataset focus on summarization task. Summarization is one of the natural language processing techniques, where the task is to unravel long, complex text and deliver precise summaries."]},{"l":"Before You Start","p":["Be sure to acquire a container or virtual machine on the MoAI Platform from your infrastructure provider and familiarize yourself with connecting to it via SSH. For example, you can sign up for the following public cloud service built on the MoAI Platform:","KT Cloud's Hyperscale AI Computing ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","If you wish to temporarily allocate trial containers and GPU resources, please contact Moreh( support@moreh.io).","After connecting via SSH, run the moreh-smi command to ensure that the MoAI Accelerator is displayed correctly. The device name may vary depending on the system."]},{"l":"Check MoAI Accelerator","p":["To train models like the Llama2 model outlined in this tutorial, you need to select an appropriate size MoAI Accelerator. Start by using the moreh-smi command to check the currently used MoAI Accelerator.","Detailed instructions for selecting the MoAI Accelerator size required for the training will be provided in 3. Model fine-tuning"]}],[{"l":"1. Prepare Fine-tuning","p":["Preparing the PyTorch script execution environment on the MoAI Platform is similar to doing so on a typical GPU server."]},{"l":"Checking PyTorch Installation","p":["After connecting to the container via SSH, run the following command to check if PyTorch is installed in the current conda environment:","The version name includes both the PyTorch version and the version of MoAI required to run it. In the example above, it indicates that version 24.2.0 of MoAI, which runs PyTorch version 1.13.1+cu116, is installed.","If you see the message conda: command not found, if the torch package is not listed, or if the torch package exists but does not include \"moreh\" in the version name, please follow the instructions in the Prepare Fine-tuning on MoAI Platform document to create a conda environment."]},{"l":"Verifying PyTorch Installation","p":["Run the following command to confirm that the torch package is properly imported and the MoAI Accelerator is recognized."]},{"l":"Download the Training Script","p":["Execute the following command to download the PyTorch script for training from the GitHub repository. In this tutorial, we will be using the train_llama2.py script located inside the tutorial directory."]},{"l":"Install Required Python Packages","p":["Execute the following command to install third-party Python packages required for script execution:"]},{"l":"Download the Model and Tokenizer","p":["Download the checkpoint and tokenizer for the Llama2-13b-hf model using Hugging Face. Please note that the Llama2 model requires community license agreement and Hugging Face token information. Additionally, since the checkpoint size for the Llama2 13B model is approximately 49GB, it is essential to have at least 50GB of storage space for the checkpoint.","Begin by visiting the following website and providing the required information to proceed with the license agreement.","meta-llama/Llama-2-13b-hf · Hugging Face","Once you've submitted the agreement form, check that the status on the page has updated as follows:","Once the status has changed, you can utilize the download_llama2_13b.py script found in the tutorial directory to download the model checkpoint and tokenizer into the ./llama-2-13b-hf directory.","Make sure to replace user-token with your Hugging Face token.","Check if the model checkpoint and tokenizer have been downloaded."]},{"l":"Download Training Data","p":["To download the training data, we'll use the prepare_llama2_dataset.py script located in the dataset directory. When you run the code, it will download the cnn_dailymail dataset, preprocess it for training, and save it as llama2_dataset.pt file.","You can then load the stored dataset in your code like this:"]}],[{"l":"2. Understanding training code","p":["If you've got all your training data ready, let's dive into running the actual fine-tuning process using the train_llama2.py script. This script is just standard PyTorch code, performing fine-tuning based on the Llama2 13B model from the Hugging Face Transformers library.","We highly recommend proceeding with the tutorial using the provided script as is. Afterward, feel free to customize the script to fine-tune the Llama2 13B model or any other publicly available model in a different manner. If needed, refer to the MoAI Platform application guide LLM Fine-tuning Parameter Guide provided by Moreh."]},{"l":"Training Code","p":["All the code used during training is exactly the same as when you're using PyTorch in general.","Import the necessary modules from the transformers library.","Then, load up the model checkpoint and tokenizer you downloaded earlier.","Load your preprocessed dataset, which you prepared during the 1. Prepare fine-tuning step, and define your data loaders.","Training proceeds as usual, just like with any other PyTorch model.","With MoAI Platform, you can seamlessly use your existing PyTorch scripts without any modifications."]},{"l":"About Advanced Parallelism","p":["In the training script used in this tutorial, there is an additional line of code as follows, which executes the top-tier parallelization feature provided by the MoAI Platform:","Training a large language model like Llama2 13B requires a significant amount of GPUs. Without using the MoAI Platform, you would need to implement parallelization techniques such as data parallelism, pipeline parallelism, and tensor parallelism to perform the training.","(Reference: https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","While DDP can be relatively easy to apply, implementing techniques like pipeline parallelism or tensor parallelism involves quite complex code modifications. To apply optimized parallelization, you need to understand how Python code acts in a multiprocessing environment while writing the training scripts. Especially in multi-node setups, configuring the environment of each node used for training is necessary. Additionally, finding the optimal parallelization method considering factors such as model type, size, and dataset requires a considerable amount of time.","In contrast, MoAI Platform's AP feature enables users to proceed with optimized parallelized training with just one line of code added to the training script, eliminating the need for users to manually apply additional parallelization techniques.","MoAI Platform's Advanced Parallelization (AP) provides optimization and automation features that are difficult to experience in other frameworks. With the AP feature, you can easily configure the optimal parameters and environment variables for Pipeline Parallelism and Tensor Parallelism which are typically required for large-scale model training, with just a single line of code."]}],[{"l":"3. Model fine-tuning","p":["Now, we will train the model through the following process."]},{"l":"Setting Accelerator Flavor","p":["AMD MI210 GPU with 32 units.","AMD MI250 GPU with 16 units:","AMD MI300X GPU with 8 units.","Before continuing with the tutorial, we recommend reaching out to your infrastructure provider to inquire about the types and quantities of GPUs associated with each flavor. Once you have this information, you can choose one of the following flavors to proceed:","Enter 8 to use","Enter q to complete the change.","First, we'll use the moreh-smi command to check the currently used MoAI Accelerator.","In MoAI Platform, physical GPUs are not directly exposed to users. Instead, virtual MoAI Accelerators are provided, which are available for use in PyTorch. By setting the accelerator's flavor, you can determine how much of the physical GPU will be utilized by PyTorch. Since the total training time and GPU usage cost vary depending on the selected accelerator flavor, users should make decisions based on their training scenarios. Refer to the following document to select the accelerator Flavor that aligns with your training objectives.","In this tutorial, we will use a 2048GB-sized MoAI Accelerator.","KT Hyperscale AI Computing (HAC) AI Accelerator Information","LLM Fine-tuning Parameter Guide","Now you can see that it has been successfully changed to .","Please refer to the document above or reach out to your infrastructure provider to inquire about the GPU types and quantities corresponding to each flavor.","Remember when we checked the MoAI Accelerator in the 1. Prepare Fine-tuning? Now let's set up the accelerator needed for learning.","Select when using KT Cloud's Hyperscale AI Computing.","Select when using Moreh's trial container.","The current MoAI Accelerator in use has a memory size of 512GB.","Therefore, after switching from the initially set flavor to , we will use the moreh-smi command to confirm that the change has been successfully applied.","To confirm that the changes have been successfully applied, use the moreh-smi command again to check the currently used MoAI Accelerator.","You can enter the number to switch to a different flavor.","You can utilize the moreh-switch-model command to review the available accelerator flavors on the current system. For seamless model training, consider using the moreh-switch-model command to switch to a MoAI Accelerator with larger memory capacity."]},{"l":"Training Execution","p":["Execute the given train_gpt.py script.","If the training proceeds smoothly, you should see the following logs. By going through this logs, you can verify that the Advanced Parallelism feature, which determines the optimal parallelization settings, is functioning properly. It's worth noting that, apart from the single line of AP code we looked at earlier in the PyTorch script, there is no handling for using multiple GPUs simultaneously in other parts of the script.","You can verify that the training is proceeding smoothly by checking the training logs.","The throughput displayed during training indicates how many tokens are being trained per second through the PyTorch script.","Throughput when using 16 AMD MI250 GPUs: Approximately 35,000 tokens/sec","Here are the approximate training times based on the type and number of GPUs:","Training time when using 16 AMD MI250 GPUs: Approximately 10 hours"]},{"l":"Checking Accelerator Status During Training","p":["During training, open another terminal and connect to the container. Then, execute the moreh-smi command to observe the MoAI Accelerator occupying memory and the training script running. Make sure to check this while the initialization process is completed and the training loss appears in the execution logs."]}],[{"l":"4. Checking Training Results","p":["Upon running the train_llama2.py script as described earlier, the resulting model will be saved in the llama2_summarization directory. This model is a pure PyTorch parameter file and is fully compatible with regular GPU servers, not just the MoAI Platform.","You can test the trained model using the inference_llama2.py script located under the tutorial directory of the pre-downloaded GitHub repository.","For testing, articles related to English Premier League (EPL) match results have been used.","Run the train script.","From the output, you'll notice that Llama2 has appropriately summarized the contents of the prompt."]}],[{"l":"5. Changing the Number of GPUs","p":["Let's rerun the fine-tuning task with a different number of GPUs. MoAI Platform abstracts GPU resources into a single accelerator and automatically performs parallel processing. Therefore, there is no need to modify the PyTorch script even when changing the number of GPUs."]},{"l":"Changing Accelerator type","p":["Switch the accelerator type using the moreh-switch-model tool. For instructions on changing the accelerator, please refer again to the 3. Model fine-tuning.","Please contact your infrastructure provider and choose one of the following options before proceeding. KT Hyperscale AI Computing (HAC) AI Accelerator Information","AMD MI250 GPU with 32 units","When using Moreh's trial container: select","When using KT Cloud's Hyperscale AI Computing: select","AMD MI210 GPU with 64 units","AMD MI300X GPU with 16 units"]},{"l":"Training Parameters","p":["Run the train_llama2.py script again.","Since the GPU memory has doubled, let's increase the batch size from the previous 256 to 512 and run the code again.","If the training proceeds smoothly, you'll see logs similar to the following:","Compared to the previous results obtained when the GPU count was halved, you'll notice that the training is progressing similarly, but with an improved throughput.","When using AMD MI250 GPU 16 → 32 : approximately 35,000 tokens/sec → 74,000 tokens/sec"]}],[{"l":"6. Conclusion","p":["From this tutorial, we have seen how to fine-tune Llama2 13B for text summarization on the MoAI Platform. Open-source LLMs like Llama can be utilized for various natural language processing tasks such as summarization, question answering, and more. With the MoAI Platform, you can easily configure the required number of GPUs without any code modifications.","The availability of large language models like LLaMA 2, fine-tuning techniques, and the MoAI Platform makes it possible for anyone to develop powerful AI applications. So please start repeating the same process outlined here on your own data.","In case if you still have any questions regarding this tutorial feel free to ask Moreh( support@moreh.io)."]},{"l":"Learn More","p":["MoAI Platform's Advanced Parallelization (AP)","Mistral Fine-tuning","GPT Fine-tuning","Baichuan2 Fine-tuning","Qwen Fine-tuning"]}],[{"l":"Llama3 8B Fine-tuning","p":["This tutorial introduces an example of fine-tuning the open-source Llama3-8b model on the MoAI Platform. Through this tutorial, you will learn how to use an AMD GPU cluster with MoAI Platform and understand the benefits of its performance and automatic parallelization."]},{"l":"Overview","p":["The Llama3 model is an open-source, decoder-only Transformer model released by Meta in April 2024. It follows the architecture of previous Llama models but is trained on seven times more data (15T), enabling it to understand more diverse and complex information.","Llama3 excels in tasks involving language understanding and generation, achieving performance that significantly surpasses previous state-of-the-art results in various natural language processing tasks. It supports multiple languages, making it capable of processing texts from around the world, and is widely accessible for research and development purposes.","In this tutorial, we will fine-tune the Llama3 model on the MoAI Platform for a summarization task using the CNN Daily Mail dataset."]},{"l":"Getting Started","p":["Before you begin, ensure that you have access to a container or virtual machine on the MoAI Platform from an infrastructure provider, and that you have received instructions on how to connect via SSH. For instance, you can apply for and use the following public cloud service based on the MoAI Platform:","Hyperscale AI Computing from KT Cloud ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","Alternatively, if you would like to temporarily allocate trial containers and GPU resources, please contact Moreh( support@moreh.io)","After connecting via SSH, run the moreh-smi command to verify that the MoAI Accelerator is properly displayed. Device names may vary depending on the system configuration."]},{"l":"Checking the MoAI Accelerator","p":["To train sLLMs like the Llama3 model described in this tutorial, you need to select an appropriately sized MoAI Accelerator. First, use the moreh-smi command to check the current MoAI Accelerator in use.","Detailed instructions on configuring the MoAI Accelerator for your specific training needs will be provided in section \"3. Model fine-tuning\""]}],[{"l":"1. Prepare Fine-tuning","p":["Setting up the PyTorch execution environment on the MoAI Platform is similar to setting it up on a typical GPU server."]},{"l":"Checking PyTorch Installation","p":["After connecting to the container via SSH, run the following command to check if PyTorch is installed in the current conda environment:","The version name includes both the PyTorch version and the MoAI version required to run it. In the example above, it indicates that PyTorch 1.13.1+cu116 is installed with MoAI version 24.5.0.","If you see the message conda: command not found, if the torch package is not listed, or if the torch package exists but does not include \"moreh\" in the version name, please follow the instructions in the Prepare Fine-tuning on MoAI Platform document to create a conda environment."]},{"l":"Verifying PyTorch Installation","p":["Run the following command to confirm that the torch package is properly imported and the MoAI Accelerator is recognized."]},{"l":"Download the Training Script","p":["Execute the following command to download the PyTorch script for training from the GitHub repository. In this tutorial, we will be using the train_llama3.py script located inside the tutorial directory."]},{"l":"Install Required Python Packages","p":["Execute the following command to install third-party Python packages required for script execution:"]},{"l":"Download the Model and Tokenizer","p":["Use Hugging Face to download the checkpoint and tokenizer for the Llama3-8b model. Note that you will need to agree to the community license and provide your Hugging Face token information. Additionally, for the Llama3 8B model, you should have approximately 20GB of free storage available for the checkpoint, which is around 16GB.","First, enter the required information and agree to the license on the following site.","https://huggingface.co/meta-llama/Meta-Llama-3-8B","After submitting the agreement, confirm that the page status has changed as shown below.","Once the status has changed, you can use the download_llama3_8b.py script in the tutorial directory to download the model checkpoint and tokenizer to the ./llama3-8b directory.","Replace user-token with your Hugging Face token.","Check if the model checkpoint and tokenizer have been downloaded successfully."]},{"l":"Download Training Data","p":["To download the training data, use the prepare_llama3_dataset.py script in the dataset directory. Running this script will download and preprocess the cnn_dailymail dataset for training, saving it as the llama3_dataset.pt file.","You can load and use the saved dataset in your code as follows."]}],[{"l":"2. Understanding training code","p":["Once you have prepared all the training data, let's take a look at the contents of the train_llama3.py script, which will carry out the actual fine-tuning process. This script executes fine-tuning based on the implementation of the Llama3 8B model in the Hugging Face Transformers library, using standard PyTorch code.","We recommend initially proceeding with the provided script as is until the end of the tutorial. Afterwards, feel free to modify the script as desired to fine-tune the Llama3 8B model in different ways. This flexibility is possible due to MoAI Platform's complete compatibility with PyTorch. If needed, refer to the MoAI Platform Application Guide provided by Moreh ( LLM Fine-tuning Parameter Guide)."]},{"l":"Training Code","p":["All the code used during training is exactly the same as when you're using PyTorch in general.","Import the necessary modules from the transformers library.","Then, load up the model checkpoint and tokenizer you downloaded earlier.","Load your preprocessed dataset, which you prepared during the 1. Prepare fine-tuning step, and define your data loaders.","Subsequently, the training proceeds similarly to regular PyTorch model training.","In this way, you can write code in MoAI Platform using the same approach as with standard PyTorch code."]},{"l":"About Advanced Parallelism","p":["In the training script used in this tutorial, there is an additional line of code as follows, which executes the top-tier parallelization feature provided by the MoAI Platform:","Training a large language model like Llama2 13B requires a significant amount of GPUs. Without using the MoAI Platform, you would need to implement parallelization techniques such as data parallelism, pipeline parallelism, and tensor parallelism to perform the training.","(Reference: https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","While DDP can be relatively easy to apply, implementing techniques like pipeline parallelism or tensor parallelism involves quite complex code modifications. To apply optimized parallelization, you need to understand how Python code acts in a multiprocessing environment while writing the training scripts. Especially in multi-node setups, configuring the environment of each node used for training is necessary. Additionally, finding the optimal parallelization method considering factors such as model type, size, and dataset requires a considerable amount of time.","In contrast, MoAI Platform's AP feature enables users to proceed with optimized parallelized training with just one line of code added to the training script, eliminating the need for users to manually apply additional parallelization techniques.","MoAI Platform's Advanced Parallelization (AP) provides optimization and automation features that are difficult to experience in other frameworks. With the AP feature, you can easily configure the optimal parameters and environment variables for Pipeline Parallelism and Tensor Parallelism which are typically required for large-scale model training, with just a single line of code."]}],[{"l":"3. Model fine-tuning","p":["Now, we will actually execute the fine-tuning process."]},{"l":"Setting Accelerator Flavor","p":["AMD MI210 GPU with 32 units.","AMD MI250 GPU with 16 units:","AMD MI300X GPU with 8 units.","Enter 8 to use","Enter q to complete the change.","First, we'll use the moreh-smi command to check the currently used MoAI Accelerator.","In MoAI Platform, physical GPUs are not directly exposed to users. Instead, virtual MoAI Accelerators are provided, which are available for use in PyTorch. By setting the accelerator's flavor, you can determine how much of the physical GPU will be utilized by PyTorch. Since the total training time and GPU usage cost vary depending on the selected accelerator flavor, users should make decisions based on their training scenarios. Refer to the following document to select the accelerator Flavor that aligns with your training objectives.","In this tutorial, we will use a 2048GB-sized MoAI Accelerator.","KT Hyperscale AI Computing (HAC) AI Accelerator Information","LLM Fine-tuning Parameter Guide","Now you can see that it has been successfully changed to .","Please refer to the document above or reach out to your infrastructure provider to inquire about the GPU types and quantities corresponding to each flavor.","Remember when we checked the MoAI Accelerator in the 1. Llama3 8B Fine-tuning? Now let's set up the accelerator needed for learning.","Select when using KT Cloud's Hyperscale AI Computing.","Select when using Moreh's trial container.","The current MoAI Accelerator in use has a memory size of 512GB.","Therefore, after switching from the initially set flavor to , we will use the moreh-smi command to confirm that the change has been successfully applied.","To confirm that the changes have been successfully applied, use the moreh-smi command again to check the currently used MoAI Accelerator.","You can enter the number to switch to a different flavor.","You can utilize the moreh-switch-model command to review the available accelerator flavors on the current system. For seamless model training, consider using the moreh-switch-model command to switch to a MoAI Accelerator with larger memory capacity."]},{"l":"Training Execution","p":["Execute the train_llama3.py script below.","If the training proceeds smoothly, you should see the following logs. By going through this logs, you can verify that the Advanced Parallelism feature, which determines the optimal parallelization settings, is functioning properly. It's worth noting that, apart from the single line of AP code we looked at earlier in the PyTorch script, there is no handling for using multiple GPUs simultaneously in other parts of the script.","Upon checking the training logs, you can confirm that the training is progressing smoothly.","The throughput displayed during training indicates how many tokens per second the script is training through this PyTorch script.","When using AMD MI250 GPU with 16 GPUs: Approximately 200,000 tokens/sec","The approximate training time depending on the GPU type and quantity is as follows:","When using AMD MI250 GPU with 16 GPUs: Approximately 90 minutes"]},{"l":"Checking Accelerator Status During Training","p":["During training, open another terminal and connect to the container. Then, execute the moreh-smi command to observe the MoAI Accelerator occupying memory and the training script running. Make sure to check this while the initialization process is completed and the training loss appears in the execution logs."]}],[{"l":"4. Checking Training Results","p":["When you execute the train_llama3.py script as in the previous section, the resulting model will be saved in the llama3_summarization directory. This model is compatible with any GPU server, not just MoAI Platform, as it is stored as a pure PyTorch model parameter file.","You can test the trained model using the inference_llama3.py script located under the tutorial directory in the GitHub repository you downloaded earlier.","For testing, articles related to soldiers deployed in Iraq were used.","Run the code.","Upon examining the output, you will see that the model appropriately summarizes the contents of the input prompt."]}],[{"l":"5. Changing the Number of GPUs","p":["Let's rerun the fine-tuning task with a different number of GPUs. MoAI Platform abstracts GPU resources into a single accelerator and automatically performs parallel processing. Therefore, there is no need to modify the PyTorch script even when changing the number of GPUs."]},{"l":"Changing the Accelerator type","p":["Switch the accelerator type using the moreh-switch-model tool. For instructions on changing the accelerator, please refer again to the 3. Model fine-tuning.","Please contact your infrastructure provider and choose one of the following options before proceeding. KT Hyperscale AI Computing (HAC) AI Accelerator Information","AMD MI250 GPU with 32 units","When using Moreh's trial container: select","When using KT Cloud's Hyperscale AI Computing: select","AMD MI210 GPU with 64 units","AMD MI300X GPU with 16 units"]},{"l":"Training Parameters","p":["Again, run the train_llama3.py script.","Since the available GPU memory has doubled, let's increase the batch size from the previous 256 to 512 and run the code again.","If the training proceeds normally, you will see similar logs to the previous run but with improved throughput due to the doubled number of GPUs.","When using AMD MI250 GPU 16 → 32 : From approximately 200,000 tokens/sec to 390,000 tokens/sec."]}],[{"l":"6. Conclusion","p":["From this tutorial, we have seen how to fine-tune Llama3 8B on the MoAI Platform. Open-source LLMs like Llama can be utilized for various tasks such as summarization, question answering, and more. With the MoAI platform, you can easily configure the number of GPUs you need without any code changes.","The availability of large language models like LLaMA 3, fine-tuning techniques, and the MoAI Platform makes it possible for anyone to develop powerful AI applications. So please start developing models by repeating the process outlined here on your own data.","In case if you still have any questions regarding this tutorial feel free to ask Moreh( support@moreh.io)"]},{"l":"Learn More","p":["MoAI Platform's Advanced Parallelization (AP)","Mistral Fine-tuning","GPT Fine-tuning","Baichuan2 Fine-tuning","Qwen Fine-tuning"]}],[{"l":"Mistral Fine-tuning","p":["This tutorial guides you on fine-tuning the open-source Mistral 7B model on the MoAI Platform. You'll learn to utilize an AMD GPU cluster using the MoAI Platform and experience the improved performance and the benefits of automatic parallelization."]},{"l":"Overview","p":["The Mistral model, released by Mistral AI in 2023, is a giant language model. It has gained attention for outperforming larger models in complex tasks like code generation, question answering, and solving mathematical problems.","The Mistral 7B model uses only the Transformer's decoder, applying techniques like Sliding Window Attention to efficiently process the length of input tokens and introducing Rolling Buffer Cache to optimize memory usage.","In this tutorial, we'll fine-tune the Mistral 7B model using the python_code_instructions_18k-alpaca dataset for the code generation task on the MoAI Platform."]},{"l":"Before You Start","p":["Be sure to acquire a container or virtual machine on the MoAI Platform from your infrastructure provider and familiarize yourself with connecting to it via SSH. For example, you can sign up for the following public cloud service built on the MoAI Platform:","KT Cloud’s Hyperscale AI Computing ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","If you wish to temporarily allocate trial containers and GPU resources, please contact Moreh( support@moreh.io).","After connecting via SSH, run the moreh-smi command to ensure that the MoAI Accelerator is displayed correctly. The device name may vary depending on the system."]},{"l":"Check MoAI Accelerator","p":["To train models like the Llama2 model outlined in this tutorial, you need to select an appropriate size MoAI Accelerator. Start by using the moreh-smi command to check the currently used MoAI Accelerator.","Detailed instructions for selecting the MoAI Accelerator size required for the training will be provided in 3. Model fine-tuning."]}],[{"l":"1. Prepare Fine-tuning","p":["Preparing the PyTorch script execution environment on the MoAI Platform is similar to doing so on a typical GPU server."]},{"l":"Checking PyTorch Installation","p":["After connecting to the container via SSH, run the following command to check if PyTorch is installed in the current conda environment:","The version name includes both the PyTorch version and the version of MoAI required to run it. In the example above, it indicates that version 24.2.0 of MoAI, which runs PyTorch version 1.13.1+cu116, is installed.","If you see the message conda: command not found, if the torch package is not listed, or if the torch package exists but does not include \"moreh\" in the version name, please follow the instructions in the Prepare Fine-tuning on MoAI Platform to create a conda environment."]},{"l":"Verifying PyTorch Installation","p":["Run the following command to ensure that the torch package is imported correctly and the MoAI Accelerator is recognized."]},{"l":"Install Required Python Packages","p":["Execute the following command to install third-party Python packages required for script execution:"]},{"l":"Download the Training Script","p":["Execute the following command to download the PyTorch script for training from the GitHub repository. In this tutorial, we will be using the train_mistral.py script located inside the tutorial directory."]},{"l":"Download the Model and Tokenizer","p":["Let's download the checkpoint and tokenizer for the Mistral 7B v0.1 model using Hugging Face. This process requires agreeing to the community license and providing your Hugging Face token information to access the Mistral model. Additionally, since the checkpoint size for the Mistral 7B model is approximately 15GB, it's essential to have at least 16GB of storage space available to store the checkpoint.","First, enter the required information on the Hugging Face website below and proceed with the license agreement.","mistralai/Mistral-7B-v0.1 · Hugging Face","After submitting the agreement form, confirm that the status on the page has changed as follows:","If the status has been updated, you can use the download_mistral_7b.py script located in the tutorial directory to download the model checkpoint and tokenizer into the ./mistral-7b directory.","Replace user-token with your Hugging Face token.","Check if the model checkpoint and tokenizer have been downloaded."]},{"l":"Download Training Data","p":["In this tutorial, we will use the python_code_instructions_18k_alpaca dataset (11.4 MB) available on Hugging Face among various datasets publicly available for code generation training.","We will execute prepare_mistral_dataset.py to download the dataset and preprocess it for training.","The preprocessed dataset will be saved as mistral_dataset.pt.","You can load the saved dataset in your code as follows."]}],[{"l":"2. Understanding training code","p":["Once you have prepared all the training data, let's delve into the contents of the train_mistral.py script to execute the actual fine-tuning process. In this step, you will confirm MoAI Platform's full compatibility with PyTorch, ensuring that the training code is identical to general PyTorch code for Nvidia GPUs. Moreover, you'll explore how efficiently MoAI Platform implements complex parallelization techniques beyond the conventional scope.","We highly recommend proceeding with the tutorial using the provided script as is. Afterward, feel free to customize the script to fine-tune the Llama2 13B model or any other publicly available model in a different manner. If needed, refer to the MoAI Platform application guide LLM Fine-tuning Parameter Guide provided by Moreh."]},{"l":"Training Code","p":["All the code used during training is exactly the same as when you're using PyTorch in general.","Import the necessary modules from the transformers library.","Load the model configuration and checkpoint publicly available on Hugging Face.","Then load the preprocessed dataset saved during the preparation for 1. Prepare Fine-tuning and define the data loaders.","Subsequently, the training proceeds similarly to general AI model training with Pytorch.","With MoAI Platform, you can seamlessly use your existing PyTorch scripts without any modifications."]},{"l":"About Advanced Parallelism","p":["In the training script used in this tutorial, there is an additional line of code as follows, which executes the top-tier parallelization feature provided by the MoAI Platform:","For colossal language models like Mistral 7B used in this tutorial, it's imperative to train them using multiple GPUs. When using frameworks other than MoAI Platform, you'll need to introduce parallelization techniques such as Data Parallel, Pipeline Parallel, and Tensor Parallel.","For instance, if a user wants to apply DDP in their typical PyTorch code, they would need to add the following code snippet. ( https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","While DDP can be relatively easy to apply, implementing techniques like pipeline parallelism or tensor parallelism involves quite complex code modifications. To apply optimized parallelization, you need to understand how Python code acts in a multiprocessing environment while writing the training scripts. Especially in multi-node setups, configuring the environment of each node used for training is necessary. Additionally, finding the optimal parallelization method considering factors such as model type, size, and dataset requires a considerable amount of time.","In contrast, MoAI Platform's AP feature enables users to proceed with optimized parallelized training with just one line of code added to the training script, eliminating the need for users to manually apply additional parallelization techniques.","MoAI Platform's Advanced Parallelization(AP) provides optimization and automation features that are difficult to experience in other frameworks. Through the AP feature, users can experience the best distributed parallel processing. By leveraging AP, users can easily configure the optimal parameters and environment variables for Pipeline Parallelism and Tensor Parallelism required for training large-scale models with just a single line of code."]}],[{"l":"3. Model fine-tuning","p":["Now, we will train the model through the following process."]},{"l":"Setting Accelerator Flavor","p":["AMD MI210 GPU with 32 units.","AMD MI250 GPU with 16 units:","AMD MI300X GPU with 8 units.","Do you remember checking MoAI Accelerator in the Mistral Fine-tuning document? Now let's set up the accelerator needed for learning.","Enter 8 to use .","Enter q to complete the change.","First, we'll use the moreh-smi command to check the currently used MoAI Accelerator.","In MoAI Platform, physical GPUs are not directly exposed to users. Instead, virtual MoAI Accelerators are provided, which are available for use in PyTorch. By setting the accelerator's flavor, you can determine how much of the physical GPU will be utilized by PyTorch. Since the total training time and GPU usage cost vary depending on the selected accelerator flavor, users should make decisions based on their training scenarios. Refer to the following document to select the accelerator Flavor that aligns with your training objectives.","In this tutorial, we will use a 2048GB-sized MoAI Accelerator.","KT Hyperscale AI Computing (HAC) AI Accelerator Information","LLM Fine-tuning Parameter Guide","Now you can see that it has been successfully changed to .","Please refer to the document above or reach out to your infrastructure provider to inquire about the GPU types and quantities corresponding to each flavor.","Select when using KT Cloud's Hyperscale AI Computing.","Select when using Moreh's trial container.","The current MoAI Accelerator in use has a memory size of 64GB.","Therefore, after switching from the initially set flavor to , we will use the moreh-smi command to confirm that the change has been successfully applied.","To confirm that the changes have been successfully applied, use the moreh-smi command again to check the currently used MoAI Accelerator.","You can choose one of the following flavors to proceed:","You can enter the number to switch to a different flavor.","You can utilize the moreh-switch-model command to review the available accelerator flavors on the current system. For seamless model training, consider using the moreh-switch-model command to switch to a MoAI Accelerator with larger memory capacity."]},{"l":"Training Execution","p":["Execute the train_mistral.py script below.","If the training proceeds smoothly, you should see the following log. Take note of the sections highlighted in blue, as they indicate that the Advanced Parallelism feature is functioning correctly. It's worth noting that in the PyTorch script we examined earlier, there was no handling for using multiple GPUs simultaneously.","You can confirm that the training is progressing smoothly by observing the loss values decreasing as follows.","The throughput displayed during training indicates how many tokens per second are being processed through the PyTorch script.","When using 16 AMD MI250 GPUs: approximately 60,000 tokens/sec","Approximate training time based on GPU type and quantity is as follows:","When using 16 AMD MI250 GPUs: approximately 50 minutes"]},{"l":"Checking Accelerator Status During Training","p":["During training, open another terminal and connect to the container. You can execute the moreh-smi command to observe the MoAI Accelerator occupying memory while the training script is running. Please check the memory occupancy of MoAI accelerator when the training loss appears in the execution log after the initialization process."]}],[{"l":"4. Checking Training Results","p":["Running the train_mistral.py script, as in the previous section, will save the resulting model in the mistral_code_generation directory. This is a pure PyTorch model parameter file and is fully compatible with regular GPU servers, not just the MoAI Platform.","You can test the trained model using the inference_mistral.py script located in the tutorial directory of the GitHub repository you downloaded earlier. In this test, the prompt \"Create a function that takes a list of strings as input and joins them with spaces\" was used.","Run the code below.","Upon examining the output, you can confirm that the model has appropriately generated the function as per the prompt."]}],[{"l":"5. Changing the Number of GPUs","p":["Let's rerun the fine-tuning task with a different number of GPUs. MoAI Platform abstracts GPU resources into a single accelerator and automatically performs parallel processing. Therefore, there is no need to modify the PyTorch script even when changing the number of GPUs."]},{"l":"Changing the Accelerator type","p":["Switch the accelerator type using the moreh-switch-model tool. For instructions on changing the accelerator, please refer to the 3. Model fine-tuning","Please contact your infrastructure provider and choose one of the following options before proceeding. KT HAC AI Accelerator Information","AMD MI250 GPU with 32 units","When using Moreh's trial container: select","When using KT Cloud's Hyperscale AI Computing: select","AMD MI210 GPU with 64 units","AMD MI300X GPU with 16 units"]},{"l":"Training Parameters","p":["Run the train_mistral.py script again without changing the batch size.","If the training proceeds normally, you should see the following logs:","Compared to the previous execution results when the number of GPUs was half, you can see that the learning is the same and the throughput has improved.","When using AMD MI250 GPU 16 → 32 : approximately 60,000 tokens/sec → 110,000 tokens/sec"]}],[{"l":"6. Conclusion","p":["From this tutorial, we have seen how to fine-tune the Mistral 7B model on the MoAI Platform. With MoAI Platform, you can easily fine-tune PyTorch-based open-source LLM models on GPU clusters while retaining your existing training code. Additionally, using the MoAI platform, you can easily configure the number of GPUs you need without any code changes. So please don’t hesitate to dive in and develop new models quickly and effortlessly with your data!"]},{"l":"Learn more","p":["MoAI Platform's Advanced Parallelization (AP)","Llama2 Fine-tuning","GPT Fine-tuning","Baichuan2 Fine-tuning","Qwen Fine-tuning"]}],[{"l":"GPT Fine-tuning","p":["This tutorial guides you on how to fine-tune GPT-based models open-sourced by Hugging Face on the MoAI Platform. Throughout this tutorial, you'll learn how to utilize an AMD GPU cluster with the MoAI Platform and explore the benefits of improved performance and automatic parallelization."]},{"l":"Overview","p":["GPT is a language model architecture that uses only the Transformer decoder structure. It was first introduced by OpenAI with GPT-1 in 2018. Since then, OpenAI has developed GPT-2, GPT-3, and GPT-4 models by increasing the dataset size and model parameters used for pre-training. Among them, the models that have been open-sourced are GPT-1 and GPT-2.","As the basic architecture of GPT is open-source, Hugging Face offers various GPT-based models beyond those developed by OpenAI.","In this tutorial, we'll use the MoAI Platform to fine-tune the Cerebras-GPT-13B model for the code generation task."]},{"l":"Before You Start","p":["Make sure to obtain a container or virtual machine on the MoAI Platform from your infrastructure provider and learn how to connect to it via SSH. For instance, you can apply for the following public cloud service based on the MoAI Platform:","KT Cloud's Hyperscale AI Computing ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","If you wish to temporarily allocate trial containers and GPU resources, please contact Moreh( support@moreh.io).","After connecting via SSH, run the moreh-smi command to ensure that the MoAI Accelerator is displayed correctly. The device name may vary depending on the system."]},{"l":"Checking MoAI Accelerator","p":["To train sLLMs like the GPT model we'll be guiding you through in this tutorial, you need to select an appropriate size MoAI Accelerator. First, use the moreh-smi command to check the currently used MoAI Accelerator.","Detailed instructions for selecting the MoAI Accelerator size required for the training will be provided in 3. Finetuning Model"]}],[{"l":"1. Prepare Fine-tuning","p":["Preparing the PyTorch script execution environment on the MoAI Platform is similar to doing so on a typical GPU server."]},{"l":"Checking PyTorch Installation","p":["After connecting to the container via SSH, run the following command to check if PyTorch is installed in the current conda environment:","The version name includes both the PyTorch version and the version of MoAI required to run it. In the example above, it indicates that version 24.2.0 of MoAI, which runs PyTorch version 1.13.1+cu116, is installed.","If you see the message conda: command not found, if the torch package is not listed, or if the torch package exists but does not include \"moreh\" in the version name, please follow the instructions in the Prepare Fine-tuning on MoAI Platform document to create a conda environment."]},{"l":"Verifying PyTorch Installation","p":["Run the following command to ensure that the torch package is imported correctly and the MoAI Accelerator is recognized."]},{"l":"Install Required Python Packages","p":["Execute the following command to pre-install third-party Python packages required for script execution:"]},{"l":"Downloading Training Script","p":["Run the following command to download the PyTorch script for training from the GitHub repository. In this tutorial, we will use the train_gpt.py script located inside the tutorial directory."]},{"l":"Downloading Training Data","p":["Hugging Face provides not only model checkpoints but also various datasets that can be used for model fine-tuning.","In this tutorial, we will use the mlabonne/Evol-Instruct-Python-26k dataset. This dataset consists of Python code written in response to given prompt conditions.","To download the training data, we will use the prepare_gpt_dataset.py script located in the dataset directory to download the dataset available on Hugging Face and preprocess it for immediate use in fine-tuning training.","The preprocessed dataset is saved as gpt_dataset.pt.","The saved dataset can be loaded and used in code as follows."]}],[{"l":"2. Understanding training code","p":["Once you have prepared all the training data, let's take a look at the contents of the train_gpt.py script to execute the actual fine-tuning process. In this step, you'll notice that the MoAI Platform offers full compatibility with PyTorch, meaning that the training code is 100% identical to typical PyTorch code for Nvidia GPUs. Furthermore, you'll see how efficiently the MoAI Platform implements complex parallelization techniques beyond what's traditionally possible.","We highly recommend proceeding with the tutorial using the provided script as is. Afterward, feel free to customize the script to fine-tune the Cerebras-GPT-13B model or any other publicly available model in a different manner. If needed, refer to the MoAI Platform application guide ( LLM Fine-tuning Parameter Guide) provided by Moreh."]},{"l":"Training Code","p":["All code remains fully consistent with general PyTorch usage.","Firstly, import the required modules from the transformers library.","Load the model configuration and checkpoint publicly available on Hugging Face.","Then load the preprocessed dataset saved during the preparation for 1. Prepare Fine-tuning and define the data loaders.","Subsequently, the training proceeds similarly to general AI model training with Pytorch.","As shown above, you can code in the same way as traditional PyTorch code on MoAI Platform."]},{"l":"About Advanced Parallelism","p":["In the training script used in this tutorial, there is an additional line of code as follows, which executes the top-tier parallelization feature provided by the MoAI Platform:","For enormous language models like Cerebras-GPT-13B used in this tutorial, it is inevitable to train them using multiple GPUs. In such cases, if you were to use frameworks other than the MoAI Platform, you would need to employ parallelization techniques like Data Parallel, Pipeline Parallel, or Tensor Parallel for training.","For instance, if a user wants to apply DDP in a typical PyTorch code, the following code snippet would need to be added. ( https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","While DDP can be relatively easy to apply, implementing techniques like pipeline parallelism or tensor parallelism involves quite complex code modifications. To apply optimized parallelization, you need to understand how Python code acts in a multiprocessing environment while writing the training scripts. Especially in multi-node setups, configuring the environment of each node used for training is necessary. Additionally, finding the optimal parallelization method considering factors such as model type, size, and dataset requires a considerable amount of time.","In contrast, MoAI Platform's AP feature enables users to proceed with optimized parallelized training with just one line of code added to the training script, eliminating the need for users to manually apply additional parallelization techniques.","Experience optimal distributed parallel processing like no other framework can offer, thanks to MoAI Platform's Advanced Parallelization (AP), a feature that optimizes and automates parallelization in ways not found in other frameworks. With the AP feature, you can easily secure the optimal parameters and environment variables for Pipeline Parallelism and Tensor Parallelism, typically required for training large-scale models, with just one simple line of code."]}],[{"l":"3. Model fine-tuning","p":["Now, we will train the model through the following process."]},{"l":"Setting the Accelerator","p":["AMD MI210 GPU with 32 units.","AMD MI250 GPU with 16 units:","AMD MI300X GPU with 8 units.","Do you remember checking MoAI Accelerator in the GPT Fine-tuning document? Now let's set up the accelerator needed for learning.","Enter 8 to use","Enter q to complete the change.","First, we'll use the moreh-smi command to check the currently used MoAI Accelerator.","In MoAI Platform, physical GPUs are not directly exposed to users. Instead, virtual MoAI Accelerators are provided, which are available for use in PyTorch. By setting the accelerator's flavor, you can determine how much of the physical GPU will be utilized by PyTorch. Since the total training time and GPU usage cost vary depending on the selected accelerator flavor, users should make decisions based on their training scenarios. Refer to the following document to select the accelerator Flavor that aligns with your training objectives.","In this tutorial, we will use a 2048GB-sized MoAI Accelerator.","KT Hyperscale AI Computing (HAC) AI Accelerator Information","LLM Fine-tuning Parameter Guide","Please refer to the document above or reach out to your infrastructure provider to inquire about the GPU types and quantities corresponding to each flavor.","Select when using KT Cloud's Hyperscale AI Computing.","Select when using Moreh's trial container.","The current MoAI Accelerator in use has a memory size of 256GB.","Therefore, after switching from the initially set flavor to , we will use the moreh-smi command to confirm that the change has been successfully applied.","To confirm that the changes have been successfully applied, use the moreh-smi command again to check the currently used MoAI Accelerator.","You can choose one of the following flavors to proceed:","You can enter the number to switch to a different flavor.","You can utilize the moreh-switch-model command to review the available accelerator flavors on the current system. For seamless model training, consider using the moreh-switch-model command to switch to a MoAI Accelerator with larger memory capacity.","로 잘 변경된 것을 확인할 수 있습니다."]},{"l":"Training Execution","p":["Execute the train_gpt.py script below.","If the training proceeds smoothly, you should see the following logs. By going through this logs, you can verify that the Advanced Parallelism feature, which determines the optimal parallelization settings, is functioning properly. It's worth noting that, apart from the single line of AP code we looked at earlier in the PyTorch script, there is no handling for using multiple GPUs simultaneously in other parts of the script.","The training loss decreases as follows, confirming normal training progress.","The throughput displayed during training indicates how many tokens per second are being processed through the PyTorch script.","When using 16 AMD MI250 GPUs: approximately 6800 tokens/sec","Approximate training times based on GPU type and quantity are as follows:","When using 16 AMD MI250 GPUs: approximately 81 minutes"]},{"l":"Checking Accelerator Status During Training","p":["During training, open another terminal and connect to the container. You can execute the moreh-smi command to observe the MoAI Accelerator occupying memory while the training script is running. Please check the memory occupancy of MoAI accelerator when the training loss appears in the execution log after the initialization process."]}],[{"l":"4. Checking Training Results","p":["As in the previous chapter, when you run the train_gpt.py script, the resulting model will be saved in the code_generation directory. This is a pure PyTorch model parameter file and is fully compatible compatible not only with MoAI Platform but also with regular GPU servers.","You can test the trained model using the inference_gpt.py script located under the tutorial directory of the GitHub repository you downloaded beforehand.","Run the train script.","Upon inspecting the output, you can confirm that the model has generated an appropriate function based on the prompt content."]}],[{"l":"5. Changing the Number of GPUs","p":["Let's rerun the fine-tuning task with a different number of GPUs. MoAI Platform abstracts GPU resources into a single accelerator and automatically performs parallel processing. Therefore, there is no need to modify the PyTorch script even when changing the number of GPUs."]},{"l":"Changing the Accelerator type","p":["Switch the accelerator type using the moreh-switch-model tool. For instructions on changing the accelerator, please refer again to the 3. Model fine-tuning document.","Please contact your infrastructure provider and choose one of the following options before proceeding. ( KT Hyperscale AI Computing (HAC) AI Accelerator Information)","AMD MI250 GPU with 32 units","When using Moreh's trial container: select","When using KT Cloud's Hyperscale AI Computing: select","AMD MI210 GPU with 64 units","AMD MI300X GPU with 16 units"]},{"l":"Training Parameters","p":["Run the train_gpt.py script again.","Since the available GPU memory has doubled, let's also change the batch size to 32 and run it.","If the training proceeds normally, the following log will be output.","Compared to the previous execution results when the number of GPUs was half, you can see that the learning is the same and the throughput has improved.","When using AMD MI250 GPU 16 → 32 : approximately 6800 tokens/sec → 13000 tokens/sec"]}],[{"l":"6. Conclusion","p":["So far, we have looked at the process of fine-tuning the GPT-based model from HuggingFace on the MoAI Platform. With MoAI Platform, you can easily fine-tune PyTorch-based open-source LLM models on GPU clusters while maintaining existing training code. Additionally, if you use the MoAI platform, you can easily configure the number of GPUs you need without changing any code. Try developing new models quickly and effortlessly with your data."]},{"l":"Learn More","p":["MoAI Platform's Advanced Parallelization(AP)","Llama2 Fine-tuning","Mistral Fine-tuning","Baichuan2 Fine-tuning","Qwen Fine-tuning"]}],[{"l":"Qwen Fine-tuning","p":["This tutorial introduces an example of fine-tuning the open-source Qwen1.5 7B model on the MoAI Platform. Through this tutorial, you'll learn how to leverage the AMD GPU cluster using the MoAI Platform and explore the benefits of performance and automatic parallelization."]},{"l":"Overview","p":["The Qwen1.5 7B model is an open-source LLM released by Tongyi Qianwen(通义千问) in China. In this tutorial, we'll be performing a code generation task on the MoAI Platform, fine-tuning the Qwen1.5 7B model using the python_code_instruction_18k_alpaca dataset, which consists of system prompts, instructions for code generation, input values, and the code to be generated."]},{"l":"Before You Start","p":["Be sure to acquire a container or virtual machine on the MoAI Platform from your infrastructure provider and familiarize yourself with connecting to it via SSH. For example, you can sign up for the following public cloud service built on the MoAI Platform:","KT Cloud’s Hyperscale AI Computing ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","If you wish to temporarily allocate trial containers and GPU resources, please contact Moreh( support@moreh.io).","After connecting via SSH, run the moreh-smi command to ensure that the MoAI Accelerator is displayed correctly. The device name may vary depending on the system."]},{"l":"Check MoAI Accelerator","p":["To train models like the Llama2 model outlined in this tutorial, you need to select an appropriate size MoAI Accelerator. Start by using the moreh-smi command to check the currently used MoAI Accelerator.","Detailed instructions for selecting the MoAI Accelerator size required for the training will be provided in 3. Model fine-tuning"]}],[{"l":"1. Prepare Fine-tuning","p":["Preparing the PyTorch script execution environment on the MoAI Platform is similar to doing so on a typical GPU server."]},{"l":"Checking PyTorch Installation","p":["After connecting to the container via SSH, run the following command to check if PyTorch is installed in the current conda environment:","The version name includes both the PyTorch version and the version of MoAI required to run it. In the example above, it indicates that version 24.2.0 of MoAI, which runs PyTorch version 1.13.1+cu116, is installed.","If you see the message conda: command not found, if the torch package is not listed, or if the torch package exists but does not include \"moreh\" in the version name, please follow the instructions in the Prepare Fine-tuning on MoAI Platform document to create a conda environment."]},{"l":"Verifying PyTorch Installation","p":["Run the following command to ensure that the torch package is imported correctly and the MoAI Accelerator is recognized."]},{"l":"Install Required Python Packages","p":["Execute the following command to install third-party Python packages required for script execution:"]},{"l":"Download the Training Script","p":["Execute the following command to download the PyTorch script for training from the GitHub repository. In this tutorial, we will be using the train_qwen.py script located inside the tutorial directory."]},{"l":"Download Training Data","p":["To download the training data, we'll use the prepare_qwen_dataset.py script located in the dataset directory. When you run the code, it will download the cnn_dailymail dataset, preprocess it for training, and save it as qwen_dataset.pt file.","The preprocessed dataset is saved as qwen_dataset.pt.","You can then load the stored dataset in your code like this:"]}],[{"l":"2. Understanding training code","p":["If you've prepared all the training data, let's now take a look at the train_qwen.py script to actually run the fine-tuning process. This script is a standard PyTorch code that executes fine-tuning based on the implementation of the Qwen model available in the Hugging Face Transformers library.","In this step, you'll observe that MoAI Platform is fully compatible with PyTorch, and the training code is exactly the same as standard PyTorch code designed for NVIDIA GPUs. Moreover, you can also see how efficiently MoAI Platform can implement complex parallelization techniques.","We recommend starting by using the provided script to complete the tutorial as-is. Afterwards, feel free to modify the script as desired to fine-tune the Qwen1.5 7B model in different ways. Thanks to MoAI Platform's full compatibility with PyTorch, such modifications are possible. If needed, refer to the LLM Fine-tuning parameter guide for assistance."]},{"l":"Training Code","p":["All the code used during training is exactly the same as the standard method of using PyTorch.","Firstly, import the required modules from the transformers library.","Load the model configuration and checkpoint publicly available on Hugging Face.","Then load the preprocessed dataset saved during the preparation for 1. Prepare Fine-tuning and define the data loaders.","Subsequently, the training proceeds similarly to general AI model training with Pytorch.","As shown above, you can code in the same way as traditional PyTorch code on MoAI Platform."]},{"l":"About Advanced Parallelism","p":["In the training script used in this tutorial, there is an additional line of code as follows, which executes the top-tier parallelization feature provided by the MoAI Platform:","To train massive language models like Qwen1.5 7B, which we use in this tutorial, it's inevitable to utilize multiple GPUs. If using different frameworks, you'll need to implement parallelization techniques such as Data Parallel, Pipeline Parallel, and Tensor Parallel to proceed with training.","For instance, if a user wants to apply DDP in a typical PyTorch code, the following code snippet would need to be added. ( https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","While DDP can be relatively easy to apply, implementing techniques like pipeline parallelism or tensor parallelism involves quite complex code modifications. To apply optimized parallelization, you need to understand how Python code acts in a multiprocessing environment while writing the training scripts. Especially in multi-node setups, configuring the environment of each node used for training is necessary. Additionally, finding the optimal parallelization method considering factors such as model type, size, and dataset requires a considerable amount of time.","In contrast, MoAI Platform's AP feature enables users to proceed with optimized parallelized training with just one line of code added to the training script, eliminating the need for users to manually apply additional parallelization techniques.","Experience optimal distributed parallel processing like no other framework can offer, thanks to MoAI Platform's Advanced Parallelization (AP), a feature that optimizes and automates parallelization in ways not found in other frameworks. With the AP feature, you can easily secure the optimal parameters and environment variables for Pipeline Parallelism and Tensor Parallelism, typically required for training large-scale models, with just one simple line of code."]}],[{"l":"3. Model fine-tuning","p":["Now, we will train the model through the following process."]},{"l":"Setting Accelerator Flavor","p":["AMD MI210 GPU with 32 units.","AMD MI250 GPU with 16 units:","AMD MI300X GPU with 8 units.","Do you remember checking MoAI Accelerator in the Mistral Fine-tuning (ENG) document? Now let's set up the accelerator needed for learning.","Enter 8 to use","Enter q to complete the change.","First, we'll use the moreh-smi command to check the currently used MoAI Accelerator.","In MoAI Platform, physical GPUs are not directly exposed to users. Instead, virtual MoAI Accelerators are provided, which are available for use in PyTorch. By setting the accelerator's flavor, you can determine how much of the physical GPU will be utilized by PyTorch. Since the total training time and GPU usage cost vary depending on the selected accelerator flavor, users should make decisions based on their training scenarios. Refer to the following document to select the accelerator Flavor that aligns with your training objectives.","In this tutorial, we will use a 2048GB-sized MoAI Accelerator.","KT Hyperscale AI Computing (HAC) AI Accelerator Information","LLM Fine-tuning Parameter Guide","Please refer to the document above or reach out to your infrastructure provider to inquire about the GPU types and quantities corresponding to each flavor.","Select when using KT Cloud's Hyperscale AI Computing.","Select when using Moreh's trial container.","The current MoAI Accelerator in use has a memory size of 64GB.","Therefore, after switching from the initially set flavor to , we will use the moreh-smi command to confirm that the change has been successfully applied.","To confirm that the changes have been successfully applied, use the moreh-smi command again to check the currently used MoAI Accelerator.","You can choose one of the following flavors to proceed:","You can enter the number to switch to a different flavor.","You can utilize the moreh-switch-model command to review the available accelerator flavors on the current system. For seamless model training, consider using the moreh-switch-model command to switch to a MoAI Accelerator with larger memory capacity.","로 잘 변경된 것을 확인할 수 있습니다."]},{"l":"학습 실행","p":["주어진 train_qwen.py 스크립트를 실행합니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력될 것입니다. 중간에 파란색으로 표시된 부분을 보시면 Advanced Parallelism 기능이 정상 동작하는 것을 확인할 수 있습니다. 앞서 살펴본 PyTorch 스크립트에서는 AP 코드 한 줄을 제외한 다른 부분에서 GPU 여러 개를 동시에 사용하기 위한 처리가 전혀 없었음을 참고하십시오.","You can confirm that the training is progressing smoothly by observing the loss values appearing as follows.","The throughput displayed during training indicates how many tokens per second are being processed through the PyTorch script.","When using 16 AMD MI250 GPUs: approximately 59,000 tokens/sec","Approximate training time based on GPU type and quantity is as follows:","When using 16 AMD MI250 GPUs: approximately 40 minutes"]},{"l":"Checking Accelerator Status During Training","p":["During training, open another terminal and connect to the container. You can execute the moreh-smi command to observe the MoAI Accelerator occupying memory while the training script is running. Please check the memory occupancy of MoAI accelerator when the training loss appears in the execution log after the initialization process."]}],[{"l":"4. Checking Training Results","p":["Running the train_qwen.py script, as in the previous chapter, will save the resulting model in the qwen_code_generation directory. This is a pure PyTorch model parameter file and is fully compatible with regular GPU servers, not just the MoAI Platform.","You can test the trained model using the inference_qwen.py script located under the tutorial directory in the GitHub repository you downloaded earlier. In the test, the prompt \"Given a list of strings, create a function that joins them with spaces\" was used.","Run the code below.","Upon examining the output, you can confirm that the model has appropriately generated the function as per the prompt."]}],[{"l":"5. Changing the Number of GPUs","p":["Let's rerun the fine-tuning task with a different number of GPUs. MoAI Platform abstracts GPU resources into a single accelerator and automatically performs parallel processing. Therefore, there is no need to modify the PyTorch script even when changing the number of GPUs."]},{"l":"Changing the Accelerator type","p":["Switch the accelerator type using the moreh-switch-model tool. For instructions on changing the accelerator, please refer to the 3. Model fine-tuning","Please contact your infrastructure provider and choose one of the following options before proceeding. KT Hyperscale AI Computing (HAC) AI Accelerator Information","AMD MI250 GPU with 32 units","When using Moreh's trial container: select","When using KT Cloud's Hyperscale AI Computing: select","AMD MI210 GPU with 64 units","AMD MI300X GPU with 16 units"]},{"l":"Training Parameters","p":["Run the train_qwen.py script again without changing the batch size.","If the training proceeds normally, you should see the following logs:","Compared to the previous execution results when the number of GPUs was half, you can see that the learning is the same and the throughput has improved.","When using AMD MI250 GPU 16 → 32 : approximately 59,000 tokens/sec → 105,000 tokens/sec"]}],[{"l":"6. Conclusion","p":["So far, we've explored the process of fine-tuning the Qwen1.5 7B model on the MoAI Platform. With MoAI Platform, you can easily fine-tune PyTorch-based open-source LLM models on GPU clusters while keeping your existing training code intact. Additionally, using MoAI Platform, you can easily configure the required number of GPUs without any code changes. So please don’t hesitate to dive in and develop new models quickly and effortlessly with your data!","In case if you still have any questions regarding this tutorial feel free to ask Moreh( support@moreh.io)."]},{"l":"Learn More","p":["MoAI Platform's Advanced Parallelization (AP)","Llama2 Fine-tuning","Mistral Fine-tuning","GPT Fine-tuning","Baichuan2 Fine-tuning"]}],[{"l":"Baichuan2 Fine-tuning","p":["The following tutorial will take you through the steps required to fine-tune Baichuan2 13B model with an example dataset, using the MoAI Platform. Through the tutorial, you'll learn how to utilize an AMD GPU cluster with MoAI Platform and discover the benefits of improved performance and automatic parallelization."]},{"l":"Overview","p":["Baichuan2 is a large-scale multilingual language model developed by Baichuan Intelligent Technology. This model offers configurations with 70 billion and 130 billion parameters trained on vast datasets consisting of 2.6 trillion tokens.","In this tutorial, we'll fine-tune the Baichuan2 13B model using the MoAI Platform with the Bitext-customer-support-llm-chatbot-training-dataset, a text-generation e-commerce dataset."]},{"l":"Before You Start","p":["Be sure to acquire a container or virtual machine on the MoAI Platform from your infrastructure provider and familiarize yourself with connecting to it via SSH. For example, you can sign up for the following public cloud service built on the MoAI Platform:","KT Cloud’s Hyperscale AI Computing ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","If you wish to temporarily allocate trial containers and GPU resources, please contact Moreh( support@moreh.io).","After connecting via SSH, run the moreh-smi command to ensure that the MoAI Accelerator is displayed correctly. The device name may vary depending on the system."]},{"l":"Check MoAI Accelerator","p":["To train models like the Llama2 model outlined in this tutorial, you need to select an appropriate size MoAI Accelerator. Start by using the moreh-smi command to check the currently used MoAI Accelerator.","Detailed instructions for selecting the MoAI Accelerator size required for the training will be provided in 3. Model fine-tuning."]}],[{"l":"1. Prepare Fine-tuning","p":["Preparing the PyTorch script execution environment on the MoAI Platform is similar to doing so on a typical GPU server."]},{"l":"Checking PyTorch Installation","p":["After connecting to the container via SSH, run the following command to check if PyTorch is installed in the current conda environment:","The version name includes both the PyTorch version and the MoAI version required to execute it. In the example above, it indicates that PyTorch version 1.13.1+cu116 is running with MoAI version 24.3.0 installed.","If you encounter a conda: command not found message, or if the torch package is not listed, or if the torch package exists but does not include \"moreh\" in the version name, please follow the instructions in the Prepare Fine-tuning on MoAI Platform to create a conda environment. If the moreh version is not 24.3.0 but a different version, please execute the following code."]},{"l":"Verifying PyTorch Installation","p":["Run the following command to ensure that the torch package is imported correctly and the MoAI Accelerator is recognized."]},{"l":"Download the Training Script","p":["Execute the following command to download the PyTorch script for training from the GitHub repository. In this tutorial, we will be using the train_baichuan2_13b.py script located inside the tutorial directory."]},{"l":"Install Required Python Packages","p":["Execute the following command to install third-party Python packages required for script execution:"]},{"l":"Download Training Data","p":["Execute the following command to install third-party Python packages required for script execution:","To download the training data for this tutorial, we'll use the prepare_baichuan_dataset.py script located inside the dataset directory. When you run the code, it will download the Bitext-custormer-support-llm-chatbot dataset, and preprocess it for training, and save it as baichuan_dataset.pt.","The preprocessed dataset is saved as baichuan_dataset.pt.","Then, You can load the stored dataset in your code like this:"]}],[{"l":"2. Understanding training code","p":["If you've prepared all the training data, let's now take a look at the contents of train_baichuan2_13b.py script for the actual fine-tuning process. In this step, you'll notice that MoAI Platform ensures full compatibility with PyTorch, confirming that the training code is identical to the typical PyTorch code for NVIDIA GPUs. Additionally, you'll explore how efficiently MoAI Platform implements complex parallelization techniques beyond this.","First and foremost, it's recommended to proceed with the tutorial using the provided script as is until the end. Afterwards, you can modify the script as you wish to fine-tune the Baichuan model in different ways. If needed, refer to the LLM Fine-tuning Parameter Guide."]},{"l":"Training Code","p":["All the code is exactly the same as when using PyTorch conventionally.","First, import the necessary modules from the transformers library.","Load the model configuration and checkpoint from HuggingFace.","Load the preprocessed dataset saved during the 1. Preparing for Fine-tuning and define the data loader.","Subsequent training proceeds just like any other model training with PyTorch.","As shown above, with MoAI Platform, you can use your existing PyTorch scripts without any modifications."]},{"l":"About Advanced Parallelism","p":["The training script used in this tutorial includes the following additional line of code, which performs automatic parallelization provided by MoAI Platform.","For huge language models like Baichuan2 13B, it's inevitable to train them using multiple GPUs. In such cases, if you're not using MoAI Platform, you'll need to introduce parallelization techniques like Data Parallel, Pipeline Parallel, and Tensor Parallelism.","For instance, if you want to apply DDP in your PyTorch code, you would need to add the following code snippet: ( https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","DDP can be relatively easy to apply, but implementing techniques like pipeline parallelism or tensor parallelism requires quite complex code modifications. To apply optimized parallelization, you need to understand how Python code acts in a multiprocessing environment while writing the training scripts. Especially in multi-node setups, configuring the environment of each node used for training is necessary. Furthermore, finding the optimal parallelization method considering factors such as model type, size, and dataset requires a considerable amount of time.","On the other hand, MoAI Platform's AP feature enables users to proceed with optimized parallelized training with just one line of code added to the training script, eliminating the need for users to manually apply additional parallelization techniques.","Experience the optimal automated distributed parallel processing that is only possible with MoAI Platform's Advanced Parallelization (AP) feature, unlike anything you've encountered in other frameworks. With AP, you can easily configure the optimal parameters and environment variables for pipeline parallelism and tensor parallelism, typically required for training large-scale models, with just a single line of code."]}],[{"l":"3. Model fine-tuning","p":["Now, we will train the model through the following process."]},{"l":"Setting Accelerator Flavor","p":["AMD MI210 GPU with 32 units.","AMD MI250 GPU with 16 units:","AMD MI300X GPU with 8 units.","Enter 8 to use .","Enter q to complete the change.","First, we'll use the moreh-smi command to check the currently used MoAI Accelerator.","In MoAI Platform, physical GPUs are not directly exposed to users. Instead, virtual MoAI Accelerators are provided, which are available for use in PyTorch. By setting the accelerator's flavor, you can determine how much of the physical GPU will be utilized by PyTorch. Since the total training time and GPU usage cost vary depending on the selected accelerator flavor, users should make decisions based on their training scenarios. Refer to the following document to select the accelerator Flavor that aligns with your training objectives.","In this tutorial, we will use a 2048GB-sized MoAI Accelerator.","KT Hyperscale AI Computing (HAC) AI Accelerator Information","LLM Fine-tuning Parameter Guide","Now you can see that it has been successfully changed to .","Please refer to the document above or reach out to your infrastructure provider to inquire about the GPU types and quantities corresponding to each flavor.","Remember we checked the MoAI Accelerator in the previous Baichuan2 Finetuning step? Now, let's set up the required accelerators for the actual training process.","Select when using KT Cloud's Hyperscale AI Computing.","Select when using Moreh's trial container.","The current MoAI Accelerator in use has a memory size of 256GB.","Therefore, after switching from the initially set flavor to , we will use the moreh-smi command to confirm that the change has been successfully applied.","To confirm that the changes have been successfully applied, use the moreh-smi command again to check the currently used MoAI Accelerator.","You can choose one of the following flavors to proceed:","You can enter the number to switch to a different flavor.","You can utilize the moreh-switch-model command to review the available accelerator flavors on the current system. For seamless model training, consider using the moreh-switch-model command to switch to a MoAI Accelerator with larger memory capacity."]},{"l":"Training Execution","p":["Execute the train_baichuan2_13b.py script below.","If the training proceeds smoothly, you should see the following logs. By going through this logs, you can verify that the Advanced Parallelism feature, which determines the optimal parallelization settings, is functioning properly. It's worth noting that, apart from the single line of AP code we looked at earlier in the PyTorch script, there is no handling for using multiple GPUs simultaneously in other parts of the script.","You can confirm that the training is progressing smoothly by observing the loss values decreasing as follows.","The throughput displayed during training indicates how many tokens per second are being processed through the PyTorch script.","When using 8 AMD MI250 GPUs: approximately 191605 tokens/sec","Approximate training time based on GPU type and quantity is as follows:","When using 8 AMD MI250 GPUs: approximately 30 minutes"]},{"l":"Checking Accelerator Status During Training","p":["During training, open another terminal and connect to the container. You can execute the moreh-smi command to observe the MoAI Accelerator occupying memory while the training script is running. Please check the memory occupancy of MoAI accelerator when the training loss appears in the execution log after the initialization process."]}],[{"l":"4. Checking Training Results","p":["Similar to the previous chapter, when you execute the train_baichuan2_13b.py script, the resulting model will be saved in the baichuan_code_generation directory. This model, stored as a pure PyTorch parameter file, is fully compatible not only with the MoAI Platform but also with regular GPU servers.","You can test the trained model using the inference_baichuan.py script located under the tutorial directory of the GitHub repository you downloaded earlier.","Run the code below.","Upon inspecting the output, you can verify that the model has generated appropriate responses to the prompts."]}],[{"l":"5. Changing the Number of GPUs","p":["Let's rerun the fine-tuning task with a different number of GPUs. MoAI Platform abstracts GPU resources into a single accelerator and automatically performs parallel processing. Therefore, there is no need to modify the PyTorch script even when changing the number of GPUs."]},{"l":"Changing the Accelerator type","p":["Switch the accelerator type using the moreh-switch-model tool. For instructions on changing the accelerator, please refer to the 3. Model fine-tuning","Please contact your infrastructure provider and choose one of the following options before proceeding. KT Hyperscale AI Computing(HAC) AI Accelerator Information","AMD MI250 GPU with 32 units","When using Moreh's trial container: select","When using KT Cloud's Hyperscale AI Computing: select","AMD MI210 GPU with 64 units","AMD MI300X GPU with 16 units"]},{"l":"Training Parameters","p":["Run the train_baichuan2_13b.py script again.","Since the available GPU memory has doubled, let's increase the batch size to 2048 and run the training.","f the training proceeds normally, you should see the following log:","Upon comparison with the results obtained when the number of GPUs was halved, you'll notice that the training progresses similarly, with an improvement in throughput.","When using AMD MI250 GPU 16 → 32 : approximately 198,000 tokens/sec → 370,000 tokens/sec"]}],[{"l":"6. Conclusion","p":["So far, we've explored the process of fine-tuning the Baichuan2 13B model, which is publicly available on Hugging Face, using the MoAI Platform. With MoAI Platform, you can easily fine-tune PyTorch-based open-source LLM models on GPU clusters while keeping your existing training code intact. Moreover, with MoAI Platform, you can effortlessly configure the number of GPUs you need without changing any code. So please dive in and develop new models quickly and effortlessly with your data!","In case if you still have any questions regarding this tutorial feel free to ask Moreh( support@moreh.io)."]},{"l":"Learn More","p":["MoAI Platform's Advanced Parallelization(AP)","Llama2 Fine-tuning","Mistral Fine-tuning","GPT Fine-tuning","Qwen Fine-tuning"]}],[{"l":"Supported Documents","p":["Moreh Toolkit Guide","Prepare Fine-tuning on MoAI Platform","Advanced Parallelization (AP)","LLM Fine-tuning Parameter guide","KT Hyperscale AI Computing (HAC) AI Accelerator Information"]}],[{"l":"Moreh Toolkit Guide","p":["The Moreh Toolkit is a command line tool designed to efficiently manage and monitor MoAI Accelerators on the MoAI Platform. With just three commands ( moreh-smi, moreh-switch-model, update-moreh), users can effectively manage MoAI Accelerators and easily update MoAI Platform."]},{"l":"Key Features","p":["The main features of the Moreh Toolkit are as follows:","Monitoring MoAI Accelerators:","Use the moreh-smi command to monitor memory usage and process status in real-time.","Switching AI Accelerators:","Use the moreh-switch-model command to change AI accelerators and execute processes to achieve optimal performance.","Updating and Rolling Back MoAI Platform:","Use the update-moreh command to update Moreh solutions to the latest version or roll back to a previous version if needed."]},{"i":"monitoring-moai-accelerators-moreh-smi","l":"Monitoring MoAI Accelerators: moreh-smi","p":["The moreh-smi command allows users to manage and monitor MoAI Accelerators. You can run it in a conda environment where MoAI Platform PyTorch is installed.","If you are currently running training using the MoAI Accelerator, executing moreh-smi in a separate terminal session will display information about the running processes. Additionally, by using moreh-smi, you can quickly identify your Job ID. If you encounter any issues during training or inference on the MoAI Platform, providing your Job ID when contacting customer support will facilitate a faster response."]},{"l":"Utilizing Multi-Accelerator Feature","p":["By default, if users do not make any additional settings, there will be only one MoAI Accelerator in a single SSH environment. Typically, with one MoAI Accelerator, only one process can be executed at a time, allowing for only one process to run in a single SSH environment.","However, there may be scenarios where users want to leverage multiple MoAI Accelerators within the same SSH environment to run multiple processes simultaneously (e.g., running multiple training experiments concurrently with the same source code but different hyperparameters). In such cases, using moreh-smi to create multiple MoAI Accelerators within a single token enables running multiple processes concurrently.","Let's go through an example of adding, changing, and removing AI accelerators."]},{"l":"Adding AI Accelerators","p":["First, let's add an AI accelerator. To use two or more AI accelerators, you can enter the moreh-smi device --add command, which will display the following interface.","Enter an integer corresponding to the model you want to use from 1 to 13, and a message \"Create device success.\" will appear, indicating that the AI accelerator corresponding to the entered device number has been created. Up to 5 AI accelerators can be created within a single VM.","In the example below, let's add the AI accelerator with model number 10."]},{"l":"Changing Default AI Accelerator","p":["The moreh-smi device --switch {Device_ID} command allows you to change the default MoAI Accelerator.","Here's how to use it:","You can see that the default MoAI Accelerator has been changed to accelerator #1 in the example above."]},{"l":"Changing AI Accelerators with moreh-switch-model","p":["moreh-switch-model is a tool that allows you to change the flavor (specification) of the currently configured MoAI Accelerator. By changing the flavor of the MoAI Accelerator, you determine how much GPU memory to use.","Here's how to use it:","For example, if the result of the moreh-smi command is as follows, it means \"The currently configured MoAI Accelerator is accelerator 0, and the type of this MoAI Accelerator is the Small.64GB model.\"","When you use the moreh-switch-model command, an input prompt will appear like below:","Enter an integer (device number) corresponding to the model you want to use from 1 to 13, and the MoAI Accelerator will be switched to the MoAI Accelerator corresponding to the entered device number with the message \"The MoAI Platform AI Accelerator model is successfully switched to .\"","Let's change the MoAI Accelerator to the model with device number 3.","You can continue the change or exit the MoAI Accelerator change by entering q or Q.","After the change is complete, when you use moreh-smi again to check, the result will be as follows:","You can see that the MoAI Accelerator type has been changed from the model to the model."]},{"l":"Removing AI Accelerators","p":["Next, let's try deleting the accelerator corresponding to a specific device ID using the command moreh-smi device --rm {Device_ID}.","By entering the command above, the AI accelerator with Device ID 1 has been deleted. To confirm, run moreh-smi again to see that the device has been removed."]},{"l":"Utilizing Various Other Options","p":["Besides, moreh-smi provides various other options. By using the --help option, you can see what options are available:","moreh-smi -p- Monitor detailed hardware status of MoAI Accelerators.","moreh-smi -t- Check MoAI Accelerator token information.","moreh-smi --reset- Terminate MoAI Accelerator processes."]},{"l":"Updating MoAI Platform with update-moreh","p":["update-moreh is a command that allows you to create a new conda environment and install Moreh solutions on it or update the version of Moreh solutions already installed in the conda environment. You can use update-moreh in the following situations:","When you create a new conda environment, you need to install the required Python packages for Moreh solutions. In this case, you can easily install the latest version of Moreh solutions using the update-moreh command.","If you want to use the latest version of Moreh solutions in an already installed conda environment, you can update the currently installed Moreh solutions to the latest version using the update-moreh command alone.","Sometimes, you may need to install a specific version of Moreh solutions. In this case, you can use the --target option to specify the specific version you want to install.","If MoAI Platform does not work properly due to dependency conflicts between different packages in the conda environment, you might have to reinstall the conda environment. In such cases, you can use update-moreh to restore the Moreh solutions in the conda environment. In the latter case, you can use the --force option to rebuild the environment. (Can be used with the --target option as well)"]}],[{"l":"Prepare Fine-tuning on MoAI Platform","p":["The MoAI Platform can be customized with various GPUs while maintaining a consistent user experience via a command line interface(CLI). This uniform access ensures all users interact with the system in the same way, making it more efficient and intuitive.","Similar to general AI training environments, the MoAI Platform supports Python-based programming. This document focuses on setting up and using a conda virtual environment as the standard configuration for AI training."]},{"l":"Setting up a Conda Environment","p":["To begin training, first create a conda environment:","Replace my-env with your desired environment name.","Activate the conda environment:","Install PyTorch. The MoAI Platform supports various PyTorch versions, allowing you to choose the one that fits your needs.","Use the moreh-smi command to check the version of the installed Moreh solution and the details of the MoAI Accelerator in use. The current MoAI Accelerator is For more information about the MoAI Accelerator, refer to the specifications.","For optimal parameters recommended for fine-tuning each model on the MoAI Platform, refer to the LLM Fine-tuning parameter guide","For detailed usage of the moreh toolkit, including moreh-smi and moreh-switch-model, please refer to the Using the MoAI Platform Toolkit"]}],[{"i":"advanced-parallelizationap","l":"Advanced Parallelization(AP)"},{"i":"what-is-advanced-parallelization","l":"What is Advanced Parallelization?","p":["Advanced Parallelization (AP) is an optimized automated distributed parallel processing feature provided by the MoAI Platform. Typically, ML engineers go through numerous trial-and-error processes to optimize model parallelization when training large models. This involves considering the memory size of the GPUs being used, directly applying various parallelization techniques, and measuring the performance of different combinations of options within each technique to determine the optimal configuration.","However, with the AP feature offered by the MoAI Platform, you can easily apply optimized parallelization techniques, significantly reducing the time and effort required before starting the training process.","Moreh의 AP 기능은 기존 최적화 과정을 자동화 함으로써, 최적의 병렬화 환경 변수 조합을 신속하게 결정합니다. 따라서 대규모 모델 훈련시 적용하는 효율적인 Pipeline Parallelism, Tensor Parallelism의 최적 매개변수와 환경 변수 조합을 간단히 얻을 수 있습니다."]}],[{"i":"advanced-parallelismap","l":"Advanced Parallelism(AP)","p":["By default, AP(Advanced Parallelism) operates on a node-by-node basis. Therefore, a multi-GPU environment is required to use AP. Before proceeding with the AP feature, please review your current accelerator information using the guide below. For detailed information on accelerator sizes, refer to the KT Hyperscale AI Computing (HAC) Service Accelerator Model Information."]},{"l":"How to Apply AP","p":["There are two ways to apply the AP feature:","Add a single line of code.","실행 코드에 다음 한줄을 추가하여 AP 기능을 킬 수 있습니다. (이를 주석처리하면 끌 수 있습니다.)","환경 변수로 입력하기","다음과 같이 터미널 세션의 환경변수로 AP 기능을 킬 수 있습니다. ( 0으로 설정하면 끌 수 있습니다.)"]},{"l":"사용 예시 살펴보기","p":["-","1024","13,015,864,320","4xlarge","64","batch size","num params","Pytorch 환경 설정이 되었다면, Github 레포지토리에서 학습을 위한 코드를 가져옵니다.","quickstart 레포지토리를 클론하여 quickstart/ap-example 디렉토리를 확인해보시면 Moreh에서 미리 준비한 AP기능 test를 위한 text_summarization_for_ap.py 를 확인하실 수 있습니다. 이 코드를 기반으로 AP 기능을 적용해봅시다.","sda","sequence length","text_summarization_for_ap.py(전체코드 제공)","먼저 AP를 적용시키는 부분이 어디인지 python 프로그램에서 확인해보시죠.","사용자가 2대 이상의 노드를 사용하는 환경이 준비 되었다면 이제 AP 기능을 사용하기 위한 학습 코드를 만들어 보겠습니다. 이 가이드에서는 Llama2 모델을 활용하여 코드를 세팅합니다. 참고로, Llama2 모델은 커뮤니티 라이센스 동의와 Hugging Face 토큰 정보가 필요합니다. 1. Fine-tuning 준비하기 를 참고하여 학습 코드를 준비해주세요.","테스트를 위한 학습 구성은 다음과 같습니다. 이를 토대로 테스트를 진행하겠습니다.","학습 코드가 준비되었다면, MoAI Platform에서 학습을 실행하기 전 아래와 같이 pytorch 환경을 설정합니다. 아래 예시의 경우 PyTorch 1.13.1+cu116 버전을 실행하는 MoAI의 24.2.0 버전이 설치되어 있음을 의미합니다. 자세한 설명은 1. Fine-tuning 준비하기 튜토리얼을 참고해주시기 바랍니다."]},{"l":"AP 기능 ON","p":["프로그램의 main 함수 시작 지점에 AP 기능을 켜는 line이 있습니다. 다음과 같이 AP를 적용한 후 학습을 실행합니다.","학습이 종료되면 다음과 같은 로그를 확인할 수 있습니다.","이처럼 단 한 줄의 AP 기능 프로그램을 추가하여 복잡한 분산 병렬처리가 수행되어 학습이 진행된 것을 확인할 수 있습니다. AP 기능을 적용하여 손쉬운 병렬화가 가능했는데요, 만약 사용자가 AP 기능을 사용하지 않았을 때는 어떤 경험을 하게 될까요?"]},{"l":"AP 기능 OFF","p":["이를 확인할 수 있도록 AP를 켜지 않았을 때의 형상을 보여 드리겠습니다. 다시 python 프로그램의 main 함수 시작 지점에 AP 기능을 켜는 line을 주석처리하여 AP 기능을 끄겠습니다.","그 다음 학습을 진행합니다.","학습이 종료되면 다음과 같은 로그를 확인할 수 있습니다.","위 로그에서 RuntimeError: Error Code 4: OUT_OF_MEMORY 라는 메시지를 볼 수 있는데, 이것이 바로 앞서 말씀드린 1 device chip의 VRAM인 64GB가 넘는 데이터를 로드 할 수 없기 때문에 발생하는 OOM 에러입니다.","MoAI Platform 이 아닌 다른 프레임워크를 사용한다면 이런 불편함을 겪어야 합니다. 그러나 MoAI Platform을 사용하는 사용자라면 별도 병렬화 최적화를 위해 오랫동안 계산하며 고민하는 시간을 들이지 않고 AP 기능 한줄을 적용하여 골치아픈 OOM 문제를 해결할 수 있습니다. 정말 편리한 기능이죠?"]}],[{"l":"LLM Fine-tuning Parameter guide","p":["1,122,745 MiB","1,138,546 MiB","1,403,047 MiB","1,651,008 MiB","1,680,233 MiB","1,706,797 MiB","1,764,955 MiB","1,767,888 MiB","1,800,656 MiB","1024","109872","11,562","11m","121,013","125,180","128","1292886 MiB","12m","13286","1360m","13m","1403,2189","144,124","1467646 MiB","1489,3","15,890","15,972","154,12123","157,859","16","1600235 MiB","163,839","172395","17m","186,353","191605","194,282","2,146,115 MiB","2,645,347 MiB","2,800,000 MiB","2,845,656 MiB","2048","20m","22m","238,212","24,156","24.2.0","24.3.0","24.5.0","24m","256","25m","26,111","27B","28m","2xlarge","3,460,240 MiB","3013826 MiB","30m","3143781 MiB","3181616 MiB","32","32,371","32,563","34m","35m","36m","376,493","38m","400m","4096","40m","442,982 MiB","47,679","480m","4xlarge","50,782","51,353","512","543816 MiB","56,385","560,835 MiB","560m","58531","586m","59m","62,481","62,582","62,740","626,391 MiB","62m","63,893","638,460 MiB","64","65,565","6841","69,840","720m","749065 MiB","784,485 MiB","790,572 MiB","790454 MiB","7m","8,934","81m","843,375 MiB","858,128 MiB","866,656 MiB","872m","8m","8xlarge","92,623","93,165","962m","99,873","9m","Advanced Parallelism is applied","Baichuan2 13B","batch size","Cerebras GPT 13B","Keep in mind that the name of the MoAI Accelerator mentioned here might differ based on the CSP utilized by user.","Llama2 13B","Mistral 7B","MoAI Accelerator","MoAI Platform version","Model","Number of Tokens","Qwen1.5 7B","sequence length","This guide provides optimal parameters offered by the MoAI Platform and should be used solely as a helpful reference during the training.","throughput","Training Time","True","VRAM Usage"]}],[{"i":"kt-hyperscale-ai-computing-hac-ai-accelerator-information","l":"KT Hyperscale AI Computing (HAC) AI Accelerator Information","p":["…","1","12","12xLarge.6144GB","2","24","24xLarge.12288GB","2xLarge.1024GB","3","3xLarge.1536GB","4","48","48xLarge.24576GB","4xLarge.2048GB","6","6xLarge.3072GB","8","8xLarge.4096GB","Large.256GB","Medium.128GB","MI250 0.5","MI250 1","MI250 12","MI250 16","MI250 192","MI250 2","MI250 24","MI250 32","MI250 4","MI250 48","MI250 8","MI250 96","Model","Number of Nodes","Physical GPUs","Small.64GB","The current HAC service is powered by AMD MI250 GPUs. While performance may vary depending on the model/application, you can generally expect the AMD MI250 to deliver performance comparable to that of a single NVIDIA A100.","The official KT Cloud Manual","xLarge.512GB"]}]]