window.__DOCS_SEARCH__ = [[{"l":"MOREH DOCS","p":["MoAI(Moreh AI appliance for AI accelerators) Platform is a scalable AI platform that allows for easy control of thousands of Graphics Processing Units (GPUs), which are required for developing large-scale deep learning models."]},{"l":"Getting Started","p":["Get started with fine-tuning MoAI Platform Beginner's Guide for Finetuning","AP Guide Advanced Parallelization (AP) Feature Instructions","Moreh Toolkit Guide Command Line Usage","MoAI Platform Features The virtualization and parallelization features of the MoAI Platform","MoAI Platform is currently in ongoing development. Therefore, the contents of the document may change at any time."]},{"l":"Core Technologies of MoAI Platform","p":["As deep learning models progress, they become increasingly complex and require significant computational resources as their parameters expand from billions to trillions. Developing large-scale models involves handling and processing an enormous volume of parameters, a task that is both daunting and time-consuming.","The MoAI Platform's automatic parallelization addresses these hurdles by simultaneously processing multiple tasks, determining the optimal computation methods for large models. This empowers AI engineers to concentrate solely on their core AI endeavors, regardless of the scale of their applications or the type of processors they use. Moreover, it efficiently utilizes GPU computational resources at a reasonable cost by allocating them only during calculation execution.","Support Various Accelerators with Single GPU Abstraction","GPU Virtualization","Dynamic GPU Allocation","AI Compiler Automatic Parallelization"]}],[{"l":"Overview"},{"i":"what-is-moai-platform","l":"What is MoAI Platform?","p":["The MoAI Platform is an AI platform for the development of large-scale deep learning models, simplifying the management of thousands of GPU clusters necessary for training and inference of AI models."]},{"l":"Core Technologies of MoAI Platform","p":["As deep learning models advance, they become increasingly intricate, comprising numerous layers with a vast number of parameters. Consequently, large-scale computing resources have become essential elements of AI infrastructure. Utilizing these resources to develop models entails optimizing the training process, including parallelizing the model and manually configuring the cluster environment. Effectively managing GPU and node resources for optimal training demands considerable time and effort from developers.","To address these challenges, the MoAI Platform offers the following features to facilitate efficient infrastructure in the era of large-scale AI:","Support Various Accelerators with Single GPU Abstraction","GPU Virtualization","Dynamic GPU Allocation","AI Compiler Automatic Parallelization"]},{"l":"1. Support Various Accelerators with Single GPU Abstraction","p":["MoAI Platform supports various AI accelerators, allowing users to execute diverse model training and inference tasks regardless of accelerator type. Users can seamlessly utilize accelerators other than NVIDIA without the need to modify existing code written in Python."]},{"l":"2. GPU Virtualization","p":["The virtualization feature of the MoAI Platform enables thousands of GPUs to be utilized as a singular unit, known as a Single Virtual Device. This allows AI engineers to easily and quickly conduct deep learning training and inference tasks without the complexities associated with multi-GPU and multi-node parallelization. By assuming only one GPU and structuring Python code accordingly, users can perform these tasks effortlessly.","Users can expand or shrink GPU resources as needed, increasing the scalability of the service. With a simple one-line command in the MoAI Platform, GPU resources virtualized into a single virtual device can be easily expanded or contracted."]},{"l":"3. Dynamic GPU Allocation","p":["In public cloud, billing starts when VM instances are created, and changing GPUs requires recreating the instances. Furthermore, once selected, it can be challenging to flexibly change the chosen virtual machine, which can hinder optimization according to the user's needs.","The MoAI Platform is designed to charge fees on a per-minute basis only when AI accelerators are actually in operation, allowing for a complete pay-as-you-go system. This design enables significant cost savings compared to existing cloud services by freeing GPUs from dependency on specific virtual machines (VMs) according to user usage patterns."]},{"l":"4. AI Compiler Automatic Parallelization","p":["Deep learning models consist of multiple layers, each containing various operations. These operations can be learned independently, enabling parallel processing. However, ML engineers must manually configure combinations of parameters and environment variables for this purpose. The MoAI Platform's automatic parallelization feature swiftly determines the optimal combination of parallelization environment variables. As a result, users can train models automatically applying parallelization techniques such as Data Parallelism, DDP, Pipeline Parallelism, and Tensor Parallelism during large-scale model training.","In the era of artificial intelligence, training and inference of large-scale models such as Large Language Models (LLMs) and Large Multimodal Models (LMMs) require substantial GPU clusters and effective GPU parallelization.","Current mainstream AI frameworks, often used alongside NVIDIA, require AI engineers to manually adjust parallelization based on the size and complexity of the model, as well as the available GPU size or cluster. This process is time-consuming and can often take several weeks.","The MoAI platform offers automatic parallelization through the Moreh AI compiler, which optimizes GPU resources based on the size of a specific AI model and GPU cluster.","Automatic parallelization can drastically reduce the training time for models that typically take weeks in NVIDIA environments (platforms) to approximately 2-3 days."]}],[{"l":"MoAI Platform Features","p":["The MoAI Platform handles hundreds of GPUs as a single accelerator through virtualization and reduces the burden of managing complex GPU clusters through parallelization.","The document below explains the operating principles and concepts of the two functions provided by the MoAI platform.","The following documents describe the operating mechanisms and concepts of two key features provided by the MoAI Platform.","GPU Virtualization","Parallelization"]}],[{"i":"gpu-virtualization-moai-accelerator","l":"GPU Virtualization: MoAI Accelerator","p":["The MoAI Platform virtualizes large GPU clusters, consisting of dozens or hundreds of GPU nodes, into a single accelerator called the MoAI Accelerator. This allows users to design and train models as if they are using a single GPU, without worrying about model parallelization or manually configuring cluster environments.","You can check the MoAI Accelerator status by entering the moreh-smi command in the terminal.","The output shows that the user is utilizing a single accelerator with 2048 GB of memory. However, in reality, it consists of 4 nodes, each with 4 GPUs.","Let's verify if the MoAI Accelerator is recognized correctly in PyTorch, one of the most widely used deep learning frameworks. Using the cuda API in the Python interpreter, we can see that PyTorch recognizes the MoAI Accelerator as a single device.","An important point to note is that there are no physical GPUs in the user's environment. When the user attempts to use GPU accelerators through APIs like cuda in deep learning frameworks such as PyTorch, the MoAI Platform automatically allocates GPU cluster resources."]},{"l":"Dynamic GPU Allocation on the MoAI Platform","p":["The MoAI Platform dynamically handles GPU allocation at the process level. This ensures that users efficiently receive physical GPU allocations while training and inferencing models using frameworks like PyTorch. It also provides flexibility, allowing users to select and adjust the number of GPUs from various pre-defined MoAI Accelerator flavors as needed.","In contrast, traditional cloud platforms typically allocate physical GPUs statically from the moment an instance is created. If users wish to change the number of GPUs or stop using them, they need to delete the existing instance or terminate the container and restart it. The MoAI Platform's dynamic allocation significantly reduces this inconvenience.","Let's go through a simple example of changing the MoAI Accelerator flavor.","First, check the current MoAI Accelerator in use by entering the moreh-smi command in the terminal.","You can see that the current flavor of the MoAI Accelerator being used is . If you need to train a larger model or want to use more GPUs to speed up training, you can easily switch the flavor by entering the moreh-switch-model command.","After entering the moreh-smi command again to check the current MoAI Accelerator flavor, you can see it has been successfully changed to ."]},{"l":"Conclusion","p":["The MoAI Platform simplifies the complexity of multi-node GPU clusters through virtualization technology known as the MoAI Accelerator. It provides users with a powerful yet flexible computing environment. By allowing users to adjust model size and the number of GPUs without complex settings and management tasks, it enables efficient resource utilization. Use the MoAI Accelerator on the MoAI Platform to design and train deep learning models quickly and efficiently."]}],[{"l":"Automatic Parallelization","p":["On the MoAI Platform, users interact with the MoAI Accelerator, which is presented as a single virtualized GPU. This means users can write code assuming the use of a single GPU without needing to worry about parallelization. But how does the MoAI Platform handle multiple GPUs?","The MoAI Platform automatically optimizes and parallelizes based on the number of GPUs in use.","For instance, if you start training with an accelerator flavor that uses 8 GPUs, the MoAI Platform automatically divides the total batch size into 8 parts, distributing them across each GPU. Let's take an example of fine-tuning the Llama3-8b model. If you select an accelerator flavor with 4 GPUs and set a batch size of 16, the platform automatically assigns 4 batches per GPU, processing approximately 125,000 tokens per second.","For faster and more efficient training, you can select an accelerator flavor with more GPUs and increase the batch size. If you choose an accelerator flavor with 16 GPUs and set the batch size to 64, the number of tokens processed per second can increase fourfold compared to the previous setup.","But what if you want to use an even larger batch size? Without additional code modifications, a typical GPU cluster might run into Out of Memory (OOM) errors. However, the MoAI Platform can automatically parallelize the model to continue training.","When you choose an accelerator flavor with 16 GPUs and set the batch size to 512, the platform applies model parallelization and data parallelization simultaneously. This allows training with a larger batch size using the same number of GPUs.","Additionally, even large models like the 70B model can be automatically parallelized without any extra work, making training straightforward. The MoAI Platform provides automatic optimization and parallelization based on the model and batch size, enabling convenient and efficient use of multiple GPUs."]}],[{"l":"Fine-tuning Tutorials","p":["This tutorial is for anyone who wants to fine-tune powerful large language models such as Llama2, Mistral for their own projects. We will walk you through the steps to fine-tune these large language models (LLMs) with MoAI Platform.","Llama3 8B","Llama3 70B","Mistral","GPT","Qwen","Baichuan2","Llama2 13B","Fine-tuning in machine learning involves adjusting a pre-trained machine learning model's weight on new data to enhance task-specific performance. Essentially, when you want to apply an AI model to a new task, you take an existing model and optimize it with new datasets. This allows you to customize the model to meet your specific needs and domain requirements.","A pre-trained model has a large number of parameters designed for general-purpose use, and effectively fine-tuning such a large model requires a sufficient amount of training data.","With the MoAI Platform, you can easily apply optimized parallelization techniques that consider the GPU's memory size, significantly reducing the time and effort needed before starting training."]},{"i":"what-you-will-learn-here","l":"What you will learn here:","p":["Loading datasets, models, and tokenizers","Running training and checking results","Applying automatic parallelization","Choosing the right training environment and AI accelerators"]}],[{"l":"Llama3 8B Fine-tuning","p":["This tutorial introduces an example of fine-tuning the open-source Llama3-8B model on the MoAI Platform. Through this tutorial, users can experience various features provided by the MoAI Platform and learn how to use an AMD GPU cluster.","Users can easily run training without complex parallelization tasks or cluster environment setups, as they can treat dozens of GPUs as a single accelerator called MoAI Accelerator. This allows users to focus solely on training without worrying about resource management.","Thanks to the automatic parallelization feature, code writing and development are simplified, and model training speed is significantly improved. This enables efficient resource utilization, allowing users to work faster and more effectively."]},{"l":"Overview","p":["The MoAI Platform is a scalable AI platform that enables easy control of thousands of GPUs for training and inference of AI models. One of its key features is providing a very simple training method through virtualization and parallelization when fine-tuning models.","The MoAI Platform provides multiple GPUs virtualized into a single accelerator called MoAI Accelerator. Therefore, there is no need for preprations or code modifications for using multiple GPUs.","The MoAI Platform automatically provides optimized parallelization when users use the virtualized MoAI Accelerator. It considers various parallelization methods based on model and data sizes to offer the optimal parallelization environment. As a result, users can experience high-performance training with simple code without any additional tasks."]},{"l":"Getting Started","p":["Please obtain a container or virtual machine on the MoAI Platform from your infrastructure provider and follow the instructions to connect via SSH. For example, you can apply for a trial container on the MoAI Platform or use public cloud services based on the MoAI Platform.","MoAI Platform Trial Container (Inquiries: support@moreh.io)","KT Cloud's Hyperscale AI Computing ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","After connecting via SSH, execute the moreh-smi command to verify that the MoAI Accelerator is properly detected. The device name may vary depending on the system."]},{"l":"Verifying MoAI Accelerator","p":["To train models like the sLLM introduced in this tutorial, it's important to select an appropriate size of the MoAI Accelerator. First, use the moreh-smi command to check the currently used MoAI Accelerator.","Detailed instructions for setting up the MoAI Accelerator specific to your training needs will be provided in the section 3. Model fine-tuning."]}],[{"l":"1. Preparing for Fine-tuning","p":["Setting up the PyTorch execution environment on the MoAI Platform is similar to setting it up on a typical GPU server. For a smooth tutorial experience, the following specifications are recommended:","CPU: 16 cores or more","Memory: 256GB or more","MAF version: 24.5.0","Storage: 40GB or more","Please verify that your environment meets these requirements before starting the tutorial."]},{"l":"Checking PyTorch Installation","p":["After connecting to the container via SSH, run the following command to check if PyTorch is installed in the current conda environment:","The version name includes both the PyTorch version and the MoAI version required to run it. In the example above, it indicates that PyTorch 1.13.1+cu116 is installed with MoAI version 24.5.0.","If you see the message conda: command not found, if the torch package is not listed, or if the torch package exists but does not include \"moreh\" in the version name, please follow the instructions in the Prepare Fine-tuning on MoAI Platform document to create a conda environment.","If the moreh version is not 24.5.0 but a different version, please execute the following code."]},{"l":"Verifying PyTorch Installation","p":["Run the following command to confirm that the torch package is properly imported and the MoAI Accelerator is recognized."]},{"l":"Download the Training Script","p":["Execute the following command to download the PyTorch script for training from the GitHub repository. In this tutorial, we will be using the train_llama3.py script located inside the tutorial directory."]},{"l":"Install Required Python Packages","p":["Execute the following command to install third-party Python packages required for script execution:"]},{"l":"Acquire Access to the Model","p":["To access and download the Llama3 8B model checkpoint from Hugging Face Hub, you will need to agree to the community license and provide your Hugging Face token information.","First, enter the required information and agree to the license on the following site.","meta-llama/Meta-Llama-3-8B · Hugging Face","After submitting the agreement, confirm that the page status has changed as shown below.","Now you can authenticate your Hugging Face token with the following command:"]}],[{"l":"2. Understanding Training Code","p":["Once you have prepared all the training data, let's take a look at the contents of the train_llama3.py script, which will carry out the actual fine-tuning process. This script executes fine-tuning based on the implementation of the Llama3 8B model in the Hugging Face Transformers library, using standard PyTorch code.","We recommend initially proceeding with the provided script as is until the end of the tutorial. Afterwards, feel free to modify the script as desired to fine-tune the Llama3 8B model in different ways. This flexibility is possible due to MoAI Platform's complete compatibility with PyTorch. If needed, refer to the MoAI Platform Application Guide provided by Moreh LLM Fine-tuning Parameter Guide."]},{"l":"Training Code","p":["All the code used during training is exactly the same as when you're using PyTorch in general.","Import the necessary modules from the transformers library.","Load the model configuration and checkpoint publicly available on Hugging Face.","Then load the training dataset from Hugging Face Hub, preprocess loaded dataset, and define the data loader.","Subsequently, the training proceeds similarly to regular PyTorch model training.","In this way, you can write code in MoAI Platform using the same approach as with standard PyTorch code."]},{"l":"About Advanced Parallelism","p":["In the training script used in this tutorial, there is an additional line of code as follows, which executes the top-tier parallelization feature provided by the MoAI Platform:","Training a large language model like Llama2 13B requires a significant amount of GPUs. Without using the MoAI Platform, you would need to implement parallelization techniques such as data parallelism, pipeline parallelism, and tensor parallelism to perform the training.","(Reference: https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","While DDP can be relatively easy to apply, implementing techniques like pipeline parallelism or tensor parallelism involves quite complex code modifications. To apply optimized parallelization, you need to understand how Python code acts in a multiprocessing environment while writing the training scripts. Especially in multi-node setups, configuring the environment of each node used for training is necessary. Additionally, finding the optimal parallelization method considering factors such as model type, size, and dataset requires a considerable amount of time.","In contrast, MoAI Platform's AP feature enables users to proceed with optimized parallelized training with just one line of code added to the training script, eliminating the need for users to manually apply additional parallelization techniques.","MoAI Platform's Advanced Parallelization (AP) provides optimization and automation features that are difficult to experience in other frameworks. With the AP feature, you can easily configure the optimal parameters and environment variables for Pipeline Parallelism and Tensor Parallelism which are typically required for large-scale model training, with just a single line of code."]}],[{"l":"3. Model Fine-tuning","p":["Now, we will actually execute the fine-tuning process."]},{"l":"Setting Accelerator Flavor","p":["AMD MI210 GPU with 32 units.","AMD MI250 GPU with 16 units:","AMD MI300X GPU with 8 units.","Enter 8 to use","Enter q to complete the change.","First, we'll use the moreh-smi command to check the currently used MoAI Accelerator.","In MoAI Platform, physical GPUs are not directly exposed to users. Instead, virtual MoAI Accelerators are provided, which are available for use in PyTorch. By setting the accelerator's flavor, you can determine how much of the physical GPU will be utilized by PyTorch. Since the total training time and GPU usage cost vary depending on the selected accelerator flavor, users should make decisions based on their training scenarios. If needed, please refer to the LLM Fine-tuning Parameter Guide to select the accelerator Flavor that aligns with your training objectives.","In this tutorial, we will use a 2048GB-sized MoAI Accelerator.","Now you can see that it has been successfully changed to .","Please refer to the document above or reach out to your infrastructure provider to inquire about the GPU types and quantities corresponding to each flavor.","Remember when we checked the MoAI Accelerator in the Llama3 8B Fine-tuning - Getting Started? Now let's set up the accelerator needed for learning.","Select when using KT Cloud's Hyperscale AI Computing.","Select when using Moreh's trial container.","The current MoAI Accelerator in use has a memory size of 512GB.","Therefore, after switching from the initially set flavor to , we will use the moreh-smi command to confirm that the change has been successfully applied.","To confirm that the changes have been successfully applied, use the moreh-smi command again to check the currently used MoAI Accelerator.","You can enter the number to switch to a different flavor.","You can utilize the moreh-switch-model command to review the available accelerator flavors on the current system. For seamless model training, consider using the moreh-switch-model command to switch to a MoAI Accelerator with larger memory capacity."]},{"l":"Training Execution","p":["Execute the train_llama3.py script below.","If the training proceeds smoothly, you should see the following logs. By going through this logs, you can verify that the Advanced Parallelism feature, which determines the optimal parallelization settings, is functioning properly. It's worth noting that, apart from the single line of AP code we looked at earlier in the PyTorch script, there is no handling for using multiple GPUs simultaneously in other parts of the script.","Upon checking the training logs, you can confirm that the training is progressing smoothly.","The throughput displayed during training indicates how many tokens per second the script is training through this PyTorch script.","When using AMD MI250 GPU with 16 GPUs: Approximately 200,000 tokens/sec","The approximate training time depending on the GPU type and quantity is as follows:","When using AMD MI250 GPU with 16 GPUs: Approximately 160 minutes"]},{"l":"Checking Accelerator Status During Training","p":["During training, open another terminal and connect to the container. Then, execute the moreh-smi command to observe the MoAI Accelerator occupying memory and the training script running. Make sure to check this while the initialization process is completed and the training loss appears in the execution logs."]}],[{"l":"4. Checking Training Results","p":["When you execute the train_llama3.py script as in the previous section, the resulting model will be saved in the llama3_summarization directory. This model is compatible with any GPU server, not just MoAI Platform, as it is stored as a pure PyTorch model parameter file.","You can test the trained model using the inference_llama3.py script located under the tutorial directory in the GitHub repository you downloaded earlier.","For testing, articles related to soldiers deployed in Iraq were used.","Run the code.","Upon examining the output, you will see that the model appropriately summarizes the contents of the input prompt."]}],[{"l":"5. Changing the Number of GPUs","p":["Let's rerun the fine-tuning task with a different number of GPUs. MoAI Platform abstracts GPU resources into a single accelerator and automatically performs parallel processing. Therefore, there is no need to modify the PyTorch script even when changing the number of GPUs."]},{"l":"Changing the Accelerator type","p":["Switch the accelerator type using the moreh-switch-model tool. For instructions on changing the accelerator, please refer again to the 3. Model fine-tuning.","Please contact your infrastructure provider and choose one of the following options before proceeding.","AMD MI250 GPU with 32 units","When using Moreh's trial container: select","When using KT Cloud's Hyperscale AI Computing: select","AMD MI210 GPU with 64 units","AMD MI300X GPU with 16 units"]},{"l":"Training Parameters","p":["Again, run the train_llama3.py script.","Since the available GPU memory has doubled, let's increase the batch size from the previous 256 to 1024 and run the code again.","If the training proceeds normally, you will see similar logs to the previous run but with improved throughput due to the doubled number of GPUs.","When using AMD MI250 GPU 16 → 32 : From approximately 200,000 tokens/sec to 390,000 tokens/sec."]}],[{"l":"6. Conclusion","p":["From this tutorial, we have seen how to fine-tune Llama3 8B on the MoAI Platform. Open-source LLMs like Llama can be utilized for various tasks such as summarization, question answering, and more. With the MoAI platform, you can easily configure the number of GPUs you need without any code changes.","The availability of large language models like LLaMA 3, fine-tuning techniques, and the MoAI Platform makes it possible for anyone to develop powerful AI applications. So please start developing models by repeating the process outlined here on your own data.","In case if you still have any questions regarding this tutorial feel free to ask Moreh( support@moreh.io)"]},{"l":"Learn More","p":["MoAI Platform's Advanced Parallelization (AP)","Llama3 70B Fine-tuning","Mistral Fine-tuning","GPT Fine-tuning","Baichuan2 Fine-tuning","Qwen Fine-tuning"]}],[{"l":"Llama3 70B Fine-tuning","p":["This tutorial introduces how to fine-tune the open-source LLama3-70B model using the MoAI Platform. By following this tutorial, you will learn how to utilize AMD GPU clusters with the MoAI Platform and experience the benefits of performance optimization and automatic parallelization."]},{"i":"moai-platforms-gpu-virtualization","l":"MoAI Platform’s GPU Virtualization","p":["The MoAI Platform is a scalable AI platform that allows for easy management of thousands of GPUs, facilitating the training and inference of AI models. One of the key features of the MoAI Platform is its ability to offer a simplified training process through virtualization and parallelization.","The MoAI Platform virtualizes multiple GPUs into a single entity known as the MoAI Accelerator. This approach makes it appear as though you are using just one GPU, eliminating the need for preparatory work or code modifications typically required for multi-GPU usage.","When using a virtualized MoAI Accelerator, the platform automatically optimizes parallelization internally. By considering the model size and data volume, it provides the optimal parallelization strategy, allowing users to achieve high-performance training with minimal additional effort and straightforward code."]}],[{"i":"1-moai-platforms-parallelization---llama3-70b","l":"1. MoAI Platform’s Parallelization - Llama3 70B","p":["To fine-tune the Llama3 70B model, you must use multiple GPUs and implement parallelization techniques such as Tensor Parallelism, Pipeline Parallelism, and Data Parallelism. This typically involves using tools like Deepspeed and configuring complex settings. For pipeline parallelism, a deep understanding of the model is necessary. Additionally, finding the most effective parallelization method requires repeatedly modifying code and training the model to discover the optimal configuration. This process demands significant effort from users to utilize multiple GPUs effectively.","However, the MoAI Platform simplifies this by providing automatic parallelization and optimal configurations without the need for extensive code modifications or in-depth model knowledge, allowing users to train models effortlessly."]},{"l":"Fine-tuning Code","p":["All code on the MoAI Platform operates the same way as standard PyTorch. You can write scripts for training Llama3 70B as if you were using a single GPU in PyTorch.","Unlike standard PyTorch, the MoAI Platform requires one additional line of code to enable automatic optimization and parallelization. This makes fine-tuning Llama3 70B straightforward.","Now, let’s start the fine-tuning tutorial for the Llama3 70B model using 64 MI250 GPUs on the MoAI Platform. Through this tutorial, you will see how easy and effective it is to use multiple GPUs on the MoAI Platform."]}],[{"l":"2. Preparing for Fine-tuning"},{"l":"Getting Started","p":["To start, you'll need to obtain a container or virtual machine on the MoAI Platform from your infrastructure provider. You can use public cloud services based on the MoAI Platform, such as:","MoAI Platform Trial Container (Inquiries: support@moreh.io)","KT Cloud Hyperscale AI Computing","After accessing the platform via SSH, run the moreh-smi command to ensure the MoAI Accelerator is properly recognized. Note that device names may vary depending on the system."]},{"l":"Verifying the MoAI Accelerator","p":["For this tutorial, which involves training a large-scale language model (LLM) like Llama3, selecting the appropriate size of MoAI Accelerator is crucial. First, use the moreh-smi command to check the current MoAI Accelerator in use.","Details on the specific MoAI Accelerator settings required for training will be provided in 3. Model Fine-tuning","Setting up the PyTorch script execution environment on the MoAI Platform is similar to working on a standard GPU server."]},{"l":"Checking PyTorch Installation","p":["Once you’ve accessed the container via SSH, check if PyTorch is installed in the current conda environment by running:","The version should display both the PyTorch version and the MoAI version it’s running on. For instance, 1.13.1+cu116 indicates PyTorch version 1.13.1 with CUDA 11.6, and MoAI version 24.5.0.","If you see a conda: command not found message, the torch package isn’t listed, or the torch package doesn’t include \"moreh\" in its version name, follow the instructions in the Prepare Fine-tuning on MoAI Platform document to create the conda environment."]},{"l":"Verifying PyTorch Functionality","p":["Run the following to ensure the torch package is properly imported and that the MoAI Accelerator is recognized:"]},{"l":"Downloading the Training Script","p":["Download the PyTorch script for training from the GitHub repository by running:","For this tutorial, we will use the train_llama3.py script located in the tutorial directory."]},{"l":"Installing Required Python Packages","p":["Install third-party Python packages needed to run the script by executing:"]},{"l":"Acquire Access to the Model","p":["To access and download the Llama3 70B model checkpoint from Hugging Face Hub, you will need to agree to the community license and provide your Hugging Face token information.","First, enter the necessary information and agree to the license on the Hugging Face website.","meta-llama/Meta-Llama-3-70B · Hugging Face","Once you've submitted the agreement form, check that the status on the page has updated as follows:","Now you can authenticate your Hugging Face token with the following command:"]}],[{"l":"3. Model Fine-tuning","p":["Now, we will actually execute the fine-tuning process."]},{"l":"Setting Accelerator Flavor","p":["Enter 10 to use .","Enter q to complete the change.","First, use the moreh-smi command to check the current MoAI Accelerator in use.","For this tutorial, we will use the 4096GB MoAI Accelerator.","In MoAI Platform, physical GPUs are not directly exposed to users. Instead, virtual MoAI Accelerators are provided, which are available for use in PyTorch. By setting the accelerator's flavor, you can determine how much of the physical GPU will be utilized by PyTorch. Since the total training time and GPU usage cost vary depending on the selected accelerator flavor, users should make decisions based on their training scenarios. If needed, please refer to the LLM Fine-tuning Parameter Guide to select the accelerator Flavor that aligns with your training objectives.","Please refer to the document above or reach out to your infrastructure provider for the types and numbers of GPUs corresponding to each flavor.","Remember when we checked the MoAI Accelerator in the Llama3 70B Fine-tuning - Getting Started? Now let's set up the accelerator needed for learning.","Select one of the following flavors to continue:","Select when using KT Cloud's Hyperscale AI Computing.","Select when using Moreh's trial container.","The current MoAI Accelerator in use has a memory size of 512GB.","Therefore, we will switch the initially set flavor to and then use the moreh-smi command to verify that the change has been applied correctly.","To confirm that the changes have been successfully applied, use the moreh-smi command again to check the currently used MoAI Accelerator.","Using 16 AMD MI300X GPUs","Using 32 AMD MI250 GPUs","Using 64 AMD MI210 GPUs","You can enter a number here to switch to a different flavor.","You can see that it has been successfully switched to .","You can utilize the moreh-switch-model command to review the available accelerator flavors on the current system. For seamless model training, consider using the moreh-switch-model command to switch to a MoAI Accelerator with larger memory capacity."]},{"l":"Training Execution","p":["Run the provided train_llama3_70b.py script.","If the training is running correctly, you will see logs similar to the following. These logs indicate that the Advanced Parallelism feature, which finds the optimal parallelization settings, is functioning correctly. Note that in the PyTorch script we reviewed earlier, no additional handling for using multiple GPUs simultaneously was necessary apart from the single AP code line.","From the training logs, you can confirm that the training is progressing smoothly.","The throughput displayed during training indicates the number of tokens being trained per second by the PyTorch script.","Using 32 AMD MI250 GPUs (64 devices): approximately 4062 tokens/sec","Estimated training time based on GPU type and count is as follows:","Using 32 AMD MI250 GPUs (64 devices): approximately 24 hours"]},{"l":"Checking Accelerator Status During Training","p":["During training, you can open another terminal and connect to the container. Then, run the moreh-smi command to see the MoAI Accelerator’s memory being utilized by the training script, as shown below."]}],[{"l":"4. Conclusion","p":["We have now explored the process of fine-tuning the Llama3-70b model on the MoAI Platform. With the MoAI Platform, you can effortlessly configure any number of GPUs—be it 1, 4, 100, or more—without modifying your code. Leverage your own data to develop new models quickly and easily."]},{"l":"Learn More","p":["MoAI Platform's Advanced Parallelization (AP)","Llama3 8B Fine-tuning","Mistral Fine-tuning","GPT Fine-tuning","Qwen Fine-tuning","Baichuan2 Fine-tuning"]}],[{"l":"Mistral Fine-tuning","p":["This tutorial guides you on fine-tuning the open-source Mistral 7B model on the MoAI Platform. Through this tutorial, users can experience various features provided by the MoAI Platform and learn how to use an AMD GPU cluster.","Users can easily run training without complex parallelization tasks or cluster environment setups, as they can treat dozens of GPUs as a single accelerator called MoAI Accelerator. This allows users to focus solely on training without worrying about resource management.","Thanks to the automatic parallelization feature, code writing and development are simplified, and model training speed is significantly improved. This enables efficient resource utilization, allowing users to work faster and more effectively."]},{"l":"Overview","p":["The MoAI Platform is a scalable AI platform that enables easy control of thousands of GPUs for training and inference of AI models. One of its key features is providing a very simple training method through virtualization and parallelization when fine-tuning models.","The MoAI Platform provides multiple GPUs virtualized into a single accelerator called MoAI Accelerator. Therefore, there is no need for preprations or code modifications for using multiple GPUs.","The MoAI Platform automatically provides optimized parallelization when users use the virtualized MoAI Accelerator. It considers various parallelization methods based on model and data sizes to offer the optimal parallelization environment. As a result, users can experience high-performance training with simple code without any additional tasks."]},{"l":"Getting Started","p":["Please obtain a container or virtual machine on the MoAI Platform from your infrastructure provider and follow the instructions to connect via SSH. For example, you can apply for a trial container on the MoAI Platform or use public cloud services based on the MoAI Platform.","MoAI Platform Trial Container (Inquiries: support@moreh.io)","KT Cloud's Hyperscale AI Computing ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","After connecting via SSH, execute the moreh-smi command to verify that the MoAI Accelerator is properly detected. The device name may vary depending on the system."]},{"l":"Verifying MoAI Accelerator","p":["To train models like the sLLM introduced in this tutorial, it's important to select an appropriate size of the MoAI Accelerator. First, use the moreh-smi command to check the currently used MoAI Accelerator.","Detailed instructions for setting up the MoAI Accelerator specific to your training needs will be provided in the section 3. Model fine-tuning."]}],[{"l":"1. Preparing for Fine-tuning","p":["Preparing the PyTorch script execution environment on the MoAI Platform is similar to doing so on a typical GPU server. For a smooth tutorial experience, the following specifications are recommended:","CPU: 16 cores or more","Memory: 256GB or more","MAF version: 24.5.0","Storage: 60GB or more","Please verify that your environment meets these requirements before starting the tutorial."]},{"l":"Checking PyTorch Installation","p":["After connecting to the container via SSH, run the following command to check if PyTorch is installed in the current conda environment:","The version name includes both the PyTorch version and the version of MoAI required to run it. In the example above, it indicates that version 24.5.0 of MoAI, which runs PyTorch version 1.13.1+cu116, is installed.","If you see the message conda: command not found, if the torch package is not listed, or if the torch package exists but does not include \"moreh\" in the version name, please follow the instructions in the Prepare Fine-tuning on MoAI Platform to create a conda environment.","If the moreh version is not 24.5.0 but a different version, please execute the following code."]},{"l":"Verifying PyTorch Installation","p":["Run the following command to ensure that the torch package is imported correctly and the MoAI Accelerator is recognized."]},{"l":"Download the Training Script","p":["Execute the following command to download the PyTorch script for training from the GitHub repository. In this tutorial, we will be using the train_mistral.py script located inside the tutorial directory."]},{"l":"Install Required Python Packages","p":["Run the following command to install the third-party Python packages required to execute the script."]},{"l":"Acquire Access to the Model","p":["To access and download the Mistral 7B v0.1 model checkpoint from Hugging Face Hub, you will need to agree to the community license and provide your Hugging Face token information.","First, enter the required information on the Hugging Face website below and proceed with the license agreement.","mistralai/Mistral-7B-v0.1 · Hugging Face","After submitting the agreement, confirm that the page status has changed as shown below.","Now you can authenticate your Hugging Face token with the following command:"]}],[{"l":"2. Understanding Training Code","p":["Once you have prepared all the training data, let's delve into the contents of the train_mistral.py script to execute the actual fine-tuning process. In this step, you will confirm MoAI Platform's full compatibility with PyTorch, ensuring that the training code is identical to general PyTorch code for Nvidia GPUs. Moreover, you'll explore how efficiently MoAI Platform implements complex parallelization techniques beyond the conventional scope.","We highly recommend proceeding with the tutorial using the provided script as is. Afterward, feel free to customize the script to fine-tune the Llama2 13B model or any other publicly available model in a different manner. If needed, refer to the MoAI Platform application guide LLM Fine-tuning Parameter Guide provided by Moreh."]},{"l":"Training Code","p":["All the code used during training is exactly the same as when you're using PyTorch in general.","Import the necessary modules from the transformers library.","Load the model configuration and checkpoint publicly available on Hugging Face.","Then load the training dataset from Hugging Face Hub, preprocess loaded dataset, and define the data loader. In this tutorial, we will use the python_code_instructions_18k_alpaca dataset available on Hugging Face among various datasets publicly available for code generation training.","Subsequently, the training proceeds similarly to general AI model training with Pytorch.","With MoAI Platform, you can seamlessly use your existing PyTorch scripts without any modifications."]},{"l":"About Advanced Parallelism","p":["In the training script used in this tutorial, there is an additional line of code as follows, which executes the top-tier parallelization feature provided by the MoAI Platform:","For colossal language models like Mistral 7B used in this tutorial, it's imperative to train them using multiple GPUs. When using frameworks other than MoAI Platform, you'll need to introduce parallelization techniques such as Data Parallel, Pipeline Parallel, and Tensor Parallel.","For instance, if a user wants to apply DDP in their typical PyTorch code, they would need to add the following code snippet. ( https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","While DDP can be relatively easy to apply, implementing techniques like pipeline parallelism or tensor parallelism involves quite complex code modifications. To apply optimized parallelization, you need to understand how Python code acts in a multiprocessing environment while writing the training scripts. Especially in multi-node setups, configuring the environment of each node used for training is necessary. Additionally, finding the optimal parallelization method considering factors such as model type, size, and dataset requires a considerable amount of time.","In contrast, MoAI Platform's AP feature enables users to proceed with optimized parallelized training with just one line of code added to the training script, eliminating the need for users to manually apply additional parallelization techniques.","MoAI Platform's Advanced Parallelization(AP) provides optimization and automation features that are difficult to experience in other frameworks. Through the AP feature, users can experience the best distributed parallel processing. By leveraging AP, users can easily configure the optimal parameters and environment variables for Pipeline Parallelism and Tensor Parallelism required for training large-scale models with just a single line of code."]}],[{"l":"3. Model Fine-tuning","p":["Now, we will train the model through the following process."]},{"l":"Setting Accelerator Flavor","p":["AMD MI210 GPU with 32 units.","AMD MI250 GPU with 16 units:","AMD MI300X GPU with 8 units.","Do you remember checking MoAI Accelerator in the Mistral Fine-tuning - Getting Started document? Now let's set up the accelerator needed for learning.","Enter 8 to use .","Enter q to complete the change.","First, we'll use the moreh-smi command to check the currently used MoAI Accelerator.","In MoAI Platform, physical GPUs are not directly exposed to users. Instead, virtual MoAI Accelerators are provided, which are available for use in PyTorch. By setting the accelerator's flavor, you can determine how much of the physical GPU will be utilized by PyTorch. Since the total training time and GPU usage cost vary depending on the selected accelerator flavor, users should make decisions based on their training scenarios. If needed, please refer to the LLM Fine-tuning Parameter Guide to select the accelerator Flavor that aligns with your training objectives.","In this tutorial, we will use a 2048GB-sized MoAI Accelerator.","Now you can see that it has been successfully changed to .","Please refer to the document above or reach out to your infrastructure provider to inquire about the GPU types and quantities corresponding to each flavor.","Select when using KT Cloud's Hyperscale AI Computing.","Select when using Moreh's trial container.","The current MoAI Accelerator in use has a memory size of 64GB.","Therefore, after switching from the initially set flavor to , we will use the moreh-smi command to confirm that the change has been successfully applied.","To confirm that the changes have been successfully applied, use the moreh-smi command again to check the currently used MoAI Accelerator.","You can choose one of the following flavors to proceed:","You can enter the number to switch to a different flavor.","You can utilize the moreh-switch-model command to review the available accelerator flavors on the current system. For seamless model training, consider using the moreh-switch-model command to switch to a MoAI Accelerator with larger memory capacity."]},{"l":"Training Execution","p":["Execute the train_mistral.py script below.","If the training proceeds smoothly, you should see the following log. Take note of the sections highlighted in blue, as they indicate that the Advanced Parallelism feature is functioning correctly. It's worth noting that in the PyTorch script we examined earlier, there was no handling for using multiple GPUs simultaneously.","You can confirm that the training is progressing smoothly by observing the loss values decreasing as follows.","The throughput displayed during training indicates how many tokens per second are being processed through the PyTorch script.","When using 16 AMD MI250 GPUs: approximately 60,000 tokens/sec","Approximate training time based on GPU type and quantity is as follows:","When using 16 AMD MI250 GPUs: approximately 15 minutes"]},{"l":"Checking Accelerator Status During Training","p":["During training, open another terminal and connect to the container. You can execute the moreh-smi command to observe the MoAI Accelerator occupying memory while the training script is running. Please check the memory occupancy of MoAI accelerator when the training loss appears in the execution log after the initialization process."]}],[{"l":"4. Checking Training Results","p":["Running the train_mistral.py script, as in the previous section, will save the resulting model in the mistral_code_generation directory. This is a pure PyTorch model parameter file and is fully compatible with regular GPU servers, not just the MoAI Platform.","You can test the trained model using the inference_mistral.py script located in the tutorial directory of the GitHub repository you downloaded earlier. In this test, the prompt \"Create a function that takes a list of strings as input and joins them with spaces\" was used.","Run the code below.","Upon examining the output, you can confirm that the model has appropriately generated the function as per the prompt."]}],[{"l":"5. Changing the Number of GPUs","p":["Let's rerun the fine-tuning task with a different number of GPUs. MoAI Platform abstracts GPU resources into a single accelerator and automatically performs parallel processing. Therefore, there is no need to modify the PyTorch script even when changing the number of GPUs."]},{"l":"Changing the Accelerator type","p":["Switch the accelerator type using the moreh-switch-model tool. For instructions on changing the accelerator, please refer to the 3. Model fine-tuning","Please contact your infrastructure provider and choose one of the following options before proceeding.","AMD MI250 GPU with 32 units","When using Moreh's trial container: select","When using KT Cloud's Hyperscale AI Computing: select","AMD MI210 GPU with 64 units","AMD MI300X GPU with 16 units"]},{"l":"Training Parameters","p":["Since the available GPU memory has doubled, let's increase the batch size from the previous 256 to 512 and run the code again.","If the training proceeds normally, you should see the following logs:","Compared to the previous execution results when the number of GPUs was half, you can see that the learning is the same and the throughput has improved.","When using AMD MI250 GPU 16 → 32 : approximately 390,000 tokens/sec → 800,000 tokens/sec"]}],[{"l":"6. Conclusion","p":["From this tutorial, we have seen how to fine-tune the Mistral 7B model on the MoAI Platform. With MoAI Platform, you can easily fine-tune PyTorch-based open-source LLM models on GPU clusters while retaining your existing training code. Additionally, using the MoAI platform, you can easily configure the number of GPUs you need without any code changes. So please don’t hesitate to dive in and develop new models quickly and effortlessly with your data!"]},{"l":"Learn more","p":["MoAI Platform's Advanced Parallelization (AP)","Llama3 8B Fine-tuning","Llama3 70B","GPT Fine-tuning","Baichuan2 Fine-tuning","Qwen Fine-tuning"]}],[{"l":"GPT Fine-tuning","p":["This tutorial guides you on how to fine-tune GPT-based models open-sourced by Hugging Face on the MoAI Platform. Through this tutorial, users can experience various features provided by the MoAI Platform and learn how to use an AMD GPU cluster.","Users can easily run training without complex parallelization tasks or cluster environment setups, as they can treat dozens of GPUs as a single accelerator called MoAI Accelerator. This allows users to focus solely on training without worrying about resource management.","Thanks to the automatic parallelization feature, code writing and development are simplified, and model training speed is significantly improved. This enables efficient resource utilization, allowing users to work faster and more effectively."]},{"l":"Overview","p":["The MoAI Platform is a scalable AI platform that enables easy control of thousands of GPUs for training and inference of AI models. One of its key features is providing a very simple training method through virtualization and parallelization when fine-tuning models.","The MoAI Platform provides multiple GPUs virtualized into a single accelerator called MoAI Accelerator. Therefore, there is no need for preprations or code modifications for using multiple GPUs.","The MoAI Platform automatically provides optimized parallelization when users use the virtualized MoAI Accelerator. It considers various parallelization methods based on model and data sizes to offer the optimal parallelization environment. As a result, users can experience high-performance training with simple code without any additional tasks."]},{"l":"Getting Started","p":["Please obtain a container or virtual machine on the MoAI Platform from your infrastructure provider and follow the instructions to connect via SSH. For example, you can apply for a trial container on the MoAI Platform or use public cloud services based on the MoAI Platform.","MoAI Platform Trial Container (Inquiries: support@moreh.io)","KT Cloud's Hyperscale AI Computing ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","After connecting via SSH, execute the moreh-smi command to verify that the MoAI Accelerator is properly detected. The device name may vary depending on the system."]},{"l":"Verifying MoAI Accelerator","p":["To train models like the sLLM introduced in this tutorial, it's important to select an appropriate size of the MoAI Accelerator. First, use the moreh-smi command to check the currently used MoAI Accelerator.","Detailed instructions for setting up the MoAI Accelerator specific to your training needs will be provided in the section 3. Finetuning Model"]}],[{"l":"1. Preparing for Fine-tuning","p":["Preparing the PyTorch script execution environment on the MoAI Platform is similar to doing so on a typical GPU server. For a smooth tutorial experience, the following specifications are recommended:","CPU: 16 cores or more","Memory: 256GB or more","MAF version: 24.5.0","Storage: 100GB or more","Please verify that your environment meets these requirements before starting the tutorial."]},{"l":"Checking PyTorch Installation","p":["After connecting to the container via SSH, run the following command to check if PyTorch is installed in the current conda environment:","The version name includes both the PyTorch version and the version of MoAI required to run it. In the example above, it indicates that version 24.5.0 of MoAI, which runs PyTorch version 1.13.1+cu116, is installed.","If you see the message conda: command not found, if the torch package is not listed, or if the torch package exists but does not include \"moreh\" in the version name, please follow the instructions in the Prepare Fine-tuning on MoAI Platform document to create a conda environment.","If the moreh version is not 24.5.0 but a different version, please execute the following code."]},{"l":"Verifying PyTorch Installation","p":["Run the following command to ensure that the torch package is imported correctly and the MoAI Accelerator is recognized."]},{"l":"Downloading Training Script","p":["Run the following command to download the PyTorch script for training from the GitHub repository. In this tutorial, we will use the train_gpt.py script located inside the tutorial directory."]},{"l":"Install Required Python Packages","p":["Run the following command to install the third-party Python packages required to execute the script."]}],[{"l":"2. Understanding Training Code","p":["Once you have prepared all the training data, let's take a look at the contents of the train_gpt.py script to execute the actual fine-tuning process. In this step, you'll notice that the MoAI Platform offers full compatibility with PyTorch, meaning that the training code is 100% identical to typical PyTorch code for Nvidia GPUs. Furthermore, you'll see how efficiently the MoAI Platform implements complex parallelization techniques beyond what's traditionally possible.","We highly recommend proceeding with the tutorial using the provided script as is. Afterward, feel free to customize the script to fine-tune the Cerebras-GPT-13B model or any other publicly available model in a different manner. If needed, refer to the MoAI Platform application guide LLM Fine-tuning Parameter Guide."]},{"l":"Training Code","p":["All code remains fully consistent with general PyTorch usage.","Firstly, import the required modules from the transformers library.","Load the model configuration and checkpoint publicly available on Hugging Face.","Then load the training dataset from Hugging Face Hub, preprocess loaded dataset, and define the data loader. In this tutorial, we will use the Evol-Instruct-Python-26k dataset. This dataset consists of Python code written in response to given prompt conditions.","Subsequently, the training proceeds similarly to general AI model training with Pytorch.","As shown above, you can code in the same way as traditional PyTorch code on MoAI Platform."]},{"l":"About Advanced Parallelism","p":["In the training script used in this tutorial, there is an additional line of code as follows, which executes the top-tier parallelization feature provided by the MoAI Platform:","For enormous language models like Cerebras-GPT-13B used in this tutorial, it is inevitable to train them using multiple GPUs. In such cases, if you were to use frameworks other than the MoAI Platform, you would need to employ parallelization techniques like Data Parallel, Pipeline Parallel, or Tensor Parallel for training.","For instance, if a user wants to apply DDP in a typical PyTorch code, the following code snippet would need to be added. ( https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","While DDP can be relatively easy to apply, implementing techniques like pipeline parallelism or tensor parallelism involves quite complex code modifications. To apply optimized parallelization, you need to understand how Python code acts in a multiprocessing environment while writing the training scripts. Especially in multi-node setups, configuring the environment of each node used for training is necessary. Additionally, finding the optimal parallelization method considering factors such as model type, size, and dataset requires a considerable amount of time.","In contrast, MoAI Platform's AP feature enables users to proceed with optimized parallelized training with just one line of code added to the training script, eliminating the need for users to manually apply additional parallelization techniques.","Experience optimal distributed parallel processing like no other framework can offer, thanks to MoAI Platform's Advanced Parallelization (AP), a feature that optimizes and automates parallelization in ways not found in other frameworks. With the AP feature, you can easily secure the optimal parameters and environment variables for Pipeline Parallelism and Tensor Parallelism, typically required for training large-scale models, with just one simple line of code."]}],[{"l":"3. Model Fine-tuning","p":["Now, we will train the model through the following process."]},{"l":"Setting the Accelerator","p":["AMD MI210 GPU with 32 units.","AMD MI250 GPU with 16 units:","AMD MI300X GPU with 8 units.","Do you remember checking MoAI Accelerator in the GPT Fine-tuning - Getting Started document? Now let's set up the accelerator needed for learning.","Enter 8 to use","Enter q to complete the change.","First, we'll use the moreh-smi command to check the currently used MoAI Accelerator.","In MoAI Platform, physical GPUs are not directly exposed to users. Instead, virtual MoAI Accelerators are provided, which are available for use in PyTorch. By setting the accelerator's flavor, you can determine how much of the physical GPU will be utilized by PyTorch. Since the total training time and GPU usage cost vary depending on the selected accelerator flavor, users should make decisions based on their training scenarios. If needed, please refer to the LLM Fine-tuning Parameter Guide to select the accelerator Flavor that aligns with your training objectives.","In this tutorial, we will use a 2048GB-sized MoAI Accelerator.","Please refer to the document above or reach out to your infrastructure provider to inquire about the GPU types and quantities corresponding to each flavor.","Select when using KT Cloud's Hyperscale AI Computing.","Select when using Moreh's trial container.","The current MoAI Accelerator in use has a memory size of 256GB.","Therefore, after switching from the initially set flavor to , we will use the moreh-smi command to confirm that the change has been successfully applied.","To confirm that the changes have been successfully applied, use the moreh-smi command again to check the currently used MoAI Accelerator.","You can choose one of the following flavors to proceed:","You can enter the number to switch to a different flavor.","You can utilize the moreh-switch-model command to review the available accelerator flavors on the current system. For seamless model training, consider using the moreh-switch-model command to switch to a MoAI Accelerator with larger memory capacity.","로 잘 변경된 것을 확인할 수 있습니다."]},{"l":"Training Execution","p":["Execute the train_gpt.py script below.","If the training proceeds smoothly, you should see the following logs. By going through this logs, you can verify that the Advanced Parallelism feature, which determines the optimal parallelization settings, is functioning properly. It's worth noting that, apart from the single line of AP code we looked at earlier in the PyTorch script, there is no handling for using multiple GPUs simultaneously in other parts of the script.","The training loss decreases as follows, confirming normal training progress.","The throughput displayed during training indicates how many tokens per second are being processed through the PyTorch script.","When using 16 AMD MI250 GPUs: approximately 6,800 tokens/sec","Approximate training times based on GPU type and quantity are as follows:","When using 16 AMD MI250 GPUs: approximately 81 minutes"]},{"l":"Checking Accelerator Status During Training","p":["During training, open another terminal and connect to the container. You can execute the moreh-smi command to observe the MoAI Accelerator occupying memory while the training script is running. Please check the memory occupancy of MoAI accelerator when the training loss appears in the execution log after the initialization process."]}],[{"l":"4. Checking Training Results","p":["As in the previous chapter, when you run the train_gpt.py script, the resulting model will be saved in the code_generation directory. This is a pure PyTorch model parameter file and is fully compatible compatible not only with MoAI Platform but also with regular GPU servers.","You can test the trained model using the inference_gpt.py script located under the tutorial directory of the GitHub repository you downloaded beforehand.","Run the train script.","Upon inspecting the output, you can confirm that the model has generated an appropriate function based on the prompt content."]}],[{"l":"5. Changing the Number of GPUs","p":["Let's rerun the fine-tuning task with a different number of GPUs. MoAI Platform abstracts GPU resources into a single accelerator and automatically performs parallel processing. Therefore, there is no need to modify the PyTorch script even when changing the number of GPUs."]},{"l":"Changing the Accelerator type","p":["Switch the accelerator type using the moreh-switch-model tool. For instructions on changing the accelerator, please refer again to the 3. Model fine-tuning document.","Please contact your infrastructure provider and choose one of the following options before proceeding.","AMD MI250 GPU with 32 units","When using Moreh's trial container: select","When using KT Cloud's Hyperscale AI Computing: select","AMD MI210 GPU with 64 units","AMD MI300X GPU with 16 units"]},{"l":"Training Parameters","p":["Run the train_gpt.py script again.","Since the available GPU memory has doubled, let's also change the batch size to 32 and run it.","If the training proceeds normally, the following log will be output.","Compared to the previous execution results when the number of GPUs was half, you can see that the learning is the same and the throughput has improved.","When using AMD MI250 GPU 16 → 32 : approximately 6,800 tokens/sec → 13,000 tokens/sec"]}],[{"l":"6. Conclusion","p":["So far, we have looked at the process of fine-tuning the GPT-based model from HuggingFace on the MoAI Platform. With MoAI Platform, you can easily fine-tune PyTorch-based open-source LLM models on GPU clusters while maintaining existing training code. Additionally, if you use the MoAI platform, you can easily configure the number of GPUs you need without changing any code. Try developing new models quickly and effortlessly with your data."]},{"l":"Learn More","p":["MoAI Platform's Advanced Parallelization(AP)","Llama3 8B Fine-tuning","Llama3 70B Fine-tuning","Mistral Fine-tuning","Baichuan2 Fine-tuning","Qwen Fine-tuning"]}],[{"l":"Qwen Fine-tuning","p":["This tutorial introduces an example of fine-tuning the open-source Qwen1.5 7B model on the MoAI Platform. Through this tutorial, users can experience various features provided by the MoAI Platform and learn how to use an AMD GPU cluster.","Users can easily run training without complex parallelization tasks or cluster environment setups, as they can treat dozens of GPUs as a single accelerator called MoAI Accelerator. This allows users to focus solely on training without worrying about resource management.","Thanks to the automatic parallelization feature, code writing and development are simplified, and model training speed is significantly improved. This enables efficient resource utilization, allowing users to work faster and more effectively."]},{"l":"Overview","p":["he MoAI Platform is a scalable AI platform that enables easy control of thousands of GPUs for training and inference of AI models. One of its key features is providing a very simple training method through virtualization and parallelization when fine-tuning models.","The MoAI Platform provides multiple GPUs virtualized into a single accelerator called MoAI Accelerator. Therefore, there is no need for preprations or code modifications for using multiple GPUs.","The MoAI Platform automatically provides optimized parallelization when users use the virtualized MoAI Accelerator. It considers various parallelization methods based on model and data sizes to offer the optimal parallelization environment. As a result, users can experience high-performance training with simple code without any additional tasks."]},{"l":"Getting Started","p":["Please obtain a container or virtual machine on the MoAI Platform from your infrastructure provider and follow the instructions to connect via SSH. For example, you can apply for a trial container on the MoAI Platform or use public cloud services based on the MoAI Platform.","MoAI Platform Trial Container (Inquiries: support@moreh.io)","KT Cloud's Hyperscale AI Computing ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","After connecting via SSH, execute the moreh-smi command to verify that the MoAI Accelerator is properly detected. The device name may vary depending on the system."]},{"l":"Verifying MoAI Accelerator","p":["To train models like the sLLM introduced in this tutorial, it's important to select an appropriate size of the MoAI Accelerator. First, use the moreh-smi command to check the currently used MoAI Accelerator.","Detailed instructions for setting up the MoAI Accelerator specific to your training needs will be provided in the section 3. Model fine-tuning"]}],[{"l":"1. Preparing for Fine-tuning","p":["Preparing the PyTorch script execution environment on the MoAI Platform is similar to doing so on a typical GPU server. For a smooth tutorial experience, the following specifications are recommended:","CPU: 16 cores or more","Memory: 256GB or more","MAF version: 24.5.0","Storage: 61GB or more","Please verify that your environment meets these requirements before starting the tutorial."]},{"l":"Checking PyTorch Installation","p":["After connecting to the container via SSH, run the following command to check if PyTorch is installed in the current conda environment:","The version name includes both the PyTorch version and the version of MoAI required to run it. In the example above, it indicates that version 24.5.0 of MoAI, which runs PyTorch version 1.13.1+cu116, is installed.","If you see the message conda: command not found, if the torch package is not listed, or if the torch package exists but does not include \"moreh\" in the version name, please follow the instructions in the Prepare Fine-tuning on MoAI Platform document to create a conda environment.","If the moreh version is not 24.5.0 but a different version, please execute the following code."]},{"l":"Verifying PyTorch Installation","p":["Run the following command to ensure that the torch package is imported correctly and the MoAI Accelerator is recognized."]},{"l":"Download the Training Script","p":["Execute the following command to download the PyTorch script for training from the GitHub repository. In this tutorial, we will be using the train_qwen.py script located inside the tutorial directory."]},{"l":"Install Required Python Packages","p":["Run the following command to install the third-party Python packages required to execute the script."]}],[{"l":"2. Understanding Training Code","p":["If you've prepared all the training data, let's now take a look at the train_qwen.py script to actually run the fine-tuning process. This script is a standard PyTorch code that executes fine-tuning based on the implementation of the Qwen model available in the Hugging Face Transformers library.","In this step, you'll observe that MoAI Platform is fully compatible with PyTorch, and the training code is exactly the same as standard PyTorch code designed for NVIDIA GPUs. Moreover, you can also see how efficiently MoAI Platform can implement complex parallelization techniques.","We recommend starting by using the provided script to complete the tutorial as-is. Afterwards, feel free to modify the script as desired to fine-tune the Qwen1.5 7B model in different ways. Thanks to MoAI Platform's full compatibility with PyTorch, such modifications are possible. If needed, refer to the LLM Fine-tuning parameter guide for assistance."]},{"l":"Training Code","p":["All the code used during training is exactly the same as the standard method of using PyTorch.","Firstly, import the required modules from the transformers library.","Load the model configuration and checkpoint publicly available on Hugging Face.","Then load the training dataset from Hugging Face Hub, preprocess loaded dataset, and define the data loader.","Subsequently, the training proceeds similarly to general AI model training with Pytorch.","As shown above, you can code in the same way as traditional PyTorch code on MoAI Platform."]},{"l":"About Advanced Parallelism","p":["In the training script used in this tutorial, there is an additional line of code as follows, which executes the top-tier parallelization feature provided by the MoAI Platform:","To train massive language models like Qwen1.5 7B, which we use in this tutorial, it's inevitable to utilize multiple GPUs. If using different frameworks, you'll need to implement parallelization techniques such as Data Parallel, Pipeline Parallel, and Tensor Parallel to proceed with training.","For instance, if a user wants to apply DDP in a typical PyTorch code, the following code snippet would need to be added. ( https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","While DDP can be relatively easy to apply, implementing techniques like pipeline parallelism or tensor parallelism involves quite complex code modifications. To apply optimized parallelization, you need to understand how Python code acts in a multiprocessing environment while writing the training scripts. Especially in multi-node setups, configuring the environment of each node used for training is necessary. Additionally, finding the optimal parallelization method considering factors such as model type, size, and dataset requires a considerable amount of time.","In contrast, MoAI Platform's AP feature enables users to proceed with optimized parallelized training with just one line of code added to the training script, eliminating the need for users to manually apply additional parallelization techniques.","Experience optimal distributed parallel processing like no other framework can offer, thanks to MoAI Platform's Advanced Parallelization (AP), a feature that optimizes and automates parallelization in ways not found in other frameworks. With the AP feature, you can easily secure the optimal parameters and environment variables for Pipeline Parallelism and Tensor Parallelism, typically required for training large-scale models, with just one simple line of code."]}],[{"l":"3. Model Fine-tuning","p":["Now, we will train the model through the following process."]},{"l":"Setting Accelerator Flavor","p":["AMD MI210 GPU with 32 units.","AMD MI250 GPU with 16 units:","AMD MI300X GPU with 8 units.","Do you remember checking MoAI Accelerator in the Mistral Fine-tuning - Getting Started document? Now let's set up the accelerator needed for learning.","Enter 8 to use","Enter q to complete the change.","First, we'll use the moreh-smi command to check the currently used MoAI Accelerator.","In MoAI Platform, physical GPUs are not directly exposed to users. Instead, virtual MoAI Accelerators are provided, which are available for use in PyTorch. By setting the accelerator's flavor, you can determine how much of the physical GPU will be utilized by PyTorch. Since the total training time and GPU usage cost vary depending on the selected accelerator flavor, users should make decisions based on their training scenarios. If needed, please refer to the LLM Fine-tuning Parameter Guide to select the accelerator Flavor that aligns with your training objectives.","In this tutorial, we will use a 2048GB-sized MoAI Accelerator.","Now you can see that it has been successfully changed to .","Please refer to the document above or reach out to your infrastructure provider to inquire about the GPU types and quantities corresponding to each flavor.","Select when using KT Cloud's Hyperscale AI Computing.","Select when using Moreh's trial container.","The current MoAI Accelerator in use has a memory size of 64GB.","Therefore, after switching from the initially set flavor to , we will use the moreh-smi command to confirm that the change has been successfully applied.","To confirm that the changes have been successfully applied, use the moreh-smi command again to check the currently used MoAI Accelerator.","You can choose one of the following flavors to proceed:","You can enter the number to switch to a different flavor.","You can utilize the moreh-switch-model command to review the available accelerator flavors on the current system. For seamless model training, consider using the moreh-switch-model command to switch to a MoAI Accelerator with larger memory capacity."]},{"l":"Training Execution","p":["Execute the train_qwen.py script below.","If the training proceeds smoothly, you should see the following logs. By going through this logs, you can verify that the Advanced Parallelism feature, which determines the optimal parallelization settings, is functioning properly. It's worth noting that, apart from the single line of AP code we looked at earlier in the PyTorch script, there is no handling for using multiple GPUs simultaneously in other parts of the script.","You can confirm that the training is progressing smoothly by observing the loss values appearing as follows.","The throughput displayed during training indicates how many tokens per second are being processed through the PyTorch script.","When using 16 AMD MI250 GPUs: approximately 190,000 tokens/sec","Approximate training time based on GPU type and quantity is as follows:","When using 16 AMD MI250 GPUs: approximately 16 minutes"]},{"l":"Checking Accelerator Status During Training","p":["During training, open another terminal and connect to the container. You can execute the moreh-smi command to observe the MoAI Accelerator occupying memory while the training script is running. Please check the memory occupancy of MoAI accelerator when the training loss appears in the execution log after the initialization process."]}],[{"l":"4. Checking Training Results","p":["Running the train_qwen.py script, as in the previous chapter, will save the resulting model in the qwen_code_generation directory. This is a pure PyTorch model parameter file and is fully compatible with regular GPU servers, not just the MoAI Platform.","You can test the trained model using the inference_qwen.py script located under the tutorial directory in the GitHub repository you downloaded earlier. In the test, the prompt \"Given a list of strings, create a function that joins them with spaces\" was used.","Run the code below.","Upon examining the output, you can confirm that the model has appropriately generated the function as per the prompt."]}],[{"l":"5. Changing the Number of GPUs","p":["Let's rerun the fine-tuning task with a different number of GPUs. MoAI Platform abstracts GPU resources into a single accelerator and automatically performs parallel processing. Therefore, there is no need to modify the PyTorch script even when changing the number of GPUs."]},{"l":"Changing the Accelerator type","p":["Switch the accelerator type using the moreh-switch-model tool. For instructions on changing the accelerator, please refer to the 3. Model fine-tuning","Please contact your infrastructure provider and choose one of the following options before proceeding.","AMD MI250 GPU with 32 units","When using Moreh's trial container: select","When using KT Cloud's Hyperscale AI Computing: select","AMD MI210 GPU with 64 units","AMD MI300X GPU with 16 units"]},{"l":"Training Parameters","p":["Since the available GPU memory has doubled, let's increase the batch size from the previous 256 to 512 and run the code again.","If the training proceeds normally, you should see the following logs:","Compared to the previous execution results when the number of GPUs was half, you can see that the learning is the same and the throughput has improved.","When using AMD MI250 GPU 16 → 32 : approximately 190,000 tokens/sec → 380,000 tokens/sec"]}],[{"l":"6. Conclusion","p":["So far, we've explored the process of fine-tuning the Qwen1.5 7B model on the MoAI Platform. With MoAI Platform, you can easily fine-tune PyTorch-based open-source LLM models on GPU clusters while keeping your existing training code intact. Additionally, using MoAI Platform, you can easily configure the required number of GPUs without any code changes. So please don’t hesitate to dive in and develop new models quickly and effortlessly with your data!","In case if you still have any questions regarding this tutorial feel free to ask Moreh( support@moreh.io)."]},{"l":"Learn More","p":["MoAI Platform's Advanced Parallelization (AP)","Llama3 8B Fine-tuning","Llama3 70B Fine-tuning","Mistral Fine-tuning","GPT Fine-tuning","Baichuan2 Fine-tuning"]}],[{"l":"Baichuan2 Fine-tuning","p":["The following tutorial will take you through the steps required to fine-tune Baichuan2 13B model with an example dataset, using the MoAI Platform. Through this tutorial, users can experience various features provided by the MoAI Platform and learn how to use an AMD GPU cluster.","Users can easily run training without complex parallelization tasks or cluster environment setups, as they can treat dozens of GPUs as a single accelerator called MoAI Accelerator. This allows users to focus solely on training without worrying about resource management.","Thanks to the automatic parallelization feature, code writing and development are simplified, and model training speed is significantly improved. This enables efficient resource utilization, allowing users to work faster and more effectively."]},{"l":"Overview","p":["The MoAI Platform is a scalable AI platform that enables easy control of thousands of GPUs for training and inference of AI models. One of its key features is providing a very simple training method through virtualization and parallelization when fine-tuning models.","The MoAI Platform provides multiple GPUs virtualized into a single accelerator called MoAI Accelerator. Therefore, there is no need for preprations or code modifications for using multiple GPUs.","The MoAI Platform automatically provides optimized parallelization when users use the virtualized MoAI Accelerator. It considers various parallelization methods based on model and data sizes to offer the optimal parallelization environment. As a result, users can experience high-performance training with simple code without any additional tasks."]},{"l":"Getting Started","p":["Please obtain a container or virtual machine on the MoAI Platform from your infrastructure provider and follow the instructions to connect via SSH. For example, you can apply for a trial container on the MoAI Platform or use public cloud services based on the MoAI Platform.","MoAI Platform Trial Container (Inquiries: support@moreh.io)","KT Cloud's Hyperscale AI Computing ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","After connecting via SSH, execute the moreh-smi command to verify that the MoAI Accelerator is properly detected. The device name may vary depending on the system."]},{"l":"Verifying MoAI Accelerator","p":["To train models like the sLLM introduced in this tutorial, it's important to select an appropriate size of the MoAI Accelerator. First, use the moreh-smi command to check the currently used MoAI Accelerator.","Detailed instructions for setting up the MoAI Accelerator specific to your training needs will be provided in the section 3. Model fine-tuning."]}],[{"l":"1. Preparing for Fine-tuning","p":["Preparing the PyTorch script execution environment on the MoAI Platform is similar to doing so on a typical GPU server. For a smooth tutorial experience, the following specifications are recommended:","CPU: 16 cores or more","Memory: 256GB or more","MAF version: 24.5.0","Storage: 55GB or more","Please verify that your environment meets these requirements before starting the tutorial."]},{"l":"Checking PyTorch Installation","p":["After connecting to the container via SSH, run the following command to check if PyTorch is installed in the current conda environment:","The version name includes both the PyTorch version and the MoAI version required to execute it. In the example above, it indicates that PyTorch version 1.13.1+cu116 is running with MoAI version 24.5.0 installed.","If you encounter a conda: command not found message, or if the torch package is not listed, or if the torch package exists but does not include \"moreh\" in the version name, please follow the instructions in the Prepare Fine-tuning on MoAI Platform to create a conda environment.","If the moreh version is not 24.5.0 but a different version, please execute the following code."]},{"l":"Verifying PyTorch Installation","p":["Run the following command to ensure that the torch package is imported correctly and the MoAI Accelerator is recognized."]},{"l":"Download the Training Script","p":["Execute the following command to download the PyTorch script for training from the GitHub repository. In this tutorial, we will be using the train_baichuan2_13b.py script located inside the tutorial directory."]},{"l":"Install Required Python Packages","p":["Execute the following command to install third-party Python packages required for script execution:"]}],[{"l":"2. Understanding Training Code","p":["If you've prepared all the training data, let's now take a look at the contents of train_baichuan2_13b.py script for the actual fine-tuning process. In this step, you'll notice that MoAI Platform ensures full compatibility with PyTorch, confirming that the training code is identical to the typical PyTorch code for NVIDIA GPUs. Additionally, you'll explore how efficiently MoAI Platform implements complex parallelization techniques beyond this.","First and foremost, it's recommended to proceed with the tutorial using the provided script as is until the end. Afterwards, you can modify the script as you wish to fine-tune the Baichuan model in different ways. If needed, refer to the LLM Fine-tuning Parameter Guide."]},{"l":"Training Code","p":["All the code is exactly the same as when using PyTorch conventionally.","First, import the necessary modules from the transformers library.","Load the checkpoint from HuggingFace.","For model optimization, we use the BaichuanForCausalLM predefined in the quickstart repository.","Then load the training dataset from Hugging Face Hub, preprocess loaded dataset, and define the data loader.","Subsequent training proceeds just like any other model training with PyTorch.","As shown above, with MoAI Platform, you can use your existing PyTorch scripts without any modifications."]},{"l":"About Advanced Parallelism","p":["The training script used in this tutorial includes the following additional line of code, which performs automatic parallelization provided by MoAI Platform.","For huge language models like Baichuan2 13B, it's inevitable to train them using multiple GPUs. In such cases, if you're not using MoAI Platform, you'll need to introduce parallelization techniques like Data Parallel, Pipeline Parallel, and Tensor Parallelism.","For instance, if you want to apply DDP in your PyTorch code, you would need to add the following code snippet: ( https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","DDP can be relatively easy to apply, but implementing techniques like pipeline parallelism or tensor parallelism requires quite complex code modifications. To apply optimized parallelization, you need to understand how Python code acts in a multiprocessing environment while writing the training scripts. Especially in multi-node setups, configuring the environment of each node used for training is necessary. Furthermore, finding the optimal parallelization method considering factors such as model type, size, and dataset requires a considerable amount of time.","On the other hand, MoAI Platform's AP feature enables users to proceed with optimized parallelized training with just one line of code added to the training script, eliminating the need for users to manually apply additional parallelization techniques.","Experience the optimal automated distributed parallel processing that is only possible with MoAI Platform's Advanced Parallelization (AP) feature, unlike anything you've encountered in other frameworks. With AP, you can easily configure the optimal parameters and environment variables for pipeline parallelism and tensor parallelism, typically required for training large-scale models, with just a single line of code."]}],[{"l":"3. Model Fine-tuning","p":["Now, we will train the model through the following process."]},{"l":"Setting Accelerator Flavor","p":["AMD MI210 GPU with 32 units.","AMD MI250 GPU with 16 units:","AMD MI300X GPU with 8 units.","Enter 8 to use .","Enter q to complete the change.","First, we'll use the moreh-smi command to check the currently used MoAI Accelerator.","In MoAI Platform, physical GPUs are not directly exposed to users. Instead, virtual MoAI Accelerators are provided, which are available for use in PyTorch. By setting the accelerator's flavor, you can determine how much of the physical GPU will be utilized by PyTorch. Since the total training time and GPU usage cost vary depending on the selected accelerator flavor, users should make decisions based on their training scenarios. If needed, please refer to the LLM Fine-tuning Parameter Guide to select the accelerator Flavor that aligns with your training objectives.","In this tutorial, we will use a 2048GB-sized MoAI Accelerator.","Now you can see that it has been successfully changed to .","Please refer to the document above or reach out to your infrastructure provider to inquire about the GPU types and quantities corresponding to each flavor.","Remember we checked the MoAI Accelerator in the previous Baichuan2 Finetuning - Getting Started step? Now, let's set up the required accelerators for the actual training process.","Select when using KT Cloud's Hyperscale AI Computing.","Select when using Moreh's trial container.","The current MoAI Accelerator in use has a memory size of 256GB.","Therefore, after switching from the initially set flavor to , we will use the moreh-smi command to confirm that the change has been successfully applied.","To confirm that the changes have been successfully applied, use the moreh-smi command again to check the currently used MoAI Accelerator.","You can choose one of the following flavors to proceed:","You can enter the number to switch to a different flavor.","You can utilize the moreh-switch-model command to review the available accelerator flavors on the current system. For seamless model training, consider using the moreh-switch-model command to switch to a MoAI Accelerator with larger memory capacity."]},{"l":"Training Execution","p":["Execute the train_baichuan2.py script below.","If the training proceeds smoothly, you should see the following logs. By going through this logs, you can verify that the Advanced Parallelism feature, which determines the optimal parallelization settings, is functioning properly. It's worth noting that, apart from the single line of AP code we looked at earlier in the PyTorch script, there is no handling for using multiple GPUs simultaneously in other parts of the script.","You can confirm that the training is progressing smoothly by observing the loss values decreasing as follows.","The throughput displayed during training indicates how many tokens per second are being processed through the PyTorch script.","When using 8 AMD MI250 GPUs: approximately 191,000 tokens/sec","Approximate training time based on GPU type and quantity is as follows:","When using 8 AMD MI250 GPUs: approximately 30 minutes"]},{"l":"Checking Accelerator Status During Training","p":["During training, open another terminal and connect to the container. You can execute the moreh-smi command to observe the MoAI Accelerator occupying memory while the training script is running. Please check the memory occupancy of MoAI accelerator when the training loss appears in the execution log after the initialization process."]}],[{"l":"4. Checking Training Results","p":["Similar to the previous chapter, when you execute the train_baichuan2_13b.py script, the resulting model will be saved in the baichuan_code_generation directory. This model, stored as a pure PyTorch parameter file, is fully compatible not only with the MoAI Platform but also with regular GPU servers.","You can test the trained model using the inference_baichuan.py script located under the tutorial directory of the GitHub repository you downloaded earlier.","Run the code below.","Upon inspecting the output, you can verify that the model has generated appropriate responses to the prompts."]}],[{"l":"5. Changing the Number of GPUs","p":["Let's rerun the fine-tuning task with a different number of GPUs. MoAI Platform abstracts GPU resources into a single accelerator and automatically performs parallel processing. Therefore, there is no need to modify the PyTorch script even when changing the number of GPUs."]},{"l":"Changing the Accelerator type","p":["Switch the accelerator type using the moreh-switch-model tool. For instructions on changing the accelerator, please refer to the 3. Model fine-tuning","Please contact your infrastructure provider and choose one of the following options before proceeding.","AMD MI250 GPU with 32 units","When using Moreh's trial container: select","When using KT Cloud's Hyperscale AI Computing: select","AMD MI210 GPU with 64 units","AMD MI300X GPU with 16 units"]},{"l":"Training Parameters","p":["Run the train_baichuan2_13b.py script again.","Since the available GPU memory has doubled, let's increase the batch size to 512 and run the training.","f the training proceeds normally, you should see the following log:","Upon comparison with the results obtained when the number of GPUs was halved, you'll notice that the training progresses similarly, with an improvement in throughput.","When using AMD MI250 GPU 16 → 32 : approximately 198,000 tokens/sec → 370,000 tokens/sec"]}],[{"l":"6. Conclusion","p":["So far, we've explored the process of fine-tuning the Baichuan2 13B model, which is publicly available on Hugging Face, using the MoAI Platform. With MoAI Platform, you can easily fine-tune PyTorch-based open-source LLM models on GPU clusters while keeping your existing training code intact. Moreover, with MoAI Platform, you can effortlessly configure the number of GPUs you need without changing any code. So please dive in and develop new models quickly and effortlessly with your data!","In case if you still have any questions regarding this tutorial feel free to ask Moreh( support@moreh.io)."]},{"l":"Learn More","p":["MoAI Platform's Advanced Parallelization(AP)","Llama3 8B Fine-tuning","Llama3 70B Fine-tuning","Mistral Fine-tuning","GPT Fine-tuning","Qwen Fine-tuning"]}],[{"l":"Llama2 13B Fine-tuning","p":["This tutorial introduces an example of fine-tuning the open-source Llama2 13B model on the MoAI Platform. Through this tutorial, users can experience various features provided by the MoAI Platform and learn how to use an AMD GPU cluster.","Users can easily run training without complex parallelization tasks or cluster environment setups, as they can treat dozens of GPUs as a single accelerator called MoAI Accelerator. This allows users to focus solely on training without worrying about resource management.","Thanks to the automatic parallelization feature, code writing and development are simplified, and model training speed is significantly improved. This enables efficient resource utilization, allowing users to work faster and more effectively."]},{"l":"Overview","p":["The MoAI Platform is a scalable AI platform that enables easy control of thousands of GPUs for training and inference of AI models. One of its key features is providing a very simple training method through virtualization and parallelization when fine-tuning models.","The MoAI Platform provides multiple GPUs virtualized into a single accelerator called MoAI Accelerator. Therefore, there is no need for preprations or code modifications for using multiple GPUs.","The MoAI Platform automatically provides optimized parallelization when users use the virtualized MoAI Accelerator. It considers various parallelization methods based on model and data sizes to offer the optimal parallelization environment. As a result, users can experience high-performance training with simple code without any additional tasks."]},{"l":"Getting Started","p":["Please obtain a container or virtual machine on the MoAI Platform from your infrastructure provider and follow the instructions to connect via SSH. For example, you can apply for a trial container on the MoAI Platform or use public cloud services based on the MoAI Platform.","MoAI Platform Trial Container (Inquiries: support@moreh.io)","KT Cloud's Hyperscale AI Computing ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","After connecting via SSH, execute the moreh-smi command to verify that the MoAI Accelerator is properly detected. The device name may vary depending on the system."]},{"l":"Verifying MoAI Accelerator","p":["To train models like the sLLM introduced in this tutorial, it's important to select an appropriate size of the MoAI Accelerator. First, use the moreh-smi command to check the currently used MoAI Accelerator.","Detailed instructions for setting up the MoAI Accelerator specific to your training needs will be provided in the section 3. Model fine-tuning"]}],[{"l":"1. Preparing for Fine-tuning","p":["Preparing the PyTorch script execution environment on the MoAI Platform is similar to doing so on a typical GPU server. For a smooth tutorial experience, the following specifications are recommended:","CPU: 16 cores or more","Memory: 256GB or more","MAF version: 24.5.0","Storage: 105GB or more","Please verify that your environment meets these requirements before starting the tutorial."]},{"l":"Checking PyTorch Installation","p":["After connecting to the container via SSH, run the following command to check if PyTorch is installed in the current conda environment:","The version name includes both the PyTorch version and the version of MoAI required to run it. In the example above, it indicates that version 24.5.0 of MoAI, which runs PyTorch version 1.13.1+cu116, is installed.","If you see the message conda: command not found, if the torch package is not listed, or if the torch package exists but does not include \"moreh\" in the version name, please follow the instructions in the Prepare Fine-tuning on MoAI Platform document to create a conda environment.","If the moreh version is not 24.5.0 but a different version, please execute the following code."]},{"l":"Verifying PyTorch Installation","p":["Run the following command to confirm that the torch package is properly imported and the MoAI Accelerator is recognized."]},{"l":"Download the Training Script","p":["Execute the following command to download the PyTorch script for training from the GitHub repository. In this tutorial, we will be using the train_llama2.py script located inside the tutorial directory."]},{"l":"Install Required Python Packages","p":["Execute the following command to install third-party Python packages required for script execution:"]},{"l":"Acquire Access to the Model","p":["To access and download the Llama2-13b-hf model checkpoint from Hugging Face Hub, you will need to agree to the community license and provide your Hugging Face token information.","First, enter the required information and agree to the license on the following site.","meta-llama/Llama-2-13b-hf · Hugging Face","After submitting the agreement, confirm that the page status has changed as shown below.","Now you can authenticate your Hugging Face token with the following command:"]}],[{"l":"2. Understanding Training Code","p":["If you've got all your training data ready, let's dive into running the actual fine-tuning process using the train_llama2.py script. This script is just standard PyTorch code, performing fine-tuning based on the Llama2 13B model from the Hugging Face Transformers library.","We highly recommend proceeding with the tutorial using the provided script as is. Afterward, feel free to customize the script to fine-tune the Llama2 13B model or any other publicly available model in a different manner. If needed, refer to the MoAI Platform application guide LLM Fine-tuning Parameter Guide provided by Moreh."]},{"l":"Training Code","p":["All the code used during training is exactly the same as when you're using PyTorch in general.","Import the necessary modules from the transformers library.","Load the model configuration and checkpoint publicly available on Hugging Face.","Then load the training dataset from Hugging Face Hub, preprocess loaded dataset, and define the data loader.","Training proceeds as usual, just like with any other PyTorch model.","With MoAI Platform, you can seamlessly use your existing PyTorch scripts without any modifications."]},{"l":"About Advanced Parallelism","p":["In the training script used in this tutorial, there is an additional line of code as follows, which executes the top-tier parallelization feature provided by the MoAI Platform:","Training a large language model like Llama2 13B requires a significant amount of GPUs. Without using the MoAI Platform, you would need to implement parallelization techniques such as data parallelism, pipeline parallelism, and tensor parallelism to perform the training.","(Reference: https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","While DDP can be relatively easy to apply, implementing techniques like pipeline parallelism or tensor parallelism involves quite complex code modifications. To apply optimized parallelization, you need to understand how Python code acts in a multiprocessing environment while writing the training scripts. Especially in multi-node setups, configuring the environment of each node used for training is necessary. Additionally, finding the optimal parallelization method considering factors such as model type, size, and dataset requires a considerable amount of time.","In contrast, MoAI Platform's AP feature enables users to proceed with optimized parallelized training with just one line of code added to the training script, eliminating the need for users to manually apply additional parallelization techniques.","MoAI Platform's Advanced Parallelization (AP) provides optimization and automation features that are difficult to experience in other frameworks. With the AP feature, you can easily configure the optimal parameters and environment variables for Pipeline Parallelism and Tensor Parallelism which are typically required for large-scale model training, with just a single line of code."]}],[{"l":"3. Model Fine-tuning","p":["Now, we will train the model through the following process."]},{"l":"Setting Accelerator Flavor","p":["AMD MI210 GPU with 32 units.","AMD MI250 GPU with 16 units:","AMD MI300X GPU with 8 units.","Before continuing with the tutorial, we recommend reaching out to your infrastructure provider to inquire about the types and quantities of GPUs associated with each flavor. Once you have this information, you can choose one of the following flavors to proceed:","Enter 8 to use","Enter q to complete the change.","First, we'll use the moreh-smi command to check the currently used MoAI Accelerator.","In MoAI Platform, physical GPUs are not directly exposed to users. Instead, virtual MoAI Accelerators are provided, which are available for use in PyTorch. By setting the accelerator's flavor, you can determine how much of the physical GPU will be utilized by PyTorch. Since the total training time and GPU usage cost vary depending on the selected accelerator flavor, users should make decisions based on their training scenarios. If needed, please refer to the LLM Fine-tuning Parameter Guide to select the accelerator Flavor that aligns with your training objectives.","In this tutorial, we will use a 2048GB-sized MoAI Accelerator.","Now you can see that it has been successfully changed to .","Please refer to the document above or reach out to your infrastructure provider to inquire about the GPU types and quantities corresponding to each flavor.","Remember when we checked the MoAI Accelerator in the Llama2 13B Fine-tuning - Getting Started? Now let's set up the accelerator needed for learning.","Select when using KT Cloud's Hyperscale AI Computing.","Select when using Moreh's trial container.","The current MoAI Accelerator in use has a memory size of 512GB.","Therefore, after switching from the initially set flavor to , we will use the moreh-smi command to confirm that the change has been successfully applied.","To confirm that the changes have been successfully applied, use the moreh-smi command again to check the currently used MoAI Accelerator.","You can enter the number to switch to a different flavor.","You can utilize the moreh-switch-model command to review the available accelerator flavors on the current system. For seamless model training, consider using the moreh-switch-model command to switch to a MoAI Accelerator with larger memory capacity."]},{"l":"Training Execution","p":["Execute the given train_llama2.py script.","If the training proceeds smoothly, you should see the following logs. By going through this logs, you can verify that the Advanced Parallelism feature, which determines the optimal parallelization settings, is functioning properly. It's worth noting that, apart from the single line of AP code we looked at earlier in the PyTorch script, there is no handling for using multiple GPUs simultaneously in other parts of the script.","You can verify that the training is proceeding smoothly by checking the training logs.","The throughput displayed during training indicates how many tokens are being trained per second through the PyTorch script.","Throughput when using 16 AMD MI250 GPUs: Approximately 150,000 tokens/sec","Here are the approximate training times based on the type and number of GPUs.","Training time when using 16 AMD MI250 GPUs: Approximately 4 hours"]},{"l":"Checking Accelerator Status During Training","p":["During training, open another terminal and connect to the container. Then, execute the moreh-smi command to observe the MoAI Accelerator occupying memory and the training script running. Make sure to check this while the initialization process is completed and the training loss appears in the execution logs."]}],[{"l":"4. Checking Training Results","p":["Upon running the train_llama2.py script as described earlier, the resulting model will be saved in the llama2_summarization directory. This model is a pure PyTorch parameter file and is fully compatible with regular GPU servers, not just the MoAI Platform.","You can test the trained model using the inference_llama2.py script located under the tutorial directory of the pre-downloaded GitHub repository.","For testing, articles related to English Premier League (EPL) match results have been used.","Run the train script.","From the output, you'll notice that Llama2 has appropriately summarized the contents of the prompt."]}],[{"l":"5. Changing the Number of GPUs","p":["Let's rerun the fine-tuning task with a different number of GPUs. MoAI Platform abstracts GPU resources into a single accelerator and automatically performs parallel processing. Therefore, there is no need to modify the PyTorch script even when changing the number of GPUs."]},{"l":"Changing Accelerator type","p":["Switch the accelerator type using the moreh-switch-model tool. For instructions on changing the accelerator, please refer again to the 3. Model fine-tuning.","AMD MI250 GPU with 32 units","When using Moreh's trial container: select","When using KT Cloud's Hyperscale AI Computing: select","AMD MI210 GPU with 64 units","AMD MI300X GPU with 16 units"]},{"l":"Training Parameters","p":["Run the train_llama2.py script again.","Since the GPU memory has doubled, let's increase the batch size from the previous 256 to 512 and run the code again.","If the training proceeds smoothly, you'll see logs similar to the following:","Compared to the previous results obtained when the GPU count was halved, you'll notice that the training is progressing similarly, but with an improved throughput.","When using AMD MI250 GPU 16 → 32 : approximately 150,000 tokens/sec → 315,000 tokens/sec"]}],[{"l":"6. Conclusion","p":["From this tutorial, we have seen how to fine-tune Llama2 13B for text summarization on the MoAI Platform. Open-source LLMs like Llama can be utilized for various natural language processing tasks such as summarization, question answering, and more. With the MoAI Platform, you can easily configure the required number of GPUs without any code modifications.","The availability of large language models like LLaMA 2, fine-tuning techniques, and the MoAI Platform makes it possible for anyone to develop powerful AI applications. So please start repeating the same process outlined here on your own data.","In case if you still have any questions regarding this tutorial feel free to ask Moreh( support@moreh.io)."]},{"l":"Learn More","p":["MoAI Platform's Advanced Parallelization (AP)","Llama3 8B Fine-tuning","Llama3 70B Fine-tuning","Mistral Fine-tuning","GPT Fine-tuning","Qwen Fine-tuning","Baichuan2 Fine-tuning"]}],[{"l":"Supported Documents","p":["Moreh Toolkit Guide","Prepare Fine-tuning on MoAI Platform","Advanced Parallelization (AP)","LLM Fine-tuning Parameter Guide","For technical support questions, it is best to report issue and we will investigate right away.","If you have a general inquiry, please feel free to contact us at support@moreh.io."]}],[{"l":"Moreh Toolkit Guide","p":["The Moreh Toolkit is a command line tool designed to efficiently manage and monitor MoAI Accelerator on the MoAI Platform. With just three commands ( moreh-smi, moreh-switch-model, update-moreh), users can effectively manage MoAI Accelerators and easily update MoAI Platform."]},{"l":"Key Features","p":["The main features of the Moreh Toolkit are as follows:","Monitoring MoAI Accelerators:","Use the moreh-smi command to monitor memory usage and process status in real-time.","Switching AI Accelerators:","Use the moreh-switch-model command to change AI accelerators and execute processes to achieve optimal performance.","Updating and Rolling Back MoAI Platform:","Use the update-moreh command to update MoAI Platform to the latest version or roll back to a previous version if needed."]},{"i":"moai-accelerator-monitoring-moreh-smi","l":"MoAI Accelerator Monitoring: moreh-smi","p":["moreh-smi is a command-line tool that allows users to manage and monitor the MoAI Accelerator. You can run it in a conda environment where MoAI Platform PyTorch is installed.","If you are currently running a training session using the MoAI Accelerator, running moreh-smi in another terminal session will display the running process information as follows. You can also use moreh-smi to quickly identify your Job ID, allowing for faster support response from MoAI Platform in case of training or inference issues. In the example below, the Job ID is 976356."]},{"i":"utilizing-moai-accelerators-multi-accelerator-feature","l":"Utilizing MoAI Accelerator's Multi Accelerator Feature","p":["By default, if users do not configure anything, there will only be one MoAI Accelerator in a VM or container environment. With one MoAI Accelerator, only one process can run. However, there may be cases where you want to run multiple processes concurrently in the same environment, even with a single MoAI Accelerator. For example, you may want to run multiple training experiments concurrently by changing the same source code or hyperparameters. In such cases, you can create multiple MoAI Accelerators within a single token using moreh-smi, enabling multiple processes to run concurrently.","Let's explore adding, modifying, and removing MoAI Accelerators with the following examples."]},{"l":"Adding MoAI Accelerators","p":["First, let's add a MoAI Accelerator. When you enter the moreh-smi device --add command to use two or more MoAI Accelerators, you will see the following interface.","When you input the integer corresponding to the model you want to use from 1 to 13, a MoAI Accelerator corresponding to the entered device number will be created with the message \"Create device success.\" Within one environment, you can create a maximum of 5 AI accelerators. If you need to create more MoAI Accelerators, please contact your infrastructure administrator.","In the example below, let's add the 10th MoAI Accelerator:"]},{"i":"changing-the-default-moai-accelerator-moreh-smi-device---switch","l":"Changing the Default MoAI Accelerator: moreh-smi device --switch","p":["moreh-smi device --switch {Device_ID} is a command that allows you to change the default MoAI Accelerator.","It can be used as follows:","This means that the current default MoAI Accelerator has been changed to Accelerator 1."]},{"i":"removing-moai-accelerators-moreh-smi-device---rm","l":"Removing MoAI Accelerators: moreh-smi device --rm","p":["This time, let's try to remove a specific accelerator corresponding to the specified device ID with the command moreh-smi device --rm {Device_ID}.","The MoAI Accelerator with Device ID 1, , has been removed using the above command. To confirm, when you run moreh-smi again, you will notice that the device has been removed."]},{"l":"Other Various Options Utilization","p":["moreh-smi provides various other options. You can use the --help option to see what options are available.","moreh-smi -p- Monitor detailed hardware status of MoAI Accelerators.","moreh-smi -t- Check MoAI Accelerator token information.","If you encounter issues during training, such as tangled processes or difficulty terminating, causing messages like \"Process Running,\" use the moreh-smi --reset command."]},{"i":"changing-moai-accelerators-moreh-switch-model","l":"Changing MoAI Accelerators: moreh-switch-model","p":["moreh-switch-model is a tool that allows you to change the flavor (specifications) of the currently configured MoAI Accelerator. By changing the flavor of the MoAI Accelerator, you determine how much GPU memory to use.","It can be used as follows:","For example, if the result of the moreh-smi command is as follows, it means that the \"MoAI Platform AI Accelerator model currently set as the default is Accelerator 0, and this MoAI Accelerator is of type model.\"","The moreh-switch-model command displays the following prompt:","If you enter an integer corresponding to the model to be used from 1 to 13 (device number), the message \" The MoAI Platform AI Accelerator model is successfully switched to {model_id}. will be displayed, and the MoAI Accelerator corresponding to the entered device number will be changed.","Let's change the MoAI Accelerator to as follows:","You can continue with the change or exit the MoAI Accelerator change by typing q or Q.","After the change is complete, when you run moreh-smi again to confirm, you will see the following result:","The MoAI Accelerator previously set as the Small.64GB model has been changed to the Large.256GB model."]},{"i":"updating-moai-platform-update-moreh","l":"Updating MoAI Platform: update-moreh","p":["update-moreh is a command that allows you to create a new conda environment and install MoAI Platform on it, or update the version of MoAI Platform already installed in the conda environment. You can use update-moreh in the following situations:","If you have created a new conda environment and MoAI Platform Python packages need to be installed, you can easily install the latest version of MoAI Platform using the update-moreh command.","If you want to use the latest version of MoAI Platform even in an existing conda environment where MoAI Platform is already installed, you can update the currently used MoAI Platform to the latest version using the update-moreh command alone.","There may be cases where you need to install a specific version of the MoAI Platform. In such cases, you can use the --target option to specify the specific version you want to install.","If the MoAI Platform is not functioning properly due to issues such as dependency conflicts between other packages in the conda environment, you may need to reconstruct the conda environment. In such cases, you can use update-moreh to restore the MoAI Platform within the conda environment. In the latter case, you can use the --force option to reconstruct the environment. (Can be used with the —-target option)"]}],[{"l":"Prepare Fine-tuning on MoAI Platform","p":["The MoAI Platform can be configured with various GPUs, yet it provides a consistent user experience through a unified interface (CLI). This uniform access allows all users to interact with the system in the same way, making it more efficient and intuitive.","The MoAI Platform supports Python-based programming, similar to typical AI training environments. This document focuses on setting up and using a conda virtual environment as the standard configuration for AI training."]},{"l":"Setting up a Conda Environment","p":["To begin training, first create a conda environment:","Replace my-env with your desired environment name.","Activate the conda environment:","The MoAI Platform supports various PyTorch versions, allowing you to choose the one that fits your needs.","Use the moreh-smi command to check the version of the installed Moreh solution and the details of the MoAI Accelerator in use. The current MoAI Accelerator is . For more information about the MoAI Accelerator, refer to the specifications.","For optimal parameters recommended for fine-tuning each model on the MoAI Platform, refer to the LLM Fine-tuning parameter guide","For detailed usage of the moreh toolkit, including moreh-smi and moreh-switch-model, please refer to the Using the MoAI Platform Toolkit"]}],[{"i":"advanced-parallelizationap","l":"Advanced Parallelization(AP)"},{"i":"what-is-advanced-parallelization","l":"What is Advanced Parallelization?","p":["The MoAI Platform's Advanced Parallelization (AP) is the automatic model optimization and distributed parallel processing feature. Typically, ML engineers go through numerous trial and error processes to optimize model parallelization during training of large-scale models. They experiment with various parallelization techniques, considering the memory size of the GPU in use, measure performance for different option combinations available in each technique, and determine optimized environment variables. This is a very laborious task that can take skilled machine learning developers from weeks to months.","With the AP feature of the MoAI Platform, complex parallel processing and model optimization tasks can be automatically performed with just a single line of code, dramatically reducing the time and effort involved in training.","Training throughput: experienced developer optimizations over one month vs. using the AP feature on the MoAI Platform"]},{"i":"why-is-parallelization-crucial","l":"\uD83D\uDCA1 Why is parallelization crucial?","p":["As a simple example, let's calculate how much GPU memory is required to train the Llama2 13B model.","The Llama2 13B model contains approximately 13 billion parameters. The memory size required to load the model, depending on the FP16 data format, is approximately 25GB. A minimum of 100-150GB of memory is required for training components such as the optimizer and gradients. Therefore, training is impossible with just the memory capacity of a typical single GPU (80-128GB). This is why GPU parallel processing is essential for model training.","For example, when using FSDP (Fully Sharded Data Parallel) or DeepSpeed, developers must manually adjust various parallelization settings. In this case, the following parameters must be carefully adjusted:","Parameter Sharding: FSDP requires specifying how to shard model parameters across GPUs. Incorrect settings can result in suboptimal performance or memory overflow errors.","Optimizer Stat Sharding: Both FSDP and DeepSpeed require sharding the optimizer state for efficient memory usage and communication overhead, which entails complex configurations.","Activation Checkpointing: Activation checkpointing may need to be activated to save memory, balancing additional computation overhead for saving memory and recalculating activations during backpropagation.","Users can focus on the goal of model training rather than the time-consuming procedure of configuring parallelization settings. With a single line of code below, the platform automatically handles the complexities of parallelization operations to assure optimal performance.","As a result, when training large models, users can easily obtain optimal parameters and environment variable combinations for parallelization techniques such as Data Parallelism or Pipeline Parallelism."]}],[{"l":"Utilizing AP","p":["The AP feature enables parallelization at the node level. Therefore, it is recommended to use multi-node accelerators when using AP. Before using the AP feature, please check the information on the accelerators you are using."]},{"l":"How to Apply the AP Feature","p":["The AP feature can be applied by adding a single line of code after import torch:"]},{"l":"Example Usage","p":["If you have an environment with two or more nodes ready, you can now create training code to use the AP feature. In this guide, we'll set up code using the Llama2 model. Note that the Llama2 model requires community license agreement and Hugging Face token information. Please refer to Llama2 Tutorial 1. Preparing for fine-tuning to prepare the training code.","Once the training code is ready, configure the PyTorch environment before running the training on the MoAI Platform. The example below shows the PyTorch 1.13.1+cu116 version running on MoAI Platform version 24.2.0.","Once the PyTorch environment is set up, fetch the training code from the GitHub repository.","Clone the quickstart repository and check the quickstart/ap-example directory. You'll find the text_summarization_for_ap.py file prepared by Moreh for testing the AP feature. Let's apply the AP feature using this code.","The training configuration for testing is as follows. We will proceed with testing based on this configuration.","Batch Size: 64","Sequence Length: 1024","MoAI Accelerator: 4xLarge"]},{"l":"Enabling the AP Feature","p":["At the beginning of the program's main function, there's a line to enable the AP feature. Apply AP and then run the training as shown below.","When the training starts, you will see logs like the following:","As shown, by adding just one line to enable the AP feature, complex distributed parallel processing is executed, and training progresses. Next, we'll explain the scenario users might encounter if they do not use the AP feature."]},{"l":"Disabling the AP Feature","p":["Let's examine the situation when the AP feature is not used. To verify this, comment out the line that enables the AP feature at the beginning of the Python program's main function.","Then proceed with the training.","After the training completes, you will see logs as the following.","In the above logs, you can see the message RuntimeError: Error Code 4: OUT_OF_MEMORY, indicating an Out of Memory (OOM) error caused by trying to load data exceeding the VRAM of the 1 device chip, which is 64GB.","If you were using a framework other than MoAI Platform, you would experience such inconvenience. However, as a user of the MoAI Platform, you can easily solve the troublesome OOM problem by applying the AP feature with just one line, without spending a long time calculating and deliberating separate parallelization optimizations."]}],[{"l":"Learn More About AP","p":["Let's take a closer look at the logs related to AP.","The MoAI Platform generates various optimized configurations for parallel processing to find the best optimization. The following log indicates that the Compiler Config Generator has set the number of candidate configurations for parallelization to 30.","Then, it generates an operation graph for each candidate.","From the above log, we can see that it took approximately 6.1 seconds to compile the configurations.","Next, it estimates the possible candidate configurations again.","Thus, it confirms that there are a total of 7 possible configurations.","Now, the graph simulator calculates the cost for each configuration, and once the calculation is complete, it selects the optimal configuration as the final choice.","The log above shows that it took about 0.8 seconds to calculate the cost until one final configuration was selected.","This information is recorded in a file named advanced_parallelization_selected_config.dump, which is created in the location where the Python program is executed. Now, let's see how advanced_parallelization_selected_config.dump looks like.","In this way, by adding just one line of code, it is possible to compute multiple parallelization candidates and achieve optimal parallelization."]}],[{"l":"LLM Fine-tuning Parameter Guide","p":["1,112,135 MiB","1,121,814 MiB","1,147,841 MiB","1,366,564 MiB","1,403,640 MiB","1,541,212 MiB","1,764,955 MiB","1,853,432 MiB","1,899,079 MiB","1,951,344 MiB","100m","1024","128","13,286 TPS","140m","144m","14m","150,406 TPS","15m","16","16m","17m","18,001 TPS","190,433 TPS","190,949 TPS","191,605 TPS","197,489 TPS","19m","2,089,476 MiB","2,845,656 MiB","2048","220m","24.5.0","249m","256","28m","2xlarge","3,460,240 MiB","30m","315,004 TPS","32","381,714 TPS","384,165 TPS","392,573 TPS","394,605 TPS","4xlarge","512","560m","6,841 TPS","62m","699,751 MiB","758,555 MiB","762652 MiB","78,274 TPS","78m","798,760 TPS","81m","866,656 MiB","867,021 MiB","8xlarge","93,291 TPS","95302 TPS","99,873 TPS","Advanced Parallelism is applied","Baichuan2 13B","batch size","Cerebras GPT 13B","Llama2 13B","Llama3 8B","Mistral 7B","MoAI Accelerator","MoAI Platform version","Model","Please note that the names specified for MoAI Accelerators may vary depending on the Cloud Service Provider (CSP) you are using.","Qwen1.5 7B","sequence length","This guide provides the optimal parameters recommended by the MoAI Platform and should be used as a reference during your training.","throughput","Training Time","True","vram Usage"]}]];