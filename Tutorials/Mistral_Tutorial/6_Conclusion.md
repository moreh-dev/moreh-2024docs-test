---
icon: terminal
tags: [tutorial, mistral]
order: 40
---

# 6. Conclusion

From this tutorial, we have seen how to fine-tune the [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) model on the MoAI Platform. With MoAI Platform, you can easily fine-tune PyTorch-based open-source LLM models on GPU clusters while retaining your existing training code. Additionally, using the MoAI platform, you can easily configure the number of GPUs you need without any code changes. So please don’t hesitate to dive in and develop new models quickly and effortlessly with your data!

## Learn more

- *[MoAI Platform의 자동병렬화 기능,  Advanced Parallelization (AP)](/Supported_Documents/)*
- [Llama2 Fine-tuning](/Tutorials/Llama2_Tutorial/index.md)
- [GPT Fine-tuning](/Tutorials/GPT_Tutorial/index.md)
- [Baichuan2 Fine-tuning](/Tutorials/Baichuan2_Tutorial/index.md)
- [Qwen Fine-tuning](/Tutorials/Qwen_Tutorial/index.md)