---
icon: terminal
tags:  [tutorial, gpt]
order: 40
---

# 6. Conclusion

So far, we have looked at the process of fine-tuning the GPT-based model from HuggingFace on the MoAI Platform. With MoAI Platform, you can easily fine-tune PyTorch-based open-source LLM models on GPU clusters while maintaining existing training code. Additionally, if you use the MoAI platform, you can easily configure the number of GPUs you need without changing any code. Try developing new models quickly and effortlessly with your data.

# Learn More

- *[MoAI Platform의 자동병렬화 기능,  Advanced Parallelization (AP)](/Supported_Documents/)*
- [Llama2 Fine-tuning](/Tutorials/Llama2_Tutorial/index.md)
- [Mistral Fine-tuning](/Tutorials/Mistral_Tutorial/index.md)
- [Baichuan2 Fine-tuning](/Tutorials/Baichuan2_Tutorial/index.md)
- [Qwen Fine-tuning](/Tutorials/Qwen_Tutorial/index.md)