---
icon: terminal
tags: [tutorial, llama3_70b]
order: 50
---

# 1. MoAI Platform 병렬화 - Llama3 70B

Llama3 70B 모델 전체를 파인튜닝하기 위해서는 필수적으로 다중 GPU를 사용해야 하며, 텐서 병렬화(Tensor Parallelism), 파이프라인 병렬화(Pipeline Parallelism), 데이터 병렬화(Data Parallelism) 등의 병렬화를 수행해야 합니다. 이를 위해서는 Deepspeed와 같은 도구를 사용하여 복잡한 config 설정을 해야 하며, 파이프라인 병렬화의 경우 모델을 잘 이해하고 활용할 줄 알아야 합니다. 또한, 가장 효과적인 병렬화 방식을 찾기 위해서는 여러 번 코드를 수정하고 모델을 학습시켜보며 최적의 조합을 찾아야 합니다. 따라서 다중 GPU를 사용하기 위해서는 사용자에게 많은 노력이 필요합니다.

그러나 MoAI Platform에서는 **복잡한 코드 수정이나 모델에 대한 깊은 이해 없이도 자동 병렬화와 최적의 조합을 제공하여 사용자가 간편하게 모델을 학습할 수 있도록 지원**합니다.

## Fine-tuning Code

**MoAI Platform의 모든 코드는 일반적인 PyTorch 사용 경험과 동일합니다.** Llama3 70B를 학습하기 위해 기존 PyTorch에서 하나의 GPU를 사용하는 것처럼 스크립트를 작성할 수 있습니다.

```python
torch.moreh.option.enable_advanced_parallelization()
    
model = AutoModelForCausalLM.from_pretrained("meta-llama/Meta-Llama-3-70B")
tokenizer = LlamaTokenizer.from_pretrained("meta-llama/Meta-Llama-3-70B")
    
# Compose pad token mask
def create_mask(input_ids, tokenizer):
	pad_token_ids = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id
	return (input_ids != pad_token_ids).long() 
			   
# Mask pad tokens for training
def mask_pads(inputs, tokenizer, ignore_index = -100):
	idx_mask = create_mask(inputs, tokenizer)
	labels = copy.deepcopy(inputs)
	labels[~idx_mask.bool()] = ignore_index
	return labels

# Define AdamW optimizer
optim = AdamW(model.parameters(), lr=args.lr)

# Start training
for epoch in range(args.num_train_epochs):
	for step, batch in enumerate(train_dataloader, start=1):
		start_time = time.perf_counter()
		input_ids = batch["input_ids"]
		inputs, labels = input_ids, mask_pads(input_ids, tokenizer)
		attn_mask = create_mask(inputs, tokenizer)
		outputs = model(
			input_ids.cuda(),
			attention_mask=attn_mask.cuda(),
			labels=labels.cuda(),
			use_cache=False,
		)
		loss = outputs[0]
		loss.backward()

		optim.step()
		model.zero_grad(set_to_none=True)
```

기존 PyTorch와 다르게 MoAI Platform에서는 다음과 같은 코드 한 줄이 추가됩니다. 이 코드는 MoAI Platform의 자동 최적화 및 병렬화를 설정하여 사용자가 간단하게 Llama3 70B를 fine-tuning할 수 있게 합니다.

```python
torch.moreh.option.enable_advanced_parallelization()
```

이제 MoAI Platform에서 MI250 GPU 64개를 사용한 Llama3 70B 모델 fine-tuning 튜토리얼을 시작하겠습니다. 이 튜토리얼을 통해 MoAI Platform에서 다중 GPU를 사용하는 방법이 얼마나 쉽고 효과적인지 확인할 수 있을 것입니다.
