window.__DOCS_SEARCH__ = [[{"l":"MOREH DOCS","p":["MoAI(Moreh AI appliance for AI accelerators) 는 대규모 딥러닝 모델 개발에 필수적인 그래픽 처리 장치(GPU)를 손쉽게 제어할 수 있는 확장 가능한 AI 플랫폼입니다."]},{"l":"Getting Started","p":["Fine-tuning 시작하기 MoAI Platform을 처음 사용하는 사용자에게 필요한 정보 안내","AP 가이드 Advanced Parallelization (AP) 기능 사용 안내","Moreh Toolkit command line 사용방법","MoAI Platform Features MoAI Platform의 가상화와 병렬화 기능","MoAI Platform은 현재 개발이 계속 진행되고 있습니다. 따라서, 문서의 내용은 언제든지 변경될 수 있습니다."]},{"l":"MoAI Platform 핵심 기술","p":["딥러닝 모델이 발전하면서 수십억, 수백억 개의 파라미터를 포함하는 복잡한 구조가 되었고, 이에 따라 대규모 컴퓨팅 자원이 AI 인프라의 중요한 부분이 되었습니다. 대규모 컴퓨팅 자원을 사용하여 모델을 개발하려면 모델의 병렬 처리와 클러스터 환경의 수동 설정과 같이 학습 프로세스를 최적화하는 과정이 필수적입니다. 특히, GPU 및 노드 관리를 통한 학습 최적화는 개발자들에게 많은 시간과 노력을 요구합니다.","MoAI Platform은 이러한 문제를 해결하기 위해 다음과 같은 기능을 제공하여 대규모 AI 시대에 효율적인 인프라를 지원합니다.","다양한 가속기, 다중 GPU 지원","GPU 가상화","동적 GPU 할당","AI Compiler 자동 병렬화"]}],[{"l":"개요"},{"i":"moai-platform이란","l":"MoAI Platform이란?","p":["MoAI Platform은 대규모 딥러닝 모델을 개발하기 위한 확장 가능한 AI 플랫폼으로, 수천 대로 이루어진 GPU 클러스터를 쉽게 제어하여 AI 모델을 학습하거나 추론할 수 있습니다."]},{"l":"MoAI Platform 핵심 기술","p":["딥러닝 모델이 발전하면서 수십억, 수백억 개의 파라미터를 포함하는 복잡한 구조가 되었고, 이에 따라 대규모 컴퓨팅 자원이 AI 인프라의 중요한 부분이 되었습니다. 대규모 컴퓨팅 자원을 사용하여 모델을 개발하려면 모델의 병렬 처리와 클러스터 환경의 수동 설정과 같이 학습 프로세스를 최적화하는 과정이 필수적입니다. 특히, GPU 및 노드 관리를 통한 학습 최적화는 개발자들에게 많은 시간과 노력을 요구합니다. MoAI Platform은 이러한 문제를 해결하기 위해 다음과 같은 기능을 제공하여 대규모 AI 시대에 효율적인 인프라를 지원합니다.","다양한 가속기, 다중 GPU 지원","GPU 가상화","동적 GPU 할당","AI Compiler 자동 병렬화"]},{"l":"1. 다양한 AI 가속기 지원","p":["MoAI Platform은 다양한 AI 가속기를 지원하기 때문에 가속기 종류와 상관없이 다양한 모델 학습과 추론 작업을 실행할 수 있습니다. 사용자는 AMD, Intel 및 NVIDIA 외의 다른 AI 가속기와 함께 사용할 수 있으며, 이를 위해 기존에 활용하던 Python으로 작성된 학습 및 추론 코드를 수정할 필요가 없습니다."]},{"l":"2. GPU 가상화","p":["MoAI Platform의 가상화 기능은 수천 개의 GPU를 하나의 GPU처럼 사용할 수 있게 합니다(Single Virtual Device). 이를 통해 멀티 GPU, 멀티 노드 병렬화 작업과 같은 최적화 프로세스가 필요없이 1개의 GPU를 가정하고 Python 코드를 구성할 수 있기 때문에 AI 엔지니어가 쉽고 빠르게 딥러닝 학습 및 추론을 실행할 수 있습니다.","필요에 따라 GPU 자원을 확장하거나 축소할 수 있어 서비스의 확장성을 높일 수 있습니다. MoAI Platform에서는 간단한 명령어 한 줄로 1개의 Single Virtual Device 로 가상화될 GPU 자원을 손쉽게 확장하고 축소할 수 있습니다."]},{"l":"3. 동적 GPU 할당","p":["Public Cloud에서는 VM 인스턴스 생성 시 과금이 시작되며, GPU를 변경하려면 인스턴스를 다시 생성해야 합니다. 또한 선택한 가상 머신이 한 번 확정되면 학습에 따라 유연하게 변경하기 어려운 경우도 많습니다.","MoAI Platform은 실제로 연산중일 때만 AI 가속기 사양에 따라 분 단위로 요금이 부과 할 수 있는 설계이므로, 유저의 GPU 실사용시에만 과금하는 완전한 종량제 방식이 가능합니다. 이용자의 사용 패턴에 맞추어 기존 클라우드 서비스의 GPU를 특정 가상머신(VM)에 종속시키는 Passthrough 방식 대비 대규모의 비용 절감이 가능합니다."]},{"l":"4. AI Compiler 자동 병렬화","p":["딥러닝 모델은 여러 개의 레이어로 이루어져 있고, 각 레이어는 다양한 연산을 포함하고 있습니다. 이러한 연산들은 독립적으로 학습될 수 있어 병렬 처리가 가능합니다. 그러나 이를 위해 ML 엔지니어는 파라미터와 환경 변수의 조합을 수동으로 설정해야 합니다. MoAI Platform의 자동 병렬화 기능은 최적의 병렬화 환경 변수 조합을 신속하게 결정합니다. 따라서 사용자는 대규모 모델 훈련 시 필요한 Data Parallelism, DDP, Pipeline Parallelism, and Tensor Parallelism 등의 병렬화 기법을 자동으로 적용하여 모델을 훈련할 수 있습니다.","인공지능 시대에는 대형 언어 모델(LLM) 및 대형 멀티모달 모델(LMM)과 같은 대규모 모델의 훈련 및 추론에 상당한 규모의 GPU 클러스터와 효과적인 GPU 병렬화가 필요합니다.","현재 NVIDIA와 함께 사용되는 일반적인 AI 프레임워크는 모델의 크기와 복잡성, 그리고 사용 가능한 GPU의 크기나 클러스터에 따라 AI 엔지니어가 병렬화를 수동으로 조정해야 합니다. 이 과정은 시간이 많이 소요되며 종종 몇 주가 걸립니다.","MoAI 플랫폼은 특정 AI 모델과 GPU 클러스터의 크기를 기반으로 GPU 리소스를 최적으로 활용하는 Moreh AI 컴파일러를 통해 자동 병렬화를 제공합니다.","자동 병렬화를 통해 NVIDIA 환경(플랫폼)에서 몇 주가 소요되는 모델 훈련을 대략 2~ 3일로 대폭 단축할 수 있습니다."]}],[{"l":"MoAI Platform Features","p":["MoAI Platform은 수백 개의 GPU를 가상화하여 단일 가속기로 처리하며, 병렬화를 통해 복잡한 GPU 클러스터 관리의 부담을 줄여줍니다. 아래 문서는 MoAI Platform 이 제공하는 대표적인 두 가지 기능에 대한 작동 원리 및 개념을 기술합니다.","GPU 가상화 및 동적 할당","자동 병렬화"]}],[{"i":"gpu-가상화-moai-accelerator","l":"GPU 가상화: MoAI Accelerator","p":["MoAI Platform은 수십, 수백 GPU노드의 대형 GPU 클러스터를 MoAI Accelerator라는 단일 가속기로 가상화하여 사용자에게 제공합니다. 사용자는 멀티 노드 사용에 따른 모델 병렬화, 클러스터 환경 수동 설정 등의 작업에 대한 고민없이 단일 가속기를 사용하는 방식으로 모델을 설계하고 학습할 수 있습니다.","MoAI Accelerator는 터미널에 moreh-smi 명령어를 입력해 확인할 수 있습니다.","출력된 내용을 확인해 보면 사용자는 2048 GB의 메모리를 갖춘 단일 가속기를 사용하는 것처럼 보입니다. 그러나 실제로는 각각 4개의 GPU로 구성된 4개의 노드를 사용하게 됩니다.","가장 많이 사용되는 딥러닝 프레임워크 중 하나인 PyTorch에서 MoAI Accelerator를 제대로 인식하는지 확인해보겠습니다. Python 인터프리터에서 cuda API를 사용해 현재 사용 가능한 가속기의 수를 출력해보면 PyTorch가 MoAI Accelerator를 하나의 가속기로 인식하고 있음을 확인할 수 있습니다.","여기서 중요한 점은 실제 사용자가 사용하는 환경에는 물리적인 GPU가 없다는 사실입니다. 사용자가 PyTorch와 같은 딥러닝 프레임워크에서 cuda 와 같은 API를 통해 GPU 가속기를 사용하려고 할 때, MoAI Platform은 GPU 클러스터 자원을 자동으로 할당합니다."]},{"l":"MoAI Platform의 GPU 동적 할당","p":["MoAI Platform은 GPU할당을 프로세스 단위로 동적으로 처리합니다. 때문에 사용자가 PyTorch와 같은 딥러닝 프레임워크를 사용하여 모델을 학습하고 추론할 때 물리GPU를 효율적으로 할당받을 수 있게 해줍니다. 이는 사용자가 미리 정의된 MoAI Accelerator의 다양한 flavor 중에서 원하는 GPU 수를 선택하고 필요에 따라 언제든지 조정할 수 있는 유연성을 제공합니다.","반면에, 기존 클라우드 플랫폼들은 일반적으로 인스턴스를 생성하는 순간부터 물리적 GPU를 고정적으로 할당받습니다. 이는 사용자가 GPU 수를 변경하거나 더 이상 GPU를 사용하지 않으려 할 때, 기존 인스턴스를 삭제하거나 컨테이너를 종료한 후 재시작해야 하는 번거로움을 초래합니다. MoAI Platform의 동적 할당 방식은 이러한 불편을 획기적으로 줄여줍니다.","간단한 예시를 통해 MoAI Accelerator의 flavor를 변경하는 방법을 확인해보겠습니다.","먼저 터미널에 moreh-smi 명령어를 입력해 현재 사용중인 MoAI Accelerator를 확인합니다.","현재 사용자가 사용중인 MoAI Accelerator의 flavor는 인 것을 확인할 수 있습니다. 더 큰 규모의 모델을 학습해야 하거나 학습 속도를 향상시키기 위해 더 많은 GPU를 사용하고 싶다면 간단하게 moreh-switch-model 명령어를 입력해 변경할 수 있습니다.","다시 moreh-smi 명령어를 입력해 사용중인 MoAI Accelerator의 flavor를 확인해보면 로 잘 변경된 것을 확인할 수 있습니다."]},{"l":"마무리","p":["MoAI Platform은 MoAI Accelerator라는 가상화 기술을 통해 복잡한 멀티 노드 GPU 클러스터를 단순화하고, 사용자에게 강력하면서도 유연한 컴퓨팅 환경을 제공합니다. 사용자가 복잡한 설정과 관리 작업 없이도 모델 크기와 GPU 수를 자유롭게 조정할 수 있는 환경을 제공하여, 효율적인 리소스 활용이 가능하게 합니다. MoAI Platform에서 MoAI Accelerator를 활용해 보다 신속하고 효율적으로 딥러닝 모델을 설계하고 학습해 보세요."]}],[{"l":"자동 병렬화","p":["MoAI Platform에서 사용자는 가상화된 하나의 GPU인 MoAI Accelerator를 사용하게 됩니다. 따라서 MoAI Platform 상에서 사용자는 별도 병렬화 작업없이 하나의 GPU를 사용하는 것을 가정하고 코드를 작성하게 됩니다. 그렇다면 MoAI Platform에서는 어떻게 여러 개의 GPU를 사용할 수 있을까요?","MoAI Platform은 사용하는 GPU 수에 따라 자동으로 가장 좋은 최적화 및 병렬화를 수행합니다.","예를 들어, 사용자가 8개의 GPU를 사용하는 가속기 flavor를 선택하여 학습을 시작하면 MoAI Platform은 자동으로 전체 배치 크기를 8등분하여 각 GPU에 분배하여 처리합니다. 더 자세한 설명을 위해 Llama3-8b 모델을 fine-tuning하는 상황을 가정해보겠습니다. 사용자가 4개의 GPU를 사용하는 가속기 flavor를 선택하고 배치 크기를 16으로 설정한다면, 각 GPU당 4개의 배치가 자동으로 분배되어 대략 1초에 12만 5천 개의 토큰을 처리하는 것을 확인할 수 있습니다.","더 빠르고 효율적인 학습을 위해 더 많은 GPU를 사용하는 가속기 flavor를 선택하고 배치 크기를 키울 수도 있습니다. 사용자가 16개의 GPU를 사용하는 가속기 flavor를 선택하고 배치 크기를 64로 설정한 후 학습을 시작하면, 1초에 처리되는 토큰 수를 기존 대비 4배로 키울 수 있습니다.","그런데 만약 사용자가 보다 더 큰 배치 크기를 사용하려는 경우 는 어떨까요? 추가적인 코드 수정이 없다면, 일반적인 GPU 클러스터에서는 Out of Memory(OOM) 에러가 발생할 가능성이 큽니다. 하지만 MoAI Platform은 자동으로 모델을 병렬화하여 학습을 진행시킬 수 있습니다.","사용자가 16개의 GPU를 사용하는 가속기 flavor를 선택하고, 배치 크기를 512로 설정하여 학습하면 자동으로 모델 병렬화와 데이터 병렬화가 동시에 적용돼 같은 개수의 GPU를 사용하더라도 더 큰 배치 크기로 학습할 수 있습니다.","이 외에도 70B와 같은 대형 모델도 별도의 추가 작업없이 자동으로 병렬화되기 때문에 간편하게 학습할 수 있습니다. 이처럼 MoAI Platform은 사용자가 사용하는 모델과 배치 크기 등에 따라 자동으로 최적화와 병렬화를 제공하여, 다중 GPU를 편리하고 효율적으로 사용할 수 있도록 합니다."]}],[{"l":"Fine-tuning 튜토리얼","p":["이 튜토리얼은 Llama2, Mistral 등의 대형 언어 모델을 fine-tuning 하고자 하는 모든 분들을 위한 것입니다. MoAI 플랫폼을 사용하여 아래 대형 언어 모델들을 미세 조정하는 과정을 안내합니다.","Llama3 8B","Llama3 70B","Mistral","GPT","Qwen","Baichuan2","Llama2 13B","머신러닝에서 미세 조정(fine-tuning)이란 사전 학습된 모델의 매개변수를 새로운 데이터로 조정하여 특정 작업의 성능을 향상시키는 것을 의미합니다. 즉, 기존 모델을 새로운 작업에 적용하고자 할 때, 새로운 데이터셋으로 모델을 최적화하여 특정 요구와 도메인에 맞게 커스터마이징하는 과정입니다.","사전학습된 모델은 범용성을 고려한 매우 큰 파라미터를 가지는 모델이며 큰 모델을 효과적으로 fine-tuning하려면 충분한 양의 학습 데이터가 필요합니다.","MoAI Platform에서는 GPU의 메모리 사이즈를 고려해 최적화된 병렬화 기법을 손쉽게 적용할 수 있어 학습 시작 전에 소요되는 시간과 노력을 획기적으로 줄일 수 있습니다."]},{"i":"이-튜토리얼에서-배우게-될-내용","l":"이 튜토리얼에서 배우게 될 내용:","p":["데이터셋, 모델, 토크나이저 로드하기","학습 실행 및 결과 확인하기","자동 병렬화 기능 적용하기","적절한 학습 환경 및 AI 가속기 선택 방법"]}],[{"l":"Llama3 8B Fine-tuning","p":["이 튜토리얼은 MoAI Platform에서 오픈 소스 Llama3-8b 모델을 fine-tuning하는 예시를 소개합니다. 튜토리얼을 통해 아래와 같은 MoAI Platform이 제공하는 여러 기능을 체험하며, AMD GPU 클러스터를 사용하는 방법을 익힐 수 있습니다.","사용자는 수십 개의 GPU를 MoAI Accelerator라는 하나의 가속기처럼 사용할 수 있어 복잡한 병렬화 작업이나 클러스터 환경 설정 없이도 쉽게 학습을 실행할 수 있습니다. 사용자는 리소스 관리에 신경쓰지 않고 학습에만 집중할 수 있습니다.","자동 병렬화 기능 덕분에 코드 작성과 개발이 간소화되며, 모델 학습 속도가 크게 향상됩니다. 이는 효율적인 자원 활용을 가능하게 하여 사용자가 더 빠르고 효과적으로 작업할 수 있도록 돕습니다."]},{"l":"개요","p":["MoAI Platform은 수천 대의 GPU를 쉽게 제어하여 AI 모델을 학습하거나 추론할 수 있는 확장 가능한 AI 플랫폼입니다. MoAI Platform의 특징은 모델을 fine-tuning할 때 가상화와 병렬화를 통해 매우 간단한 학습 방법을 제공한다는 점입니다.","MoAI Platform은 여러개의 GPU를 가상화하여 하나의 가속기인 MoAI Accelerator 로 제공합니다. 따라서 다중 GPU 사용을 위해 필요한 사전 준비나 코드 수정이 필요하지 않습니다.","MoAI Platform은 고객이 가상화된 MoAI Accelerator를 사용할 때 내부적으로 자동 최적화된 병렬화를 제공합니다. 모델 크기, 데이터 크기에 대해서 다양한 병렬화 방법을 고려해 최적의 병렬화 환경을 제공하며, 사용자는 별도의 작업이 없이 간단한 코드로 고성능 학습을 경험할 수 있습니다."]},{"l":"시작하기 전에","p":["MoAI Platform 상의 컨테이너 혹은 가상 머신을 인프라 제공자로부터 발급받고, 여기에 SSH로 접속하는 방법을 안내 받으시기 바랍니다. 예를 들어 MoAI Platform의 체험판 컨테이너 또는 MoAI Platform 기반으로 운영되는 퍼블릭 클라우드 서비스를 신청하여 사용할 수 있습니다.","MoAI Platform 체험판 컨테이너 사용 문의: support@moreh.io","KT Cloud Hyperscale AI Computing ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","SSH로 접속한 다음 moreh-smi 명령을 실행하여 MoAI Accelerator가 잘 표시되는지 확인하시기 바랍니다. 디바이스 이름은 시스템마다 다르게 설정되어 있을 수 있습니다."]},{"l":"MoAI Accelerator 확인","p":["이 튜토리얼에서 안내할 Llama3 모델과 같은 sLLM을 학습하기 위해서는 적절한 크기의 MoAI Accelerator를 선택해야 합니다. 먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","수행할 학습에 필요한 구체적인 MoAI Accelerator 설정에 대한 설명은 3. 학습 실행하기 에서 제공하겠습니다."]}],[{"l":"1. Fine-tuning 준비하기","p":["MoAI Platform에서 PyTorch 스크립트 실행 환경을 준비하는 것은 일반적인 GPU 서버에서와 크게 다르지 않습니다. 단, 튜토리얼 진행을 위해 아래의 사양들이 권장됩니다.","CPU: 16 core 이상","memory: 256GB 이상","MAF 버전: 24.5.0","스토리지: 40GB 이상","원할한 튜토리얼 진행 전 실행 환경을 확인하시길 바랍니다."]},{"l":"PyTorch 설치 여부 확인하기","p":["SSH로 컨테이너에 접속한 다음 아래와 같이 실행하여 현재 conda 환경에 PyTorch가 설치되어 있는지 확인합니다.","버전명에는 PyTorch 버전과 이를 실행시키기 위한 MoAI 버전이 함께 표시되어 있습니다. 위 예시의 경우 PyTorch 1.13.1+cu116 버전을 실행하는 MoAI의 24.5.0 버전이 설치되어 있음을 의미합니다.","만약 conda: command not found 메시지가 표시되거나, torch 패키지가 리스트되지 않거나, 혹은 torch 패키지가 존재하더라도 버전명에 “moreh”가 포함되지 않은 경우 MoAI Platform에서 Fine-tuning 준비하기 문서에 따라 conda 환경을 생성하십시오.","만약 해당 MoAI 버전이 24.5.0이 아닌 다른 버전이라면 아래의 코드를 실행시키십시오."]},{"l":"PyTorch 동작 여부 확인하기","p":["다음과 같이 실행하여 torch 패키지가 정상적으로 import되고 MoAI Accelerator가 인식되는지 확인합니다."]},{"l":"학습 스크립트 다운로드","p":["다음과 같이 실행하여 GitHub 레포지토리에서 학습을 위한 PyTorch 스크립트를 다운로드합니다. 본 튜토리얼에서는 tutorial 디렉토리 안에 있는 train_llama3.py 스크립트를 사용할 것입니다."]},{"l":"필요 Python 패키지 설치","p":["다음과 같이 실행하여 스크립트 실행에 필요한 서드 파티 Python 패키지들을 미리 설치합니다."]},{"l":"학습 모델 및 토크나이저 다운로드","p":["Hugging Face에 공개된 Llama3 모델 체크포인트를 사용하기 위해서는 커뮤니티 라이센스 동의와 Hugging Face 토큰 정보가 필요합니다.","먼저 다음 사이트에서 필요한 정보를 입력한 후 라이센스 동의를 진행합니다.","meta-llama/Meta-Llama-3-8B · Hugging Face","동의서 제출 후 페이지의 상태가 다음과 같이 변경된 것을 확인합니다.","다음과 같은 명령어를 터미널에 입력하고 안내에 따라 Hugging Face 사용자 토큰을 입력합니다."]}],[{"l":"2. Moreh의 학습 코드 톺아보기","p":["학습 데이터를 모두 준비하셨다면 다음으로는 실제 fine-tuning 과정을 실행할 train_llama3.py 스크립트의 내용에 대해 살펴 보겠습니다. 이 스크립트는 통상적인 PyTorch 코드로서 Hugging Face Transformers 라이브러리에 있는 Llama3 8B 모델 구현을 기반으로 fine tuning 작업을 실행합니다.","우선 제공된 스크립트를 그대로 사용하여 튜토리얼을 끝까지 진행해 보시기를 권장합니다. 이후 스크립트를 원하는 대로 수정하셔서 Llama2 13B 모델을 다른 방식으로 fine-tuning 하는 것도 얼마든지 가능합니다. MoAI Platform은 PyTorch와의 완전한 호환성을 제공하기 때문입니다. 필요하시다면 Moreh에서 제공하는 MoAI Platform 응용 가이드( LLM Fine-tuning 파라미터 가이드)를 참고하십시오."]},{"l":"Training Code","p":["모든 코드는 일반적인 pytorch 사용 경험과 완벽하게 동일합니다.","먼저, transformers 라이브러리에서 필요한 모듈을 불러옵니다.","HuggingFace에 공개된 모델 config와 체크포인트를 불러옵니다.","Hugging Face에 공개된 학습 데이터셋 을 불러와 전처리하고, 데이터 로더를 정의합니다.","이후 학습도 일반적인 Pytorch를 사용하여 모델 학습과 동일하게 진행됩니다.","위와 같이 MoAI Platform에서는 기존 PyTorch 코드와 동일한 방식으로 작성하실 수 있습니다."]},{"l":"About Advanced Parallelism","p":["본 튜토리얼에 사용되는 학습 스크립트에서는 아래와 같은 코드가 추가로 한 줄 존재합니다. 이는 MoAI Platform에서 제공하는 최고의 병렬화 기능을 수행하는 코드입니다.","본 튜토리얼에서 사용하는 Llama3 8B와 같은 거대한 언어 모델의 경우 필연적으로 여러 개의 GPU를 사용하여 학습시켜야만 합니다. 이 경우 MoAI Platform이 아닌 다른 프레임워크를 사용할 경우, Data Parallel, Pipeline Parallel, Tensor Parallel과 같은 병렬화 기법을 도입하여 학습을 수행해야 합니다.","예를 들어, 사용자가 일반적인 pytorch 코드에서 DDP를 적용하고 싶다면, 다음과 같은 코드 스니펫이 추가되어야 합니다.","(Reference: https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","DDP는 비교적 쉽게 적용할 수 있지만, 파이프라인 병렬 처리 나 텐서 병렬 처리 를 적용하려면 상당히 복잡한 코드 수정이 필요합니다. 최적화된 병렬화 처리를 적용하려면 학습 스크립트 작성 과정에서 Python 코드가 다중 처리 환경에서 어떻게 동작하는지 이해해야 하며, 특히 다중 노드 설정에서는 학습에 사용되는 각 노드의 환경을 구성해야 합니다. 또한, 모델 종류, 크기, 데이터셋 등을 고려해 최적의 병렬화 방법을 찾기 위해서는 상당히 많은 시간이 필요합니다.","반면, MoAI Platform의 AP 기능을 통해 사용자는 별도의 병렬화 기법을 적용할 필요 없이, 학습 스크립트에 단 한 줄의 코드를 추가하는 것으로도 최적화된 병렬화 학습을 진행할 수 있습니다.","다른 프레임워크에서는 경험할 수 없는 MoAI Platform만의 Advanced Parallelization(AP) 기능을 통해 최적의 자동화된 분산 병렬처리를 경험해보세요. AP기능을 이용하면 대규모 모델 훈련시 일반적으로 필요한 Pipeline Parallelism, Tensor Parallelism의 최적 매개변수와 환경변수를 아주 간단한 코드 한 줄로 설정할 수 있습니다."]}],[{"l":"3. 학습 실행하기","p":["이제 실제로 fine tuning을 실행해 보겠습니다."]},{"l":"가속기 Flavor 설정","p":["AMD MI210 GPU 32개 사용","AMD MI250 GPU 16개 사용","AMD MI300X GPU 8개 사용","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","MoAI Platform에서는 사용자에게 물리 GPU가 노출되지 않습니다. 대신에, PyTorch에서 사용할 수 있는 가상의 MoAI Accelerator를 MoAI Accelerator가 제공됩니다. 가속기의 Flavor를 설정함으로써 실제 물리 GPU를 얼마나 활용할지를 결정할 수 있습니다. 선택한 가속기 Flavor에 따라 총 학습 시간과 GPU 사용 비용이 달라지므로 사용자는 학습 상황을 고려하여 결정해야 합니다. 사용자의 학습 목표에 맞는 가속기 Flavor를 선택하기 위해 LLM Fine-tuning 파라미터 가이드 를 참고하세요.","moreh-switch-model 툴을 사용하여 현재 시스템에서 사용 가능한 가속기 flavor 리스트를 확인할 수 있습니다. 원활한 모델 학습을 위해 moreh-switch-model 명령어를 이용해 더 큰 메모리의 MoAI Accelerator로 변경할 수 있습니다.","Moreh의 체험판 컨테이너 사용 시: 선택","q 를 입력해 변경을 완료합니다.","따라서 처음 설정되어 있던 flavor를 로 전환한 다음 moreh-smi 명령을 사용하여 정상적으로 반영되었는지 확인하겠습니다.","로 잘 변경된 것을 확인할 수 있습니다.","먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","변경 사항이 잘 반영되었는지 확인하기 위해 다시 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","사용을 위해 8을 입력합니다.","앞서 Llama3 8B Fine-tuning - 시작하기 전에 에서 MoAI Accelerator를 확인했던 것을 기억하시나요? 이제 본격적인 학습 실행을 위해 필요한 가속기를 설정해보겠습니다.","여기서 번호를 입력하여 다른 flavor로 전환할 수 있습니다.","이번 튜토리얼에서는 2048GB 크기의 MoAI Accelerator를 이용하겠습니다.","튜토리얼을 계속 진행하기 위해 인프라 제공자에게 각 flavor에 대응되는 GPU 종류 및 개수를 문의하십시오. 다음 중 하나에 해당하는 flavor를 선택하여 계속 진행하십시오.","현재 사용중인 MoAI Accelerator의 메모리 크기는 512GB입니다."]},{"l":"학습 실행","p":["주어진 train_llama3.py 스크립트를 실행합니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력 될 것입니다. 로그를 통해 최적의 병렬화 설정을 찾는 Advanced Parallelism 기능이 정상 동작하는 것을 확인할 수 있습니다. 앞서 살펴 본 PyTorch 스크립트 상에서는 AP 코드 한 줄을 제외한 다른 부분에서 GPU 여러 개를 동시에 사용하기 위한 처리가 전혀 없었음을 참고하십시오.","훈련 로그를 확인해보면 학습이 정상적으로 이루어지는 것을 확인할 수 있습니다.","학습 도중에 출력되는 throughput은 해당 PyTorch 스크립트를 통해 초당 몇 개의 token을 학습하고 있는지를 의미합니다.","AMD MI250 GPU 16개 사용 시: 약 200,000 tokens/sec","GPU 종류 및 개수에 따른 대략적인 학습 소요 시간은 다음과 같습니다.","AMD MI250 GPU 16개 사용 시: 약 160분"]},{"l":"학습 중에 가속기 상태 확인","p":["학습 도중에 터미널을 하나 더 띄워서 컨테이너에 접속한 후 moreh-smi 명령을 실행하시면 다음과 같이 MoAI Accelerator의 메모리를 점유하며 학습 스크립트가 실행되는 것을 확인하실 수 있습니다. 실행 로그상에서 초기화 과정이 끝나고 Loss가 출력되는 도중에 확인해 보시기 바랍니다."]}],[{"l":"4. 학습 결과 확인하기","p":["앞 장과 같이 train_llama3.py 스크립트를 실행하면 결과 모델이 llama3_summarization 디렉토리에 저장됩니다. 이는 순수한 PyTorch 모델 파라미터 파일로 MoAI Platform이 아닌 일반 GPU 서버에서도 100% 호환됩니다.","미리 다운로드한 GitHub 레포지토리의 tutorial 디렉토리 아래에 있는 inference_llama3.py 스크립트로 학습된 모델을 테스트해 볼 수 있습니다.","코드를 실행합니다.","출력값을 확인해보면 모델이 입력된 프롬프트의 내용을 적절히 요약한 것을 확인할 수 있습니다."]}],[{"l":"5. GPU 개수 변경하기","p":["앞과 동일한 fine-tuning 작업을 GPU 개수를 바꾸어 다시 실행해 보겠습니다. MoAI Platform은 GPU 자원을 단일 가속기로 추상화하여 제공하며 자동으로 병렬 처리를 수행합니다. 따라서 GPU 개수를 변경하더라도 PyTorch 스크립트를 수정할 필요가 전혀 없습니다."]},{"l":"가속기 Flavor 변경","p":["moreh-switch-model 툴을 사용하여 가속기 flavor를 전환합니다. 가속기 변경 방법은 3. 학습 실행하기 문서를 한번 더 참고해주시기 바랍니다.","인프라 제공자에게 문의하여 다음 중 하나를 선택한 다음 계속 진행하십시오.","AMD MI250 GPU 32개 사용","Moreh의 체험판 컨테이너 사용 시: 선택","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","AMD MI210 GPU 64개 사용","AMD MI300X GPU 16개 사용"]},{"l":"학습 실행","p":["다시 train_llama3.py 스크립트를 실행합니다.","사용 가능한 GPU 메모리가 2배 늘었기 때문에, 배치 사이즈 또한 기존 256 에서 1024 로 변경하여 실행시켜 보겠습니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력될 것입니다.","앞서 GPU 개수가 절반이었을 때 실행한 결과와 비교해 동일하게 학습이 이루어지며 throughput이 향상되었음을 확인할 수 있습니다.","AMD MI250 GPU 16 → 32개 사용 시: 약 200,000 tokens/sec → 390,000 tokens/sec"]}],[{"l":"6. 마무리","p":["지금까지 MoAI Platform에서 Llama3 8B 를 fine-tuning하는 과정을 살펴 보았습니다. Llama 와 같은 오픈 소스 LLM은 요약, 질의 응답 등 다양한 태스크에 활용할 수 있습니다. MoAI 플랫폼을 사용한다면 여러분이 필요한 GPU 수를 코드 변경 없이 손쉽게 설정할 수 있습니다. 여러분만의 데이터로 새로운 모델을 빠르고 쉽게 개발해 보세요.","LLaMA 3와 같은 대형 언어 모델, 미세 조정 기술, MoAI 플랫폼의 가용성 덕분에 누구나 강력한 AI 애플리케이션을 개발할 수 있게 되었습니다. 따라서 이 튜토리얼에서 수행해 본 과정을 바탕으로 여러분만의 데이터로 새로운 모델을 빠르고 쉽게 개발해 보세요.","이 튜토리얼이나 MoAI Platform에 관련하여 궁금한 점이나 문의 사항이 있는 경우 Moreh( support@moreh.io)에 문의하시기 바랍니다."]},{"l":"더 알아보기","p":["MoAI Platform의 자동병렬화 기능, Advanced Parallelization (AP)","Llama3 70B","GPT Fine-tuning","Mistral Fine-tuning","Baichuan2 Fine-tuning","Qwen Fine-tuning"]}],[{"l":"Llama3 70B Fine-tuning","p":["이 튜토리얼은 MoAI Platform에서 오픈 소스 LLama3-70B 모델을 fine-tuning하는 예시를 소개합니다. 이 튜토리얼을 통해 MoAI Platform으로 AMD GPU 클러스터를 사용하는 방법을 익히고 향상된 성능과 자동 병렬화의 이점을 확인할 수 있습니다."]},{"l":"개요","p":["MoAI Platform은 GPU를 손쉽게 제어할 수 있는 확장 가능한 AI 플랫폼으로, 수천 대의 GPU를 쉽게 제어하여 AI 모델을 학습하거나 추론할 수 있습니다. 모델을 파인튜닝하는 데 있어서 MoAI Platform의 특징은 가상화와 병렬화를 통해 고객에게 매우 간단한 학습 방법을 제안한다는 점입니다.","MoAI Platform은 여러 개의 GPU를 가상화하여 하나의 GPU인 MoAI Accelerator로 고객에게 제공합니다. 이는 학습 시 하나의 GPU만 사용하는 것처럼 보이게 하며, 다중 GPU를 사용하기 위해 필요한 사전 준비나 코드 작업이 필요하지 않습니다.","MoAI Platform은 고객이 가상화된 MoAI Accelerator를 사용할 때, 그 내부에서 자동으로 최적화된 병렬화를 제공합니다. 모델 크기와 데이터 크기에 따라 다양한 병렬화 방법을 고려하여 최적의 병렬화를 제공함으로써, 사용자가 추가적인 작업 없이도 간단한 코드로 매우 고성능의 학습을 경험할 수 있습니다."]}],[{"i":"1-moai-platform-병렬화---llama3-70b","l":"1. MoAI Platform 병렬화 - Llama3 70B","p":["Llama3 70B 모델 전체를 파인튜닝하기 위해서는 필수적으로 다중 GPU를 사용해야 하며, 텐서 병렬화(Tensor Parallelism), 파이프라인 병렬화(Pipeline Parallelism), 데이터 병렬화(Data Parallelism) 등의 병렬화를 수행해야 합니다. 이를 위해서는 Deepspeed와 같은 도구를 사용하여 복잡한 config 설정을 해야 하며, 파이프라인 병렬화의 경우 모델을 잘 이해하고 활용할 줄 알아야 합니다. 또한, 가장 효과적인 병렬화 방식을 찾기 위해서는 여러 번 코드를 수정하고 모델을 학습시켜보며 최적의 조합을 찾아야 합니다. 따라서 다중 GPU를 사용하기 위해서는 사용자에게 많은 노력이 필요합니다.","그러나 MoAI Platform에서는 복잡한 코드 수정이나 모델에 대한 깊은 이해 없이도 자동 병렬화와 최적의 조합을 제공하여 사용자가 간편하게 모델을 학습할 수 있도록 지원 합니다."]},{"l":"Fine-tuning Code","p":["MoAI Platform의 모든 코드는 일반적인 PyTorch 사용 경험과 동일합니다. Llama3 70B를 학습하기 위해 기존 PyTorch에서 하나의 GPU를 사용하는 것처럼 스크립트를 작성할 수 있습니다.","기존 PyTorch와 다르게 MoAI Platform에서는 다음과 같은 코드 한 줄이 추가됩니다. 이 코드는 MoAI Platform의 자동 최적화 및 병렬화를 설정하여 사용자가 간단하게 Llama3 70B를 fine-tuning할 수 있게 합니다.","이제 MoAI Platform에서 MI250 GPU 64개를 사용한 Llama3 70B 모델 fine-tuning 튜토리얼을 시작하겠습니다. 이 튜토리얼을 통해 MoAI Platform에서 다중 GPU를 사용하는 방법이 얼마나 쉽고 효과적인지 확인할 수 있을 것입니다."]}],[{"l":"2. Fine-tuning 준비하기"},{"l":"시작하기 전에","p":["MoAI Platform 상의 컨테이너 혹은 가상 머신을 인프라 제공자로부터 발급받고, 여기에 SSH로 접속하는 방법을 안내 받으시기 바랍니다. 예를 들어 MoAI Platform의 체험판 컨테이너 또는 MoAI Platform 기반으로 운영되는 퍼블릭 클라우드 서비스를 신청하여 사용할 수 있습니다.","MoAI Platform 체험판 컨테이너 사용 문의: support@moreh.io","KT Cloud Hyperscale AI Computing ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","SSH로 접속한 다음 moreh-smi 명령을 실행하여 MoAI Accelerator가 잘 표시되는지 확인하시기 바랍니다. 디바이스 이름은 시스템마다 다르게 설정되어 있을 수 있습니다."]},{"l":"MoAI Accelerator 확인","p":["이 튜토리얼에서 안내할 Llama3 모델과 같은 sLLM을 학습하기 위해서는 적절한 크기의 MoAI Accelerator를 선택해야 합니다. 먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","수행할 학습에 필요한 구체적인 MoAI Accelerator 설정에 대한 설명은 “3. 학습 실행하기”에서 제공하겠습니다.","MoAI Platform에서 PyTorch 스크립트 실행 환경을 준비하는 것은 일반적인 GPU 서버에서와 크게 다르지 않습니다."]},{"l":"PyTorch 설치 여부 확인하기","p":["SSH로 컨테이너에 접속한 다음 아래와 같이 실행하여 현재 conda 환경에 PyTorch가 설치되어 있는지 확인합니다.","버전명에는 PyTorch 버전과 이를 실행시키기 위한 MoAI 버전이 함께 표시되어 있습니다. 위 예시의 경우 PyTorch 1.13.1+cu116 버전을 실행하는 MoAI의 24.5.0 버전이 설치되어 있음을 의미합니다.","만약 conda: command not found 메시지가 표시되거나, torch 패키지가 리스트되지 않거나, 혹은 torch 패키지가 존재하더라도 버전명에 “moreh”가 포함되지 않은 경우 MoAI Platform에서 Fine-tuning 준비하기 문서에 따라 conda 환경을 생성하십시오."]},{"l":"PyTorch 동작 여부 확인하기","p":["다음과 같이 실행하여 torch 패키지가 정상적으로 import되고 MoAI Accelerator가 인식되는지 확인합니다."]},{"l":"학습 스크립트 다운로드","p":["다음과 같이 실행하여 GitHub 레포지토리에서 학습을 위한 PyTorch 스크립트를 다운로드합니다. 본 튜토리얼에서는 tutorial 디렉토리 안에 있는 train_llama3.py 스크립트를 사용할 것입니다."]},{"l":"필요 Python 패키지 설치","p":["다음과 같이 실행하여 스크립트 실행에 필요한 서드 파티 Python 패키지들을 미리 설치합니다."]},{"l":"학습 모델 및 토크나이저 다운로드","p":["Hugging Face에 공개된 Llama3 70B 모델 체크포인트를 사용하기 위해서는 커뮤니티 라이센스 동의와 Hugging Face 토큰 정보가 필요합니다.","먼저 다음 사이트에서 필요한 정보를 입력한 후 라이센스 동의를 진행합니다.","동의서 제출 후 페이지의 상태가 다음과 같이 변경된 것을 확인합니다.","meta-llama/Meta-Llama-3-70B · Hugging Face","다음과 같은 명령어를 터미널에 입력하고 안내에 따라 Hugging Face 사용자 토큰을 입력합니다."]}],[{"l":"3. 학습 실행하기","p":["이제 실제로 fine tuning을 실행해 보겠습니다."]},{"l":"가속기 Flavor 설정","p":["AMD MI210 GPU 64개 사용","AMD MI250 GPU 32개 사용","AMD MI300X GPU 16개 사용","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","MoAI Platform에서는 사용자에게 물리 GPU가 노출되지 않습니다. 대신에, PyTorch에서 사용할 수 있는 가상의 MoAI Accelerator를 MoAI Accelerator가 제공됩니다. 가속기의 Flavor를 설정함으로써 실제 물리 GPU를 얼마나 활용할지를 결정할 수 있습니다. 선택한 가속기 Flavor에 따라 총 학습 시간과 GPU 사용 비용이 달라지므로 사용자는 학습 상황을 고려하여 결정해야 합니다. 사용자의 학습 목표에 맞는 가속기 Flavor를 선택하기 위해 LLM Fine-tuning 파라미터 가이드 를 참고하세요.","moreh-switch-model 툴을 사용하여 현재 시스템에서 사용 가능한 가속기 flavor 리스트를 확인할 수 있습니다. 원활한 모델 학습을 위해 moreh-switch-model 명령어를 이용해 더 큰 메모리의 MoAI Accelerator로 변경할 수 있습니다.","Moreh의 체험판 컨테이너 사용 시: 선택","q 를 입력해 변경을 완료합니다.","따라서 처음 설정되어 있던 flavor를 로 전환한 다음 moreh-smi 명령을 사용하여 정상적으로 반영되었는지 확인하겠습니다.","로 잘 변경된 것을 확인할 수 있습니다.","먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","변경 사항이 잘 반영되었는지 확인하기 위해 다시 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","사용을 위해 10 을 입력합니다.","앞서 Llama3 70B Fine-tuning - 시작하기 전에 에서 MoAI Accelerator를 확인했던 것을 기억하시나요? 이제 본격적인 학습 실행을 위해 필요한 가속기를 설정해보겠습니다.","여기서 번호를 입력하여 다른 flavor로 전환할 수 있습니다.","이번 튜토리얼에서는 4096GB 크기의 MoAI Accelerator를 이용하겠습니다.","튜토리얼을 계속 진행하기 위해 인프라 제공자에게 각 flavor에 대응되는 GPU 종류 및 개수를 문의하십시오. 다음 중 하나에 해당하는 flavor를 선택하여 계속 진행하십시오.","현재 사용중인 MoAI Accelerator의 메모리 크기는 512GB입니다."]},{"l":"학습 실행","p":["주어진 train_llama3_70b.py 스크립트를 실행합니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력 될 것입니다. 로그를 통해 최적의 병렬화 설정을 찾는 Advanced Parallelism 기능이 정상 동작하는 것을 확인할 수 있습니다. 앞서 살펴 본 PyTorch 스크립트 상에서는 AP 코드 한 줄을 제외한 다른 부분에서 GPU 여러 개를 동시에 사용하기 위한 처리가 전혀 없었음을 참고하십시오.","훈련 로그를 확인해보면 학습이 정상적으로 이루어지는 것을 확인할 수 있습니다.","학습 도중에 출력되는 throughput은 해당 PyTorch 스크립트를 통해 초당 몇 개의 token을 학습하고 있는지를 의미합니다.","AMD MI250 GPU 32개(device 64개) 사용 시: 약 4062 tokens/sec","GPU 종류 및 개수에 따른 대략적인 학습 소요 시간은 다음과 같습니다.","AMD MI250 32개(device 64개) 사용 시 : 약 24 시간"]},{"l":"학습 중에 가속기 상태 확인","p":["학습 도중에 터미널을 하나 더 띄워서 컨테이너에 접속한 후 moreh-smi 명령을 실행하시면 다음과 같이 MoAI Accelerator의 메모리를 점유하며 학습 스크립트가 실행되는 것을 확인하실 수 있습니다. 실행 로그상에서 초기화 과정이 끝나고 Loss가 출력되는 도중에 확인해 보시기 바랍니다."]}],[{"l":"4. 마무리","p":["지금까지 MoAI Platform에서 Llama3-70b 모델을 fine-tuning하는 과정을 살펴 보았습니다. MoAI 플랫폼을 사용한다면 여러분이 1개, 4개, 100개 등 모든 개수의 GPU를 코드 변경 없이 손쉽게 설정할 수 있습니다. 여러분만의 데이터로 새로운 모델을 빠르고 쉽게 개발해 보세요.","이 튜토리얼이나 MoAI Platform에 관련하여 궁금한 점이나 문의 사항이 있는 경우 Moreh( support@moreh.io)에 문의하시기 바랍니다."]},{"l":"더 알아보기","p":["MoAI Platform의 자동병렬화 기능, Advanced Parallelization (AP)","GPT Fine-tuning","Llama3 8B Fine-tuning","Mistral Fine-tuning","Baichuan2 Fine-tuning","Qwen Fine-tuning"]}],[{"l":"Mistral Fine-tuning","p":["이 튜토리얼은 MoAI Platform에서 오픈 소스 Mistral 7B 모델을 fine-tuning하는 예시를 소개합니다. 튜토리얼을 통해 아래와 같은 MoAI Platform이 제공하는 여러 기능을 체험하며, AMD GPU 클러스터를 사용하는 방법을 익힐 수 있습니다.","사용자는 수십 개의 GPU를 MoAI Accelerator라는 하나의 가속기처럼 사용할 수 있어 복잡한 병렬화 작업이나 클러스터 환경 설정 없이도 쉽게 학습을 실행할 수 있습니다. 사용자는 리소스 관리에 신경쓰지 않고 학습에만 집중할 수 있습니다.","자동 병렬화 기능 덕분에 코드 작성과 개발이 간소화되며, 모델 학습 속도가 크게 향상됩니다. 이는 효율적인 자원 활용을 가능하게 하여 사용자가 더 빠르고 효과적으로 작업할 수 있도록 돕습니다."]},{"l":"개요","p":["MoAI Platform은 수천 대의 GPU를 쉽게 제어하여 AI 모델을 학습하거나 추론할 수 있는 확장 가능한 AI 플랫폼입니다. MoAI Platform의 특징은 모델을 fine-tuning할 때 가상화와 병렬화를 통해 매우 간단한 학습 방법을 제공한다는 점입니다.","MoAI Platform은 여러개의 GPU를 가상화하여 하나의 가속기인 MoAI Accelerator 로 제공합니다. 따라서 다중 GPU 사용을 위해 필요한 사전 준비나 코드 수정이 필요하지 않습니다.","MoAI Platform은 고객이 가상화된 MoAI Accelerator를 사용할 때 내부적으로 자동 최적화된 병렬화를 제공합니다. 모델 크기, 데이터 크기에 대해서 다양한 병렬화 방법을 고려해 최적의 병렬화 환경을 제공하며, 사용자는 별도의 작업이 없이 간단한 코드로 고성능 학습을 경험할 수 있습니다."]},{"l":"시작하기 전에","p":["MoAI Platform 상의 컨테이너 혹은 가상 머신을 인프라 제공자로부터 발급받고, 여기에 SSH로 접속하는 방법을 안내 받으시기 바랍니다. 예를 들어 MoAI Platform의 체험판 컨테이너 또는 MoAI Platform 기반으로 운영되는 퍼블릭 클라우드 서비스를 신청하여 사용할 수 있습니다.","MoAI Platform 체험판 컨테이너 사용 문의: support@moreh.io","KT Cloud's Hyperscale AI Computing ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","SSH로 접속한 다음 moreh-smi 명령을 실행하여 MoAI Accelerator가 잘 표시되는지 확인하시기 바랍니다. 디바이스 이름은 시스템마다 다르게 설정되어 있을 수 있습니다."]},{"l":"MoAI Accelerator 확인","p":["이 튜토리얼에서 안내할 Mistral 7B 모델과 같은 sLLM을 학습하기 위해서는 적절한 크기의 MoAI Accelerator를 선택해야 합니다. 먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","수행할 학습에 필요한 구체적인 MoAI Accelerator 설정에 대한 설명은 3. 학습 실행하기 에서 제공하겠습니다."]}],[{"l":"1. Fine-tuning 준비하기","p":["MoAI Platform에서 PyTorch 스크립트 실행 환경을 준비하는 것은 일반적인 GPU 서버에서와 크게 다르지 않습니다. 단, 튜토리얼 진행을 위해 아래의 사양들이 권장됩니다.","CPU: 16 core 이상","memory: 256GB 이상","MAF 버전: 24.5.0","스토리지: 60GB 이상","원할한 튜토리얼 진행 전 실행 환경을 확인하시길 바랍니다."]},{"l":"PyTorch 설치 여부 확인하기","p":["SSH로 컨테이너에 접속한 다음 아래와 같이 실행하여 현재 conda 환경에 PyTorch가 설치되어 있는지 확인합니다.","버전명에는 PyTorch 버전과 이를 실행시키기 위한 MoAI 버전이 함께 표시되어 있습니다. 위 예시의 경우 PyTorch 1.13.1+cu116 버전을 실행하는 MoAI의 24.5.0 버전이 설치되어 있음을 의미합니다.","만약 conda: command not found 메시지가 표시되거나, torch 패키지가 리스트되지 않거나, 혹은 torch 패키지가 존재하더라도 버전명에 “moreh”가 포함되지 않은 경우 MoAI Platform에서 Fine-tuning 준비하기 문서에 따라 conda 환경을 생성하십시오.","만약 해당 MoAI 버전이 24.5.0이 아닌 다른 버전이라면 아래의 코드를 실행시키십시오."]},{"l":"PyTorch 동작 여부 확인하기","p":["다음과 같이 실행하여 torch 패키지가 정상적으로 import되고 MoAI Accelerator가 인식되는지 확인합니다."]},{"l":"학습 스크립트 다운로드","p":["다음과 같이 실행하여 GitHub 레포지토리에서 학습을 위한 PyTorch 스크립트를 다운로드합니다. 본 튜토리얼에서는 tutorial 디렉토리 안에 있는 train_mistral.py 스크립트를 사용할 것입니다."]},{"l":"필요 Python 패키지 설치","p":["다음과 같이 실행하여 스크립트 실행에 필요한 서드 파티 Python 패키지들을 미리 설치합니다."]},{"l":"학습 모델 및 토크나이저 다운로드","p":["Hugging Face를 이용해 Mistral 7B v0.1 모델 체크포인트를 사용하기 위해서는 커뮤니티 라이센스 동의와 Hugging Face 토큰 정보가 필요합니다.","먼저 다음 사이트에서 필요한 정보를 입력한 후 라이센스 동의를 진행합니다.","mistralai/Mistral-7B-v0.1 · Hugging Face","동의서 제출 후 페이지의 상태가 다음과 같이 변경된 것을 확인합니다.","상태 변경이 되었다면, 다음과 같은 명령어를 터미널에 입력하고 안내에 따라 Hugging Face 사용자 토큰을 입력합니다."]}],[{"l":"2. Moreh의 학습 코드 톺아보기","p":["학습 데이터를 모두 준비하셨다면 다음으로는 실제 fine-tuning 과정을 실행할 train_mistral.py 스크립트의 내용을 살펴 보겠습니다. 이번 단계에서는 MoAI Platform은 pytorch와의 완전한 호환성으로 학습 코드가 일반적인 nvidia gpu를 위한 pytorch 코드와 100% 동일하다는 것을 확인하실 수 있습니다. 또한 이를 넘어서 기존의 복잡한 병렬화 기법들을 MoAI Platform에서는 얼마나 효율적으로 구현할 수 있는지도 확인하실 수 있습니다.","우선 제공된 스크립트를 그대로 사용하여 튜토리얼을 끝까지 진행해 보시기를 권장합니다. 이후 스크립트를 원하는 대로 수정하셔서 Mistral 7B 모델, 혹은 다른 공개된 모델을 다른 방식으로 fine-tuning하는 것도 얼마든지 가능합니다. 필요하시다면 Moreh에서 제공하는 MoAI Platform 응용 가이드( LLM Fine-tuning 파라미터 가이드)를 참고하십시오."]},{"l":"Training Code","p":["모든 코드는 일반적인 pytorch 사용 경험과 완벽하게 동일합니다.","먼저, transformers 라이브러리에서 필요한 모듈을 불러옵니다.","HuggingFace에 공개된 모델 config와 체크포인트를 불러옵니다.","Hugging Face에 공개된 학습 데이터셋을 불러와 전처리하고, 데이터 로더를 정의합니다. 이 튜토리얼에서는 코드 생성 훈련을 위해 공개된 여러 데이터셋들 중 Hugging Face에 공개되어 있는 python_code_instructions_18k_alpaca 데이터셋을 사용할 것입니다.","이후 학습도 일반적인 Pytorch를 사용하여 모델 학습과 동일하게 진행됩니다.","위와 같이 MoAI Platform에서는 기존 pytorch 코드와 동일한 방식으로 작성하실 수 있습니다."]},{"l":"About Advanced Parallelism","p":["본 튜토리얼에 사용되는 학습 스크립트에서는 아래와 같은 코드가 추가로 한 줄 존재합니다. 이는 MoAI Platform에서 제공하는 최고의 병렬화 기능을 수행하는 코드입니다.","본 튜토리얼에서 사용하는 Mistral 7B 와 같은 거대한 언어 모델의 경우 필연적으로 여러 개의 GPU를 사용하여 학습시켜야만 합니다. 이때, MoAI Platform이 아닌 다른 프레임워크를 사용할 경우, Data Parallel, Pipeline Parallel , Tensor Parallel과 같은 병렬화 기법을 도입하여 학습을 수행해야 합니다.","예를 들어, 사용자가 일반적인 pytorch 코드에서 DDP를 적용하고 싶다면, 다음과 같은 코드 스니펫이 추가되어야 합니다. ( https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","DDP는 비교적 쉽게 적용할 수 있지만, 파이프라인 병렬 처리 나 텐서 병렬 처리 를 적용하려면 상당히 복잡한 코드 수정이 필요합니다. 최적화된 병렬화 처리를 적용하려면 학습 스크립트 작성 과정에서 Python 코드가 다중 처리 환경에서 어떻게 동작하는지 이해해야 하며, 특히 다중 노드 설정에서는 학습에 사용되는 각 노드의 환경을 구성해야 합니다. 또한, 모델 종류, 크기, 데이터셋 등을 고려해 최적의 병렬화 방법을 찾기 위해서는 상당히 많은 시간이 필요합니다.","반면, MoAI Platform의 AP 기능을 통해 사용자는 별도의 병렬화 기법을 적용할 필요 없이, 학습 스크립트에 단 한 줄의 코드를 추가하는 것으로도 최적화된 병렬화 학습을 진행할 수 있습니다.","다른 프레임워크에서는 경험할 수 없는 MoAI Platform만의 Advanced Parallelization(AP) 기능을 통해 최적의 자동화된 분산 병렬처리 를 경험해보세요. AP기능을 이용하면 대규모 모델 훈련시 일반적으로 필요한 Pipeline Parallelism, Tensor Parallelism의 최적 매개변수와 환경변수를 아주 간단한 코드 한 줄로 설정할 수 있습니다."]}],[{"l":"3. 학습 실행하기","p":["이제 실제로 fine tuning을 실행해 보겠습니다."]},{"l":"가속기 Flavor 설정","p":["AMD MI210 GPU 32개 사용","AMD MI250 GPU 16개 사용","AMD MI300X GPU 8개 사용","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","MoAI Platform에서는 사용자에게 물리 GPU가 노출되지 않습니다. 대신 PyTorch에서 사용 가능한 가상의 MoAI Accelerator가 제공됩니다. 가속기의 flavor를 설정함으로써 실제로 PyTorch에서 물리 GPU를 얼마나 활용할지를 결정할 수 있습니다. 선택한 가속기 Flavor에 따라 학습 총 시간 및 gpu 사용 비용이 달라지므로 사용자의 학습 상황에 따른 판단이 필요합니다. 사용자의 학습 목표에 맞는 가속기 Flavor를 선택하기 위해 LLM Fine-tuning 파라미터 가이드 를 참고하세요.","moreh-switch-model 툴을 사용하여 현재 시스템에서 사용 가능한 가속기 flavor 리스트를 확인할 수 있습니다. 원활한 모델 학습을 위해 moreh-switch-model 명령어를 이용해 더 큰 메모리의 MoAI Accelerator로 변경할 수 있습니다.","Moreh의 체험판 컨테이너 사용 시: 선택","q 를 입력해 변경을 완료합니다.","따라서 처음 설정되어 있던 flavor를 로 전환한 다음 moreh-smi 명령을 사용하여 정상적으로 반영되었는지 확인하겠습니다.","로 잘 변경된 것을 확인할 수 있습니다.","먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","변경 사항이 잘 반영되었는지 확인하기 위해 다시 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","사용을 위해 8을 입력합니다.","앞서 Mistral Fine-tuning - 시작하기 전에 에서 MoAI Accelerator를 확인했던 것을 기억하시나요? 이제 본격적인 학습 실행을 위해 필요한 가속기를 설정해보겠습니다.","여기서 번호를 입력하여 다른 flavor로 전환할 수 있습니다.","이번 튜토리얼에서는 2048GB 크기의 MoAI Accelerator를 이용하겠습니다.","튜토리얼을 계속 진행하기 위해 인프라 제공자에게 각 flavor에 대응되는 GPU 종류 및 개수를 문의하십시오. 다음 중 하나에 해당하는 flavor를 선택하여 계속 진행하십시오.","현재 사용중인 MoAI Accelerator의 메모리 크기는 64GB입니다."]},{"l":"학습 실행","p":["주어진 train_mistral.py 스크립트를 실행합니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력 될 것입니다. 중간에 파란색으로 표시된 부분을 보시면 Advanced Parallelism 기능이 정상 동작하는 것을 확인할 수 있습니다. 앞서 살펴 본 PyTorch 스크립트 상에서는 GPU 여러 개를 동시에 사용하기 위한 처리가 전혀 없었음을 참고하십시오.","Loss 값이 다음과 같이 나타나며 정상 학습이 이루어지는 것을 확인할 수 있습니다.","학습 도중에 출력되는 throughput은 해당 PyTorch 스크립트를 통해 초당 몇 개의 token을 학습하고 있는지를 의미합니다.","AMD MI250 GPU 16개 사용 시: 약 390,000 tokens/sec","GPU 종류 및 개수에 따른 대략적인 학습 소요 시간은 다음과 같습니다.","AMD MI250 GPU 16개 사용 시: 약 15분"]},{"l":"학습 중에 가속기 상태 확인","p":["학습 중에 터미널을 하나 더 열어서 컨테이너에 접속한 후에 moreh-smi 명령을 실행하면 MoAI Accelerator의 메모리를 차지하며 학습 스크립트가 실행되고 있는 것을 확인할 수 있습니다. 실행 로그를 보면 초기화가 완료되고 Loss가 출력되는 도중에 확인해 보시기 바랍니다."]}],[{"l":"4. 학습 결과 확인하기","p":["앞 장과 같이 train_mistral.py 스크립트를 실행하면 결과 모델이 mistral_code_generation 디렉터리에 저장됩니다. 이는 순수한 PyTorch 모델 파라미터 파일로 MoAI Platform이 아닌 일반 GPU 서버에서도 100% 호환됩니다.","미리 다운로드한 GitHub 레포지토리의 tutorial 디렉토리 아래에 있는 inference_mistral.py 스크립트로 학습된 모델을 테스트해 볼 수 있습니다. 테스트에는 ‘주어진 문자열 리스트를 입력 받아 공백으로 결합하는 함수를 만들어’라는 프롬프트가 사용되었습니다.","코드를 실행합니다.","출력값을 확인해보면 모델이 프롬프트 내용대로 적절한 함수를 생성한 것을 확인할 수 있습니다."]}],[{"l":"5. GPU 개수 변경하기","p":["앞과 동일한 fine-tuning 작업을 GPU 개수를 바꾸어 다시 실행해 보겠습니다. GPU 개수를 변경하여 다시 실행해 보겠습니다. MoAI Platform은 GPU 자원을 단일 가속기로 추상화하여 제공하므로 자동으로 병렬 처리를 수행합니다. 따라서 GPU 개수를 변경하더라도 PyTorch 스크립트를 전혀 수정할 필요가 없습니다."]},{"l":"가속기 Flavor 변경","p":["moreh-switch-model 을 사용하여 가속기 flavor를 전환합니다. 가속기 변경 방법은 3. 학습 실행하기 문서를 한번 더 참고해주시기 바랍니다.","인프라 제공자에게 문의하여 다음 중 하나를 선택한 다음 계속 진행하십시오.","AMD MI250 GPU 32개 사용","Moreh의 체험판 컨테이너 사용 시: 선택","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","AMD MI210 GPU 64개 사용","AMD MI300X GPU 16개 사용"]},{"l":"학습 실행","p":["배치 사이즈 변경 없이 train_mistral.py 스크립트를 실행합니다.","사용 가능한 GPU 메모리가 2배 늘었기 때문에, 배치 사이즈 또한 기존 256 에서 512로 변경하여 실행시켜 보겠습니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력될 것입니다.","앞서 GPU 개수가 절반이었을 때 실행한 결과와 비교해 동일하게 학습이 이루어지며 throughput이 향상되었음을 확인할 수 있습니다.","AMD MI250 GPU 16 → 32개 사용 시: 약 390,000 tokens/sec → 800,000 tokens/sec"]}],[{"l":"6. 마무리","p":["지금까지 MoAI Platform에서 Mistral 7B 모델을 fine-tuning하는 과정을 살펴 보았습니다. MoAI Platform을 사용하면 기존의 학습 코드를 그대로 사용하면서 PyTorch 기반 오픈 소스 LLM 모델을 쉽게 GPU 클러스터에서 fine-tuning할 수 있습니다. 또한, MoAI 플랫폼을 사용한다면 여러분이 필요한 GPU 수를 코드 변경 없이 손쉽게 설정할 수 있습니다. 여러분만의 데이터로 새로운 모델을 빠르고 쉽게 개발해 보세요.","이 튜토리얼이나 MoAI Platform에 관련하여 궁금한 점이나 문의 사항이 있는 경우 Moreh( support@moreh.io)에 문의하시기 바랍니다."]},{"l":"더 알아보기","p":["MoAI Platform의 자동병렬화 기능, Advanced Parallelization (AP)","Llama3 8B Fine-tuning","Llama3 70B Fine-tuning","GPT Fine-tuning","Baichuan2 Fine-tuning","Qwen Fine-tuning"]}],[{"l":"GPT Fine-tuning","p":["이 튜토리얼은 MoAI Platform에서 Hugging Face 에 오픈소스로 공개된 GPT 기반의 모델을 fine-tuning하는 예시를 소개합니다. 튜토리얼을 통해 아래와 같은 MoAI Platform이 제공하는 여러 기능을 체험하며, AMD GPU 클러스터를 사용하는 방법을 익힐 수 있습니다.","사용자는 수십 개의 GPU를 MoAI Accelerator라는 하나의 가속기처럼 사용할 수 있어 복잡한 병렬화 작업이나 클러스터 환경 설정 없이도 쉽게 학습을 실행할 수 있습니다. 사용자는 리소스 관리에 신경쓰지 않고 학습에만 집중할 수 있습니다.","자동 병렬화 기능 덕분에 코드 작성과 개발이 간소화되며, 모델 학습 속도가 크게 향상됩니다. 이는 효율적인 자원 활용을 가능하게 하여 사용자가 더 빠르고 효과적으로 작업할 수 있도록 돕습니다."]},{"l":"개요","p":["MoAI Platform은 수천 대의 GPU를 쉽게 제어하여 AI 모델을 학습하거나 추론할 수 있는 확장 가능한 AI 플랫폼입니다. MoAI Platform의 특징은 모델을 fine-tuning할 때 가상화와 병렬화를 통해 매우 간단한 학습 방법을 제공한다는 점입니다.","MoAI Platform은 여러개의 GPU를 가상화하여 하나의 가속기인 MoAI Accelerator 로 제공합니다. 따라서 다중 GPU 사용을 위해 필요한 사전 준비나 코드 수정이 필요하지 않습니다.","MoAI Platform은 고객이 가상화된 MoAI Accelerator를 사용할 때 내부적으로 자동 최적화된 병렬화를 제공합니다. 모델 크기, 데이터 크기에 대해서 다양한 병렬화 방법을 고려해 최적의 병렬화 환경을 제공하며, 사용자는 별도의 작업이 없이 간단한 코드로 고성능 학습을 경험할 수 있습니다."]},{"l":"시작하기 전에","p":["MoAI Platform 상의 컨테이너 혹은 가상 머신을 인프라 제공자로부터 발급받고, 여기에 SSH로 접속하는 방법을 안내 받으시기 바랍니다. 예를 들어 MoAI Platform의 체험판 컨테이너 또는 MoAI Platform 기반으로 운영되는 퍼블릭 클라우드 서비스를 신청하여 사용할 수 있습니다.","MoAI Platform 체험판 컨테이너 사용 문의: support@moreh.io","KT Cloud Hyperscale AI Computing","SSH로 접속한 다음 moreh-smi 명령을 실행하여 MoAI Accelerator가 잘 표시되는지 확인하시기 바랍니다. 디바이스 이름은 시스템마다 다르게 설정되어 있을 수 있습니다."]},{"l":"MoAI Accelerator 확인","p":["이 튜토리얼에서 안내할 GPT 모델과 같은 sLLM을 학습하기 위해서는 적절한 크기의 MoAI Accelerator를 선택해야 합니다. 먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","수행할 학습에 필요한 구체적인 MoAI Accelerator 설정에 대한 설명은 3. 학습 실행하기 에서 제공하겠습니다."]}],[{"l":"1. Fine-tuning 준비하기","p":["MoAI Platform에서 PyTorch 스크립트 실행 환경을 준비하는 것은 일반적인 GPU 서버에서와 크게 다르지 않습니다. 단, 튜토리얼 진행을 위해 아래의 사양들이 권장됩니다:","CPU: 16 core 이상","memory: 256GB 이상","MAF 버전: 24.5.0","스토리지: 100GB 이상","원할한 튜토리얼 진행 전 실행 환경을 확인하시길 바랍니다."]},{"l":"PyTorch 설치 여부 확인하기","p":["SSH로 컨테이너에 접속한 다음 아래와 같이 실행하여 현재 conda 환경에 PyTorch가 설치되어 있는지 확인합니다.","버전명에는 PyTorch 버전과 이를 실행시키기 위한 MoAI 버전이 함께 표시되어 있습니다. 위 예시의 경우 PyTorch 1.13.1+cu116 버전을 실행하는 MoAI의 24.5.0 버전이 설치되어 있음을 의미합니다.","만약 conda: command not found 메시지가 표시되거나, torch 패키지가 리스트되지 않거나, 혹은 torch 패키지가 존재하더라도 버전명에 “moreh”가 포함되지 않은 경우 MoAI Platform에서 Fine-tuning 준비하기 문서에 따라 conda 환경을 생성하십시오.","만약 해당 MoAI 버전이 24.5.0이 아닌 다른 버전이라면 아래의 코드를 실행시키십시오."]},{"l":"PyTorch 동작 여부 확인하기","p":["다음과 같이 실행하여 torch 패키지가 정상적으로 import되고 MoAI Accelerator가 인식되는지 확인합니다."]},{"l":"학습 스크립트 다운로드","p":["다음과 같이 실행하여 GitHub 레포지토리에서 학습을 위한 PyTorch 스크립트를 다운로드합니다. 본 튜토리얼에서는 tutorial 디렉토리 안에 있는 train_gpt.py 스크립트를 사용할 것입니다."]},{"l":"필요 Python 패키지 설치","p":["다음과 같이 실행하여 스크립트 실행에 필요한 서드 파티 Python 패키지들을 미리 설치합니다."]}],[{"l":"2. Moreh의 학습 코드 톺아보기","p":["학습 데이터를 모두 준비하셨다면 다음으로는 실제 fine-tuning 과정을 실행할 train_gpt.py 스크립트의 내용을 살펴 보겠습니다. 이번 단계에서는 MoAI Platform은 pytorch와의 완전한 호환성으로 학습 코드가 일반적인 nvidia gpu를 위한 pytorch 코드와 100% 동일하다는 것을 확인하실 수 있습니다. 또한 이를 넘어서 기존의 복잡한 병렬화 기법들을 MoAI Platform에서는 얼마나 효율적으로 구현할 수 있는지도 확인하실 수 있습니다.","우선 제공된 스크립트를 그대로 사용하여 튜토리얼을 끝까지 진행해 보시기를 권장합니다. 이후 스크립트를 원하는 대로 수정하셔서 Cerebras-GPT-13B 모델, 혹은 다른 공개된 모델을 다른 방식으로 fine-tuning하는 것도 얼마든지 가능합니다. 필요하시다면 Moreh에서 제공하는 MoAI Platform 응용 가이드( LLM Fine-tuning 파라미터 가이드)를 참고하십시오."]},{"l":"Training Code","p":["모든 코드는 일반적인 pytorch 사용 경험과 완벽하게 동일합니다.","먼저, transformers 라이브러리에서 필요한 모듈을 불러옵니다.","HuggingFace에 공개된 모델 config와 체크포인트를 불러옵니다.","Hugging Face에 공개된 학습 데이터셋을 불러와 전처리하고, 데이터 로더를 정의합니다. 이 튜토리얼에서는 Evol-Instruct-Python-26k 데이터셋을 사용합니다. 이 데이터셋은 주어진 프롬프트에 대응하여 작성된 파이썬 코드로 구성되어 있습니다.","이후 학습도 일반적인 Pytorch를 사용하여 모델 학습과 동일하게 진행됩니다.","위와 같이 MoAI Platform에서는 기존 pytorch 코드와 동일한 방식으로 작성하실 수 있습니다."]},{"l":"About Advanced Parallelism","p":["본 튜토리얼에 사용되는 학습 스크립트에서는 아래와 같은 코드가 추가로 한 줄 존재합니다. 이는 MoAI Platform에서 제공하는 최고의 병렬화 기능을 수행하는 코드입니다.","본 튜토리얼에서 사용하는 Cerebras-GPT-13B 과 같은 거대한 언어 모델의 경우 필연적으로 여러 개의 GPU를 사용하여 학습시켜야만 합니다. 이때, MoAI Platform이 아닌 다른 프레임워크를 사용할 경우, Data Parallel, Pipeline Parallel, Tensor Parallel과 같은 병렬화 기법을 도입하여 학습을 수행해야 합니다.","예를 들어, 사용자가 일반적인 pytorch 코드에서 DDP를 적용하고 싶다면, 다음과 같은 코드 스니펫이 추가되어야 합니다. ( https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","DDP는 비교적 쉽게 적용할 수 있지만, 파이프라인 병렬 처리 나 텐서 병렬 처리 를 적용하려면 상당히 복잡한 코드 수정이 필요합니다. 최적화된 병렬화 처리를 적용하려면 학습 스크립트 작성 과정에서 Python 코드가 다중 처리 환경에서 어떻게 동작하는지 이해해야 하며, 특히 다중 노드 설정에서는 학습에 사용되는 각 노드의 환경을 구성해야 합니다. 또한, 모델 종류, 크기, 데이터셋 등을 고려해 최적의 병렬화 방법을 찾기 위해서는 상당히 많은 시간이 필요합니다.","반면, MoAI Platform의 AP 기능을 통해 사용자는 별도의 병렬화 기법을 적용할 필요 없이, 학습 스크립트에 단 한 줄의 코드를 추가하는 것으로도 최적화된 병렬화 학습을 진행할 수 있습니다.","다른 프레임워크에서는 경험할 수 없는 MoAI Platform만의 Advanced Parallelization(AP) 기능을 통해 최적의 자동화된 분산 병렬처리를 경험해보세요.","AP기능을 이용하면 대규모 모델 훈련시 일반적으로 필요한 Pipeline Parallelism, Tensor Parallelism의 최적 매개변수와 환경변수를 아주 간단한 코드 한 줄로 설정할 수 있습니다."]}],[{"l":"3. 학습 실행하기","p":["이제 실제로 fine tuning을 실행해 보겠습니다."]},{"l":"가속기 Flavor 설정","p":["AMD MI210 GPU 32개 사용","AMD MI250 GPU 16개 사용","AMD MI300X GPU 8개 사용","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","MoAI Platform에서는 사용자에게 물리 GPU가 노출되지 않습니다. 대신 PyTorch에서 사용 가능한 가상의 MoAI Accelerator가 제공됩니다. 가속기의 flavor를 설정함으로써 실제로 PyTorch에서 물리 GPU를 얼마나 활용할지를 결정할 수 있습니다. 선택한 가속기 Flavor에 따라 총 학습 시간 및 GPU 사용 비용이 달라지므로 사용자의 학습 상황에 따른 판단이 필요합니다. 사용자의 학습 목표에 맞는 가속기 Flavor를 선택하기 위해 LLM Fine-tuning 파라미터 가이드 를 참고하세요.","moreh-switch-model 툴을 사용하여 현재 시스템에서 사용 가능한 가속기 flavor 리스트를 확인할 수 있습니다. 원활한 모델 학습을 위해 moreh-switch-model 명령어를 이용해 더 큰 메모리의 MoAI Accelerator로 변경할 수 있습니다.","Moreh의 체험판 컨테이너 사용 시: 선택","q 를 입력해 변경을 완료합니다.","따라서 처음 설정되어 있던 flavor를 로 전환한 다음 moreh-smi 명령을 사용하여 정상적으로 반영되었는지 확인하겠습니다.","로 잘 변경된 것을 확인할 수 있습니다.","먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","변경 사항이 잘 반영되었는지 확인하기 위해 다시 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","사용을 위해 8을 입력합니다.","앞서 GPT Fine-tuning - 시작하기 전에 에서 MoAI Accelerator를 확인했던 것을 기억하시나요? 이제 본격적인 학습 실행을 위해 필요한 가속기를 설정해보겠습니다.","여기서 번호를 입력하여 다른 flavor로 전환할 수 있습니다.","이번 튜토리얼에서는 2048GB 크기의 MoAI Accelerator를 이용하겠습니다.","튜토리얼을 계속 진행하기 위해 인프라 제공자에게 각 flavor에 대응되는 GPU 종류 및 개수를 문의하십시오. 다음 중 하나에 해당하는 flavor를 선택하여 계속 진행하십시오.","현재 사용중인 MoAI Accelerator의 메모리 크기는 256GB입니다."]},{"l":"학습 실행","p":["주어진 train_gpt.py 스크립트를 실행합니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력 될 것입니다. 로그를 통해 최적의 병렬화 설정을 찾는 Advanced Parallelism 기능이 정상 동작하는 것을 확인할 수 있습니다. 앞서 살펴 본 PyTorch 스크립트 상에서는 AP 코드 한 줄을 제외한 다른 부분에서 GPU 여러 개를 동시에 사용하기 위한 처리가 전혀 없었음을 참고하십시오.","Loss 값이 다음과 같이 떨어지며 정상 학습이 이루어지는 것을 확인할 수 있습니다.","학습 도중에 출력되는 throughput은 해당 PyTorch 스크립트를 통해 초당 몇 개의 token을 학습하고 있는지를 의미합니다.","AMD MI250 GPU 16개 사용 시: 약 6,800 tokens/sec","GPU 종류 및 개수에 따른 대략적인 학습 소요 시간은 다음과 같습니다.","AMD MI250 GPU 16개 사용 시: 약 81분"]},{"l":"학습 중에 가속기 상태 확인","p":["학습 도중에 터미널을 하나 더 띄워서 컨테이너에 접속한 후 moreh-smi 명령을 실행하시면 다음과 같이 MoAI Accelerator의 메모리를 점유하며 학습 스크립트가 실행되는 것을 확인하실 수 있습니다. 실행 로그상에서 초기화 과정이 끝나고 Step 1~ 15가 출력되는 도중에 확인해 보시기 바랍니다."]}],[{"l":"4. 학습 결과 확인하기","p":["앞 장과 같이 train_gpt.py 스크립트를 실행하면 결과 모델이 code_generation 디렉토리에 저장됩니다. 이는 순수한 PyTorch 모델 파라미터 파일로 MoAI Platform이 아닌 일반 GPU 서버에서도 100% 호환됩니다.","미리 다운로드한 GitHub 레포지토리의 tutorial 디렉토리 아래에 있는 inference_gpt.py 스크립트로 학습된 모델을 테스트해 볼 수 있습니다.","코드를 실행합니다.","출력값을 확인해보면 모델이 프롬프트 내용대로 적절한 함수를 생성한 것을 확인할 수 있습니다."]}],[{"l":"5. GPU 개수 변경하기","p":["앞과 동일한 fine-tuning 작업을 GPU 개수를 바꾸어 다시 실행해 보겠습니다. MoAI Platform은 GPU 자원을 단일 가속기로 추상화하여 제공하며 자동으로 병렬 처리를 수행합니다. 따라서 GPU 개수를 변경하더라도 PyTorch 스크립트는 전혀 고칠 필요가 없습니다."]},{"l":"가속기 Flavor 변경","p":["moreh-switch-model 툴을 사용하여 가속기 flavor를 전환합니다. 가속기 변경 방법은 3. 학습 실행하기 문서를 한번 더 참고해주시기 바랍니다.","인프라 제공자에게 문의하여 다음 중 하나를 선택한 다음 계속 진행하십시오.","AMD MI250 GPU 32개 사용","Moreh의 체험판 컨테이너 사용 시: 선택","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","AMD MI210 GPU 64개 사용","AMD MI300X GPU 16개 사용"]},{"l":"학습 실행","p":["다시 train_gpt.py 스크립트를 실행합니다.","사용 가능한 GPU 메모리가 2배가 늘었기 때문에, 배치 사이즈 또한 32로 변경하여 실행시켜 보겠습니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력될 것입니다.","앞서 GPU 개수가 절반이었을 때 실행한 결과와 비교해 동일하게 학습이 이루어지며 throughput이 향상되었음을 확인할 수 있습니다.","AMD MI250 GPU 16 → 32개 사용 시: 약 6,800 tokens/sec → 13,000 tokens/sec"]}],[{"l":"6. 마무리","p":["지금까지 MoAI Platform에서 HuggingFace에 공개된 GPT 기반 모델을 fine-tuning하는 과정을 살펴 보았습니다. MoAI Platform을 사용하면 기존의 학습 코드를 그대로 사용하면서 PyTorch 기반 오픈 소스 LLM 모델을 쉽게 GPU 클러스터에서 fine-tuning할 수 있습니다. 또한, MoAI 플랫폼을 사용한다면 여러분이 필요한 GPU 수를 코드 변경 없이 손쉽게 설정할 수 있습니다. 여러분만의 데이터로 새로운 모델을 빠르고 쉽게 개발해 보세요.","이 튜토리얼이나 MoAI Platform에 관련하여 궁금한 점이나 문의 사항이 있는 경우 Moreh( support@moreh.io)에 문의하시기 바랍니다."]},{"l":"더 알아보기","p":["MoAI Platform의 자동병렬화 기능, Advanced Parallelization (AP)","Llama3 8B Fine-tuning","Mistral Fine-tuning","Baichuan2 Fine-tuning","Qwen Fine-tuning"]}],[{"l":"Qwen Fine-tuning","p":["이 튜토리얼은 MoAI Platform에서 오픈 소스 Qwen1.5 7B 모델을 fine-tuning 하는 예시를 소개합니다. 튜토리얼을 통해 아래와 같은 MoAI Platform이 제공하는 여러 기능을 체험하며, AMD GPU 클러스터를 사용하는 방법을 익힐 수 있습니다.","사용자는 수십 개의 GPU를 MoAI Accelerator라는 하나의 가속기처럼 사용할 수 있어 복잡한 병렬화 작업이나 클러스터 환경 설정 없이도 쉽게 학습을 실행할 수 있습니다. 사용자는 리소스 관리에 신경쓰지 않고 학습에만 집중할 수 있습니다.","자동 병렬화 기능 덕분에 코드 작성과 개발이 간소화되며, 모델 학습 속도가 크게 향상됩니다. 이는 효율적인 자원 활용을 가능하게 하여 사용자가 더 빠르고 효과적으로 작업할 수 있도록 돕습니다."]},{"l":"개요","p":["MoAI Platform은 수천 대의 GPU를 쉽게 제어하여 AI 모델을 학습하거나 추론할 수 있는 확장 가능한 AI 플랫폼입니다. MoAI Platform의 특징은 모델을 fine-tuning할 때 가상화와 병렬화를 통해 매우 간단한 학습 방법을 제공한다는 점입니다.","MoAI Platform은 여러개의 GPU를 가상화하여 하나의 가속기인 MoAI Accelerator 로 제공합니다. 따라서 다중 GPU 사용을 위해 필요한 사전 준비나 코드 수정이 필요하지 않습니다.","MoAI Platform은 고객이 가상화된 MoAI Accelerator를 사용할 때 내부적으로 자동 최적화된 병렬화를 제공합니다. 모델 크기, 데이터 크기에 대해서 다양한 병렬화 방법을 고려해 최적의 병렬화 환경을 제공하며, 사용자는 별도의 작업이 없이 간단한 코드로 고성능 학습을 경험할 수 있습니다."]},{"l":"시작하기 전에","p":["MoAI Platform 상의 컨테이너 혹은 가상 머신을 인프라 제공자로부터 발급받고, 여기에 SSH로 접속하는 방법을 안내 받으시기 바랍니다. 예를 들어 MoAI Platform의 체험판 컨테이너 또는 MoAI Platform 기반으로 운영되는 퍼블릭 클라우드 서비스를 신청하여 사용할 수 있습니다.","MoAI Platform 체험판 컨테이너 사용 문의: support@moreh.io","KT Cloud Hyperscale AI Computing","SSH로 접속한 다음 moreh-smi 명령을 실행하여 MoAI Accelerator가 잘 표시되는지 확인하시기 바랍니다. 디바이스 이름은 시스템마다 다르게 설정되어 있을 수 있습니다."]},{"l":"MoAI Accelerator 확인","p":["이 튜토리얼에서 안내할 Qwen 모델과 같은 sLLM을 학습하기 위해서는 적절한 크기의 MoAI Accelerator를 선택해야 합니다. 먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","수행할 학습에 필요한 구체적인 MoAI Accelerator 설정에 대한 설명은 3. 학습 실행하기 에서 제공하겠습니다."]}],[{"l":"1. Fine-tuning 준비하기","p":["MoAI Platform에서 PyTorch 스크립트 실행 환경을 준비하는 것은 일반적인 GPU 서버에서와 크게 다르지 않습니다. 단, 튜토리얼 진행을 위해 아래의 사양들이 권장됩니다.","CPU: 16 core 이상","memory: 256GB 이상","MAF 버전: 24.5.0","스토리지: 61GB 이상","원할한 튜토리얼 진행 전 실행 환경을 확인하시길 바랍니다."]},{"l":"PyTorch 설치 여부 확인하기","p":["SSH로 컨테이너에 접속한 다음 아래와 같이 실행하여 현재 conda 환경에 PyTorch가 설치되어 있는지 확인합니다.","버전명에는 PyTorch 버전과 이를 실행시키기 위한 MoAI 버전이 함께 표시되어 있습니다. 위 예시의 경우 PyTorch 1.13.1+cu116 버전을 실행하는 MoAI의 24.5.0 버전이 설치되어 있음을 의미합니다.","만약 conda: command not found 메시지가 표시되거나, torch 패키지가 리스트되지 않거나, 혹은 torch 패키지가 존재하더라도 버전명에 “moreh”가 포함되지 않은 경우 MoAI Platform에서 Fine-tuning 준비하기 문서에 따라 conda 환경을 생성하십시오.","만약 해당 MoAI 버전이 24.5.0이 아닌 다른 버전이라면 아래의 코드를 실행시키십시오."]},{"l":"PyTorch 동작 여부 확인하기","p":["다음과 같이 실행하여 torch 패키지가 정상적으로 import되고 MoAI Accelerator가 인식되는지 확인합니다."]},{"l":"학습 스크립트 다운로드","p":["다음과 같이 실행하여 GitHub 레포지토리에서 학습을 위한 PyTorch 스크립트를 다운로드합니다. 본 튜토리얼에서는 tutorial 디렉토리 안에 있는 train_qwen.py 스크립트를 사용할 것입니다."]},{"l":"필요 Python 패키지 설치","p":["다음과 같이 실행하여 스크립트 실행에 필요한 서드 파티 Python 패키지들을 미리 설치합니다."]}],[{"l":"2. Moreh의 학습 코드 톺아보기","p":["학습 데이터를 모두 준비하셨다면 다음으로는 실제 fine-tuning 과정을 실행할 train_qwen.py 스크립트의 내용에 대해 살펴 보겠습니다. 이 스크립트는 통상적인 PyTorch 코드로서 Hugging Face Transformers 라이브러리에 있는 Qwen 모델 구현을 기반으로 fine-tuning 작업을 실행합니다.","이번 단계에서는 MoAI Platform은 pytorch와의 완전한 호환성으로 학습 코드가 일반적인 nvidia gpu를 위한 pytorch 코드와 100% 동일하다는 것을 확인하실 수 있습니다. 또한 이를 넘어서 기존의 복잡한 병렬화 기법들을 MoAI Platform에서는 얼마나 효율적으로 구현할 수 있는지도 확인하실 수 있습니다.","우선 제공된 스크립트를 그대로 사용하여 튜토리얼을 끝까지 진행해 보시기를 권장합니다. 이후 스크립트를 원하는 대로 수정하셔서 Qwen1.5 7B 모델을 다른 방식으로 fine-tuning하는 것도 얼마든지 가능합니다. MoAI Platform은 PyTorch와의 완전한 호환성을 제공하기 때문입니다. 필요하시다면 Moreh에서 제공하는 MoAI Platform 응용 가이드( LLM Fine-tuning 파라미터 가이드) 참고하십시오."]},{"l":"Training Code","p":["모든 코드는 일반적인 pytorch 사용 경험과 완벽하게 동일합니다.","먼저, transformers 라이브러리에서 필요한 모듈을 불러옵니다.","HuggingFace에 공개된 모델 config와 체크포인트를 불러옵니다.","Hugging Face에 공개된 학습 데이터셋 을 불러와 전처리하고, 데이터 로더를 정의합니다.","이후 학습도 일반적인 Pytorch를 사용하여 모델 학습과 동일하게 진행됩니다.","위와 같이 MoAI Platform에서는 기존 pytorch 코드와 동일한 방식으로 작성하실 수 있습니다."]},{"l":"About Advanced Parallelism","p":["본 튜토리얼에 사용되는 학습 스크립트에는 아래와 같은 코드가 추가로 한 줄 존재합니다. 이는 MoAI Platform에서 제공하는 자동 병렬화 기능을 수행하는 코드입니다.","본 튜토리얼에서 사용하는 Qwen1.5 7B 와 같은 거대한 언어 모델을 학습시키기 위해서는 필연적으로 여러 개의 GPU를 사용해야 합니다. 다른 프레임워크를 사용할 경우 Data Parallel, Pipeline Parallel, Tensor Parallel 등의 병렬화 기법을 도입하여 학습을 진행해야 합니다.","예를 들어, 사용자가 일반적인 pytorch 코드에서 DDP를 적용하고 싶다면, 다음과 같은 코드 스니펫이 추가되어야 합니다. ( https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","DDP는 비교적 쉽게 적용할 수 있지만, 파이프라인 병렬 처리 나 텐서 병렬 처리 를 적용하려면 상당히 복잡한 코드 수정이 필요합니다. 최적화된 병렬화 처리를 적용하려면 학습 스크립트 작성 과정에서 Python 코드가 다중 처리 환경에서 어떻게 동작하는지 이해해야 하며, 특히 다중 노드 설정에서는 학습에 사용되는 각 노드의 환경을 구성해야 합니다. 또한, 모델 종류, 크기, 데이터셋 등을 고려해 최적의 병렬화 방법을 찾기 위해서는 상당히 많은 시간이 필요합니다.","반면, MoAI Platform의 AP 기능을 통해 사용자는 별도의 병렬화 기법을 적용할 필요 없이, 학습 스크립트에 단 한 줄의 코드를 추가하는 것으로도 최적화된 병렬화 학습을 진행할 수 있습니다.","다른 프레임워크에서는 경험할 수 없는 MoAI Platform만의 Advanced Parallelization(AP) 기능을 통해 최적의 자동화된 분산 병렬처리 를 경험해보세요. AP기능을 이용하면 대규모 모델 훈련시 일반적으로 필요한 Pipeline Parallelism, Tensor Parallelism의 최적 매개변수와 환경변수를 아주 간단한 코드 한 줄로 설정할 수 있습니다."]}],[{"l":"3. 학습 실행하기","p":["이제 실제로 fine tuning을 실행해 보겠습니다."]},{"l":"가속기 Flavor 설정","p":["AMD MI210 GPU 32개 사용","AMD MI250 GPU 16개 사용","AMD MI300X GPU 8개 사용","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","MoAI Platform에서는 사용자에게 물리 GPU가 노출되지 않습니다. 대신 PyTorch에서 사용 가능한 가상의 MoAI Accelerator가 제공됩니다. 사용자는 가속기의 Flavor를 설정하여 실제 물리 GPU를 어떻게 활용할지 결정할 수 있습니다. 선택한 가속기 Flavor에 따라 총 학습 시간과 GPU 사용 비용이 달라지므로 사용자는 학습 상황에 따라 판단해야 합니다. 사용자의 학습 목표에 맞는 가속기 Flavor를 선택하기 위해 LLM Fine-tuning 파라미터 가이드 를 참고하세요.","moreh-switch-model 툴을 사용하여 현재 시스템에서 사용 가능한 가속기 flavor 리스트를 확인할 수 있습니다. 원활한 모델 학습을 위해 moreh-switch-model 명령어를 이용해 더 큰 메모리의 MoAI Accelerator로 변경할 수 있습니다.","Moreh의 체험판 컨테이너 사용 시: 선택","q 를 입력해 변경을 완료합니다.","다음 중 하나에 해당하는 flavor를 선택하여 계속 진행하십시오.","따라서 처음 설정되어 있던 flavor를 로 전환한 다음 moreh-smi 명령을 사용하여 정상적으로 반영되었는지 확인하겠습니다.","로 잘 변경된 것을 확인할 수 있습니다.","먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","변경 사항이 잘 반영되었는지 확인하기 위해 다시 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","사용을 위해 8을 입력합니다.","앞서 Qwen Fine-tuning - 시작하기 전에 에서 MoAI Accelerator를 확인했던 것을 기억하시나요? 이제 본격적인 학습 실행을 위해 필요한 가속기를 설정해보겠습니다.","여기서 번호를 입력하여 다른 flavor로 전환할 수 있습니다.","이번 튜토리얼에서는 2048GB 크기의 MoAI Accelerator를 이용하겠습니다.","튜토리얼을 계속 진행하기 위해 인프라 제공자에게 각 flavor에 대응되는 GPU 종류 및 개수를 문의하십시오. 다음 중 하나에 해당하는 flavor를 선택하여 계속 진행하십시오.","현재 사용중인 MoAI Accelerator의 메모리 크기는 512GB입니다."]},{"l":"학습 실행","p":["주어진 train_qwen.py 스크립트를 실행합니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력될 것입니다. 중간에 파란색으로 표시된 부분을 보시면 Advanced Parallelism 기능이 정상 동작하는 것을 확인할 수 있습니다. 앞서 살펴본 PyTorch 스크립트에서는 AP 코드 한 줄을 제외한 다른 부분에서 GPU 여러 개를 동시에 사용하기 위한 처리가 전혀 없었음을 참고하십시오.","Loss 값이 아래와 같이 나타나며 정상적으로 학습이 진행되고 있음을 확인할 수 있습니다.","학습 중에 표시되는 throughput은 해당 PyTorch 스크립트를 통해 초당 몇 개의 토큰을 학습하고 있는지를 의미합니다.","AMD MI250 GPU 16개 사용 시: 약 190,000 tokens/sec","GPU 종류 및 개수에 따른 대략적인 학습 소요 시간은 다음과 같습니다.","AMD MI250 GPU 16개 사용 시: 약 16분"]},{"l":"학습 중에 가속기 상태 확인","p":["학습 도중에 터미널을 하나 더 띄워서 컨테이너에 접속한 후 moreh-smi 명령을 실행하시면 다음과 같이 MoAI Accelerator의 메모리가 차지되는 것을 확인할 수 있습니다. 실행 로그를 보면 초기화 과정이 완료되고 Loss가 출력되는 도중에 확인해 보세요."]}],[{"l":"4. 학습 결과 확인하기","p":["앞 장과 같이 train_qwen.py 스크립트를 실행하면 결과 모델이 qwen_code_generation 디렉터리에 저장됩니다. 이는 순수한 PyTorch 모델 파라미터 파일로, MoAI Platform이 아닌 일반 GPU 서버에서도 완벽히 호환됩니다.","미리 다운로드한 GitHub 레포지토리의 tutorial 디렉터리 아래에 있는 inference_qwen.py 스크립트로 학습된 모델을 테스트해 볼 수 있습니다. 테스트에서는 \"주어진 문자열 리스트를 입력받아 공백으로 결합하는 함수를 만들어\"라는 프롬프트가 사용되었습니다.","코드를 실행합니다.","출력값을 확인해보면 모델이 프롬프트 내용대로 적절한 함수를 생성한 것을 확인할 수 있습니다."]}],[{"l":"5. GPU 개수 변경하기","p":["앞과 동일한 fine tuning 작업을 GPU 개수를 바꾸어 다시 실행해 보겠습니다. MoAI Platform은 GPU 자원을 단일 가속기로 추상화하여 제공하며 자동으로 병렬 처리를 수행합니다. 그러므로 GPU 개수를 변경하더라도 PyTorch 스크립트를 수정할 필요가 전혀 없습니다."]},{"l":"가속기 Flavor 변경","p":["moreh-switch-model 툴을 사용하여 가속기 flavor를 전환합니다. 가속기 변경 방법은 3. 학습 실행하기 문서를 한번 더 참고해주시기 바랍니다.","인프라 제공자에게 문의하여 다음 중 하나를 선택한 다음 계속 진행하십시오.","AMD MI250 GPU 32개 사용","Moreh의 체험판 컨테이너 사용 시: 선택","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","AMD MI210 GPU 64개 사용","AMD MI300X GPU 16개 사용"]},{"l":"학습 실행","p":["다시 별도의 배치 사이즈 변경 없이 train_qwen.py 스크립트를 실행합니다.","사용 가능한 GPU 메모리가 2배 늘었기 때문에, 배치 사이즈 또한 기존 256 에서 512로 변경하여 실행시켜 보겠습니다. 학습이 정상적으로 진행된다면 다음과 같은 로그가 출력될 것입니다.","앞서 GPU 개수가 절반이었을 때 실행한 결과와 비교해 동일하게 학습이 이루어지며 throughput이 향상되었음을 확인할 수 있습니다.","AMD MI250 GPU 16 → 32개 사용 시: 약 190,000 tokens/sec → 380,000 tokens/sec"]}],[{"l":"6. 마무리","p":["지금까지 MoAI Platform에서 Qwen1.5 7B 모델을 fine-tuning하는 과정을 살펴 보았습니다. MoAI Platform을 사용하면 기존의 학습 코드를 그대로 사용하면서 PyTorch 기반 오픈 소스 LLM 모델을 쉽게 GPU 클러스터에서 fine-tuning할 수 있습니다. 또한, MoAI 플랫폼을 사용한다면 여러분이 필요한 GPU 수를 코드 변경 없이 손쉽게 설정할 수 있습니다. 여러분만의 데이터로 새로운 모델을 빠르고 쉽게 개발해 보세요.","이 튜토리얼이나 MoAI Platform에 관련하여 궁금한 점이나 문의 사항이 있는 경우 Moreh( support@moreh.io)에 문의하시기 바랍니다."]},{"l":"더 알아보기","p":["MoAI Platform의 자동병렬화 기능, Advanced Parallelization (AP)","GPT Fine-tuning","Llama3 8B Fine-tuning","Llama3 70B Fine-tuning","Mistral Fine-tuning","Baichuan2 Fine-tuning"]}],[{"l":"Baichuan2 Fine-tuning","p":["이 튜토리얼은 MoAI Platform에서 오픈 소스 Baichuan2 13B 모델을 fine-tuning하는 예시를 소개합니다. 튜토리얼을 통해 아래와 같은 MoAI Platform이 제공하는 여러 기능을 체험하며, AMD GPU 클러스터를 사용하는 방법을 익힐 수 있습니다.","사용자는 수십 개의 GPU를 MoAI Accelerator라는 하나의 가속기처럼 사용할 수 있어 복잡한 병렬화 작업이나 클러스터 환경 설정 없이도 쉽게 학습을 실행할 수 있습니다. 사용자는 리소스 관리에 신경쓰지 않고 학습에만 집중할 수 있습니다.","자동 병렬화 기능 덕분에 코드 작성과 개발이 간소화되며, 모델 학습 속도가 크게 향상됩니다. 이는 효율적인 자원 활용을 가능하게 하여 사용자가 더 빠르고 효과적으로 작업할 수 있도록 돕습니다."]},{"l":"개요","p":["MoAI Platform은 수천 대의 GPU를 쉽게 제어하여 AI 모델을 학습하거나 추론할 수 있는 확장 가능한 AI 플랫폼입니다. MoAI Platform의 특징은 모델을 fine-tuning할 때 가상화와 병렬화를 통해 매우 간단한 학습 방법을 제공한다는 점입니다.","MoAI Platform은 여러개의 GPU를 가상화하여 하나의 가속기인 MoAI Accelerator 로 제공합니다. 따라서 다중 GPU 사용을 위해 필요한 사전 준비나 코드 수정이 필요하지 않습니다.","MoAI Platform은 고객이 가상화된 MoAI Accelerator를 사용할 때 내부적으로 자동 최적화된 병렬화를 제공합니다. 모델 크기, 데이터 크기에 대해서 다양한 병렬화 방법을 고려해 최적의 병렬화 환경을 제공하며, 사용자는 별도의 작업이 없이 간단한 코드로 고성능 학습을 경험할 수 있습니다."]},{"l":"시작하기 전에","p":["MoAI Platform 상의 컨테이너 혹은 가상 머신을 인프라 제공자로부터 발급받고, 여기에 SSH로 접속하는 방법을 안내 받으시기 바랍니다. 예를 들어 MoAI Platform의 체험판 컨테이너 또는 MoAI Platform 기반으로 운영되는 퍼블릭 클라우드 서비스를 신청하여 사용할 수 있습니다.","MoAI Platform 체험판 컨테이너 사용 문의: support@moreh.io","KT Cloud Hyperscale AI Computing","SSH로 접속한 다음 moreh-smi 명령을 실행하여 MoAI Accelerator가 잘 표시되는지 확인하시기 바랍니다. 디바이스 이름은 시스템마다 다르게 설정되어 있을 수 있습니다."]},{"l":"MoAI Accelerator 확인","p":["이 튜토리얼에서 안내할 Baichuan2 13B 모델과 같은 sLLM을 학습하기 위해서는 적절한 크기의 MoAI Accelerator를 선택해야 합니다. 먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","수행할 학습에 필요한 구체적인 MoAI Accelerator 설정에 대한 설명은 3. 학습 실행하기 에서 제공하겠습니다."]}],[{"l":"1. Fine-tuning 준비하기","p":["MoAI Platform에서 PyTorch 스크립트 실행 환경을 준비하는 것은 일반적인 GPU 서버에서와 크게 다르지 않습니다. 단, 튜토리얼 진행을 위해 아래의 사양들이 권장됩니다:","CPU: 16 core 이상","memory: 256GB 이상","MAF 버전: 24.5.0","스토리지: 55GB 이상","원할한 튜토리얼 진행 전 실행 환경을 확인하시길 바랍니다."]},{"l":"PyTorch 설치 여부 확인하기","p":["SSH로 컨테이너에 접속한 다음 아래와 같이 실행하여 현재 conda 환경에 PyTorch가 설치되어 있는지 확인합니다.","버전명에는 PyTorch 버전과 이를 실행시키기 위한 MoAI 버전이 함께 표시되어 있습니다. 위 예시의 경우 PyTorch 1.13.1+cu116 버전을 실행하는 MoAI의 24.5.0 버전이 설치되어 있음을 의미합니다.","만약 conda: command not found 메시지가 표시되거나, torch 패키지가 리스트되지 않거나, 혹은 torch 패키지가 존재하더라도 버전명에 “moreh”가 포함되지 않은 경우 (MoAI Platform에서 Fine-tuning 준비하기) 문서에 따라 conda 환경을 생성하십시오.","만약 해당 MoAI 버전이 24.5.0이 아닌 다른 버전이라면 아래의 코드를 실행시키십시오."]},{"l":"PyTorch 동작 여부 확인하기","p":["다음과 같이 실행하여 torch 패키지가 정상적으로 import되고 MoAI Accelerator가 인식되는지 확인합니다."]},{"l":"학습 스크립트 다운로드","p":["다음과 같이 실행하여 GitHub 레포지토리에서 학습을 위한 PyTorch 스크립트를 다운로드합니다. 본 튜토리얼에서는 tutorial 디렉토리 안에 있는 train_baichuan2_13b.py 스크립트를 사용할 것입니다."]},{"l":"필요 Python 패키지 설치","p":["다음과 같이 실행하여 스크립트 실행에 필요한 서드 파티 Python 패키지들을 미리 설치합니다."]}],[{"l":"2. Moreh의 학습 코드 톺아보기","p":["학습 데이터를 모두 준비하셨다면 다음으로는 실제 fine-tuning 과정을 실행할 train_baichuan2_13b.py 스크립트의 내용을 살펴 보겠습니다. 이번 단계에서는 MoAI Platform은 pytorch와의 완전한 호환성으로 학습 코드가 일반적인 nvidia gpu를 위한 pytorch 코드와 100% 동일하다는 것을 확인하실 수 있습니다. 또한 이를 넘어서 기존의 복잡한 병렬화 기법들을 MoAI Platform에서는 얼마나 효율적으로 구현할 수 있는지도 확인하실 수 있습니다.","우선 제공된 스크립트를 그대로 사용하여 튜토리얼을 끝까지 진행해 보시기를 권장합니다. 이후 스크립트를 원하는 대로 수정하셔서 Baichuan 모델을 다른 방식으로 fine-tuning하는 것도 얼마든지 가능합니다. 필요하시다면 Moreh에서 제공하는 MoAI Platform 응용 가이드 LLM Fine-tuning 파라미터 가이드 를 참고하십시오."]},{"l":"Training Code","p":["모든 코드는 일반적인 pytorch 사용 경험과 완벽하게 동일합니다.","먼저, transformers 라이브러리에서 필요한 모듈을 불러옵니다.","HuggingFace에 공개된 모델 체크포인트를 불러옵니다.","모델 최적화를 위해 quickstart repository에 미리 정의된 BaichuanForCausalLM 를 사용합니다.","Hugging Face에 공개된 학습 데이터셋 을 불러와 전처리하고, 데이터 로더를 정의합니다.","이후 학습도 일반적인 Pytorch를 사용하여 모델 학습과 동일하게 진행됩니다.","위와 같이 MoAI Platform에서는 기존에 사용하시던 PyTorch 스크립트를 수정 없이 동일하게 사용하실 수 있습니다."]},{"l":"About Advanced Parallelism","p":["본 튜토리얼에 사용되는 학습 스크립트에는 아래와 같은 코드가 추가로 한 줄 존재합니다. 이는 MoAI Platform에서 제공하는 자동 병렬화 기능을 수행하는 코드입니다.","본 튜토리얼에서 사용하는 Baichuan2 13B 와 같은 거대한 언어 모델의 경우 필연적으로 여러 개의 GPU를 사용하여 학습시켜야만 합니다. 이때, MoAI Platform이 아닌 다른 프레임워크를 사용할 경우, Data Parallel, Pipeline Parallel, Tensor Parallel과 같은 병렬화 기법을 도입하여 학습을 수행해야 합니다.","예를 들어, 사용자가 일반적인 pytorch 코드에서 DDP를 적용하고 싶다면, 다음과 같은 코드 스니펫이 추가되어야 합니다. ( https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)","DDP는 비교적 쉽게 적용할 수 있지만, 파이프라인 병렬 처리 나 텐서 병렬 처리 를 적용하려면 상당히 복잡한 코드 수정이 필요합니다. 최적화된 병렬화 처리를 적용하려면 학습 스크립트 작성 과정에서 Python 코드가 다중 처리 환경에서 어떻게 동작하는지 이해해야 하며, 특히 다중 노드 설정에서는 학습에 사용되는 각 노드의 환경을 구성해야 합니다. 또한, 모델 종류, 크기, 데이터셋 등을 고려해 최적의 병렬화 방법을 찾기 위해서는 상당히 많은 시간이 필요합니다.","반면, MoAI Platform의 AP 기능을 통해 사용자는 별도의 병렬화 기법을 적용할 필요 없이, 학습 스크립트에 단 한 줄의 코드를 추가하는 것으로도 최적화된 병렬화 학습을 진행할 수 있습니다.","다른 프레임워크에서는 경험할 수 없는 MoAI Platform만의 Advanced Parallelization(AP) 기능을 통해 최적의 자동화된 분산 병렬처리를 경험해보세요. AP기능을 이용하면 대규모 모델 훈련시 일반적으로 필요한 Pipeline Parallelism, Tensor Parallelism의 최적 매개변수와 환경변수를 아주 간단한 코드 한 줄로 설정할 수 있습니다."]}],[{"l":"3. 학습 실행하기","p":["이제 실제로 fine tuning을 실행해 보겠습니다."]},{"l":"가속기 Flavor 설정","p":["4xLarge.2048GB 로 잘 변경된 것을 확인할 수 있습니다.","AMD MI210 GPU 32개 사용","AMD MI250 GPU 16개 사용","AMD MI300X GPU 8개 사용","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","MoAI Platform에서는 사용자에게 물리 GPU가 노출되지 않습니다. 대신 PyTorch에서 사용 가능한 가상의 MoAI Accelerator가 제공됩니다. 가속기의 flavor를 설정함으로써 실제로 PyTorch에서 물리 GPU를 얼마나 활용할지를 결정할 수 있습니다. 선택한 가속기 Flavor에 따라 총 학습 시간과 GPU 사용 비용이 달라지므로 사용자의 학습 상황에 따른 판단이 필요합니다. 사용자의 학습 목표에 맞는 가속기 Flavor를 선택하기 위해 LLM Fine-tuning 파라미터 가이드 를 참고하세요.","moreh-switch-model 툴을 사용하여 현재 시스템에서 사용 가능한 가속기 flavor 리스트를 확인할 수 있습니다. 원활한 모델 학습을 위해 moreh-switch-model 명령어를 이용해 더 큰 메모리의 MoAI Accelerator로 변경할 수 있습니다.","Moreh의 체험판 컨테이너 사용 시: 선택","q 를 입력해 변경을 완료합니다.","따라서 처음 설정되어 있던 flavor를 로 전환한 다음 moreh-smi 명령을 사용하여 정상적으로 반영되었는지 확인하겠습니다.","먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","변경 사항이 잘 반영되었는지 확인하기 위해 다시 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","사용을 위해 8을 입력합니다.","앞서 Baichuan2 Finetuning - 시작하기 전에 에서 MoAI Accelerator를 확인했던 것을 기억하시나요? 이제 본격적인 학습 실행을 위해 필요한 가속기를 설정해보겠습니다.","여기서 번호를 입력하여 다른 flavor로 전환할 수 있습니다.","이번 튜토리얼에서는 2048GB 크기의 MoAI Accelerator를 이용하겠습니다.","튜토리얼을 계속 진행하기 위해 인프라 제공자에게 각 flavor에 대응되는 GPU 종류 및 개수를 문의하십시오. 다음 중 하나에 해당하는 flavor를 선택하여 계속 진행하십시오.","현재 사용중인 MoAI Accelerator의 메모리 크기는 256GB입니다."]},{"l":"학습 실행","p":["주어진 train_baichuan2.py 스크립트를 실행합니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력 될 것입니다. 로그를 통해 최적의 병렬화 설정을 찾는 Advanced Parallelism 기능이 정상 동작하는 것을 확인할 수 있습니다. 앞서 살펴 본 PyTorch 스크립트 상에서는 AP 코드 한 줄을 제외한 다른 부분에서 GPU 여러 개를 동시에 사용하기 위한 처리가 전혀 없었음을 참고하십시오.","Loss 값이 다음과 같이 떨어지며 정상 학습이 이루어지는 것을 확인할 수 있습니다.","학습 도중에 출력되는 throughput은 해당 PyTorch 스크립트를 통해 초당 몇 개의 token을 학습하고 있는지를 의미합니다.","AMD MI250 GPU 8개 사용 시: 약 191,000 tokens/sec","GPU 종류 및 개수에 따른 대략적인 학습 소요 시간은 다음과 같습니다.","AMD MI250 GPU 8개 사용 시: 약 30분"]},{"l":"학습 중에 가속기 상태 확인","p":["학습 도중에 터미널을 하나 더 띄워서 컨테이너에 접속한 후 moreh-smi 명령을 실행하시면 다음과 같이 MoAI Accelerator의 메모리를 점유하며 학습 스크립트가 실행되는 것을 확인하실 수 있습니다. 실행 로그상에서 초기화 과정이 끝나고 Loss가 출력되는 도중에 확인해 보시기 바랍니다."]}],[{"l":"4. 학습 결과 확인하기","p":["앞 장과 같이 train_baichuan2_13b.py 스크립트를 실행하면 결과 모델이 baichuan_code_generation 디렉토리에 저장됩니다. 이는 순수한 PyTorch 모델 파라미터 파일로 MoAI Platform이 아닌 일반 GPU 서버에서도 100% 호환됩니다.","미리 다운로드한 GitHub 레포지토리의 tutorial 디렉토리 아래에 있는 inference_baichuan.py 스크립트로 학습된 모델을 테스트해 볼 수 있습니다.","코드를 실행합니다.","출력값을 확인해보면 모델이 프롬프트에 대한 적절한 답변 생성한 것을 확인할 수 있습니다."]}],[{"l":"5. GPU 개수 변경하기","p":["앞과 동일한 fine-tuning 작업을 GPU 개수를 바꾸어 다시 실행해 보겠습니다. MoAI Platform은 GPU 자원을 단일 가속기로 추상화하여 제공하며 자동으로 병렬 처리를 수행합니다. 따라서 GPU 개수를 변경하더라도 PyTorch 스크립트는 전혀 고칠 필요가 없습니다."]},{"l":"가속기 Flavor 변경","p":["moreh-switch-model 툴을 사용하여 가속기 flavor를 전환합니다. 가속기 변경 방법은 3. 학습 실행하기 문서를 한번 더 참고해주시기 바랍니다.","인프라 제공자에게 문의하여 다음 중 하나를 선택한 다음 계속 진행하십시오.","AMD MI250 GPU 32개 사용","Moreh의 체험판 컨테이너 사용 시: 선택","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","AMD MI210 GPU 64개 사용","AMD MI300X GPU 16개 사용"]},{"l":"학습 실행","p":["다시 train_baichuan2_13b.py 스크립트를 실행합니다.","사용 가능한 GPU 메모리가 2배가 늘었기 때문에, 배치 사이즈 또한 512로 변경하여 실행시켜 보겠습니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력될 것입니다.","앞서 GPU 개수가 절반이었을 때 실행한 결과와 비교해 동일하게 학습이 이루어지며 throughput이 향상되었음을 확인할 수 있습니다.","AMD MI250 GPU 16 → 32개 사용 시: 약 198,000 tokens/sec → 370,000 tokens/sec"]}],[{"l":"6. 마무리","p":["지금까지 MoAI Platform에서 HuggingFace에 공개된 Baichuan2 13B 모델을 fine-tuning하는 과정을 살펴 보았습니다. MoAI Platform을 사용하면 기존의 학습 코드를 그대로 사용하면서 PyTorch 기반 오픈 소스 LLM 모델을 쉽게 GPU 클러스터에서 fine-tuning할 수 있습니다. 또한, MoAI 플랫폼을 사용한다면 여러분이 필요한 GPU 수를 코드 변경 없이 손쉽게 설정할 수 있습니다. 여러분만의 데이터로 새로운 모델을 빠르고 쉽게 개발해 보세요.","이 튜토리얼이나 MoAI Platform에 관련하여 궁금한 점이나 문의 사항이 있는 경우 Moreh( support@moreh.io)에 문의하시기 바랍니다."]},{"l":"더 알아보기","p":["MoAI Platform의 자동병렬화 기능, Advanced Parallelization (AP)","Llama3 8B Fine-tuning","Mistral Fine-tuning","GPT Fine-tuning","Qwen Fine-tuning"]}],[{"l":"Llama2 13B Fine-tuning","p":["이 튜토리얼은 MoAI Platform에서 오픈 소스 Llama2 13B 을 fine-tuning하는 예시를 소개합니다. 튜토리얼을 통해 아래와 같은 MoAI Platform이 제공하는 여러 기능을 체험하며, AMD GPU 클러스터를 사용하는 방법을 익힐 수 있습니다.","사용자는 수십 개의 GPU를 MoAI Accelerator라는 하나의 가속기처럼 사용할 수 있어 복잡한 병렬화 작업이나 클러스터 환경 설정 없이도 쉽게 학습을 실행할 수 있습니다. 사용자는 리소스 관리에 신경쓰지 않고 학습에만 집중할 수 있습니다.","자동 병렬화 기능 덕분에 코드 작성과 개발이 간소화되며, 모델 학습 속도가 크게 향상됩니다. 이는 효율적인 자원 활용을 가능하게 하여 사용자가 더 빠르고 효과적으로 작업할 수 있도록 돕습니다."]},{"l":"개요","p":["MoAI Platform은 수천 대의 GPU를 쉽게 제어하여 AI 모델을 학습하거나 추론할 수 있는 확장 가능한 AI 플랫폼입니다. MoAI Platform의 특징은 모델을 fine-tuning할 때 가상화와 병렬화를 통해 매우 간단한 학습 방법을 제공한다는 점입니다.","MoAI Platform은 여러개의 GPU를 가상화하여 하나의 가속기인 MoAI Accelerator 로 제공합니다. 따라서 다중 GPU 사용을 위해 필요한 사전 준비나 코드 수정이 필요하지 않습니다.","MoAI Platform은 고객이 가상화된 MoAI Accelerator를 사용할 때 내부적으로 자동 최적화된 병렬화를 제공합니다. 모델 크기, 데이터 크기에 대해서 다양한 병렬화 방법을 고려해 최적의 병렬화 환경을 제공하며, 사용자는 별도의 작업이 없이 간단한 코드로 고성능 학습을 경험할 수 있습니다."]},{"l":"시작하기 전에","p":["MoAI Platform 상의 컨테이너 혹은 가상 머신을 인프라 제공자로부터 발급받고, 여기에 SSH로 접속하는 방법을 안내 받으시기 바랍니다. 예를 들어 MoAI Platform의 체험판 컨테이너 또는 MoAI Platform 기반으로 운영되는 퍼블릭 클라우드 서비스를 신청하여 사용할 수 있습니다.","MoAI Platform 체험판 컨테이너 사용 문의: support@moreh.io","KT Cloud Hyperscale AI Computing ( https://cloud.kt.com/solution/hyperscaleAiComputing/)","SSH로 접속한 다음 moreh-smi 명령을 실행하여 MoAI Accelerator가 잘 표시되는지 확인하시기 바랍니다. 디바이스 이름은 시스템마다 다르게 설정되어 있을 수 있습니다."]},{"l":"MoAI Accelerator 확인","p":["이 튜토리얼에서 안내할 Llama2 모델과 같은 sLLM을 학습하기 위해서는 적절한 크기의 MoAI Accelerator를 선택해야 합니다. 먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","수행할 학습에 필요한 구체적인 MoAI Accelerator 설정에 대한 설명은 3. 학습 실행하기 에서 제공하겠습니다."]}],[{"l":"1. Fine-tuning 준비하기","p":["MoAI Platform에서 PyTorch 스크립트 실행 환경을 준비하는 것은 일반적인 GPU 서버에서와 크게 다르지 않습니다. 단, 튜토리얼 진행을 위해 아래의 사양들이 권장됩니다:","CPU: 16 core 이상","memory: 256GB 이상","MAF 버전: 24.5.0","스토리지: 105GB 이상","원할한 튜토리얼 진행 전 실행 환경을 확인하시길 바랍니다."]},{"l":"PyTorch 설치 여부 확인하기","p":["SSH로 컨테이너에 접속한 다음 아래와 같이 실행하여 현재 conda 환경에 PyTorch가 설치되어 있는지 확인합니다.","버전명에는 PyTorch 버전과 이를 실행시키기 위한 MoAI 버전이 함께 표시되어 있습니다. 위 예시의 경우 PyTorch 1.13.1+cu116 버전을 실행하는 MoAI의 24.5.0 버전이 설치되어 있음을 의미합니다.","만약 conda: command not found 메시지가 표시되거나, torch 패키지가 리스트되지 않거나, 혹은 torch 패키지가 존재하더라도 버전명에 “moreh”가 포함되지 않은 경우 MoAI Platform에서 Fine-tuning 준비하기 문서에 따라 conda 환경을 생성하십시오.","만약 해당 MoAI 버전이 24.5.0이 아닌 다른 버전이라면 아래의 코드를 실행시키십시오."]},{"l":"PyTorch 동작 여부 확인하기","p":["다음과 같이 실행하여 torch 패키지가 정상적으로 import 되고 MoAI Accelerator가 인식되는지 확인합니다."]},{"l":"학습 스크립트 다운로드","p":["다음과 같이 실행하여 GitHub 레포지토리에서 학습을 위한 PyTorch 스크립트를 다운로드합니다. 본 튜토리얼에서는 tutorial 디렉토리 안에 있는 train_llama2.py 스크립트를 사용할 것입니다."]},{"l":"필요 Python 패키지 설치","p":["다음과 같이 실행하여 스크립트 실행에 필요한 서드 파티 Python 패키지들을 미리 설치합니다."]},{"l":"학습 모델 및 토크나이저 다운로드","p":["Hugging Face에 공개된 Llama2-13b-hf 모델 체크포인트를 사용하기 위해서는 커뮤니티 라이센스 동의와 Hugging Face 토큰 정보가 필요합니다.","먼저 다음 사이트에서 필요한 정보를 입력한 후 라이센스 동의를 진행합니다.","meta-llama/Llama-2-13b-hf · Hugging Face","동의서 제출 후 페이지의 상태가 다음과 같이 변경된 것을 확인합니다.","다음과 같은 명령어를 터미널에 입력하고 안내에 따라 Hugging Face 사용자 토큰을 입력합니다."]}],[{"l":"2. Moreh의 학습 코드 톺아보기","p":["학습 데이터를 모두 준비하셨다면 다음으로는 실제 fine-tuning 과정을 실행할 train_llama2.py 스크립트의 내용에 대해 살펴보겠습니다. 이 스크립트는 통상적인 PyTorch 코드로서 Hugging Face Transformers 라이브러리에 있는 Llama2 13B 모델 구현을 기반으로 fine-tuning 작업을 실행합니다.","우선 제공된 스크립트를 그대로 사용하여 튜토리얼을 끝까지 진행해 보시기를 권장합니다. 이후 스크립트를 원하는 대로 수정하셔서 Llama2 13B 모델을 다른 방식으로 fine-tuning 하는 것도 얼마든지 가능합니다. MoAI Platform은 PyTorch와의 완전한 호환성을 제공하기 때문입니다. 필요하시다면 Moreh에서 제공하는 MoAI Platform 응용 가이드 LLM Fine-tuning 파라미터 가이드 를 참고하십시오."]},{"l":"Training Code","p":["모든 코드는 일반적인 pytorch 사용 경험과 완벽하게 동일합니다.","먼저, transformers 라이브러리에서 필요한 모듈을 불러옵니다.","HuggingFace에 공개된 모델 config와 체크포인트를 불러옵니다.","Hugging Face에 공개된 학습 데이터셋 을 불러와 전처리하고, 데이터 로더를 정의합니다.","이후 학습도 일반적인 Pytorch를 사용하여 모델 학습과 동일하게 진행됩니다.","위와 같이 MoAI Platform에서는 기존에 사용하시던 PyTorch 스크립트를 수정 없이 동일하게 사용하실 수 있습니다."]},{"l":"About Advanced Parallelism","p":["이 튜토리얼에서 사용되는 학습 스크립트에는 MoAI Platform에서 제공하는 자동 병렬화 기능을 수행하는 코드가 있습니다.","거대 언어 모델인 Llama2 13B를 학습하는 경우 많은 양의 GPU를 필요로 합니다. 따라서 MoAI Platform을 사용하지 않는 경우에는 데이터 병렬 처리(Data Parallelism), 파이프라인 병렬 처리(Pipeline Parallelism), 텐서 병렬 처리(Tensor Parallelism)과 같은 병렬화 기법을 도입하여 학습을 수행해야 합니다.","예를 들어, 사용자가 일반적인 pytorch 코드에서 DDP를 적용하고 싶다면, 다음과 같은 코드 스니펫이 추가되어야 합니다. https://pytorch.org/tutorials/intermediate/ddp_tutorial.html","DDP는 비교적 쉽게 적용할 수 있지만, 파이프라인 병렬 처리 나 텐서 병렬 처리 를 적용하려면 상당히 복잡한 코드 수정이 필요합니다. 최적화된 병렬화 처리를 적용하려면 학습 스크립트 작성 과정에서 Python 코드가 다중 처리 환경에서 어떻게 동작하는지 이해해야 하며, 특히 다중 노드 설정에서는 학습에 사용되는 각 노드의 환경을 구성해야 합니다. 또한, 모델 종류, 크기, 데이터셋 등을 고려해 최적의 병렬화 방법을 찾기 위해서는 상당히 많은 시간이 필요합니다.","반면, MoAI Platform의 AP 기능을 통해 사용자는 별도의 병렬화 기법을 적용할 필요 없이, 학습 스크립트에 단 한 줄의 코드를 추가하는 것으로도 최적화된 병렬화 학습을 진행할 수 있습니다.","다른 프레임워크에서는 경험할 수 없는 MoAI Platform만의 Advanced Parallelization(AP) 기능을 통해 최적의 자동화된 분산 병렬처리를 경험해보세요.","AP기능을 이용하면 대규모 모델 훈련시 일반적으로 필요한 Pipeline Parallelism, Tensor Parallelism의 최적 매개변수와 환경변수를 아주 간단한 코드 한 줄로 설정할 수 있습니다."]}],[{"l":"3. 학습 실행하기","p":["이제 실제로 fine tuning을 실행해 보겠습니다."]},{"l":"가속기 Flavor 설정","p":["AMD MI210 GPU 32개 사용","AMD MI250 GPU 16개 사용","AMD MI300X GPU 8개 사용","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","MoAI Platform에서는 사용자에게 물리 GPU가 노출되지 않습니다. 대신에, PyTorch에서 사용할 수 있는 가상의 MoAI Accelerator를 MoAI Accelerator가 제공됩니다. 가속기의 Flavor를 설정함으로써 실제 물리 GPU를 얼마나 활용할지를 결정할 수 있습니다. 선택한 가속기 Flavor에 따라 총 학습 시간과 GPU 사용 비용이 달라지므로 사용자는 학습 상황을 고려하여 결정해야 합니다. 사용자의 학습 목표에 맞는 가속기 Flavor를 선택하기 위해 LLM Fine-tuning 파라미터 가이드 를 참고하세요.","moreh-switch-model 툴을 사용하여 현재 시스템에서 사용 가능한 가속기 flavor 리스트를 확인할 수 있습니다. 원활한 모델 학습을 위해 moreh-switch-model 명령어를 이용해 더 큰 메모리의 MoAI Accelerator로 변경할 수 있습니다.","Moreh의 체험판 컨테이너 사용 시: 선택","q 를 입력해 변경을 완료합니다.","따라서 처음 설정되어 있던 flavor를 로 전환한 다음 moreh-smi 명령을 사용하여 정상적으로 반영되었는지 확인하겠습니다.","로 잘 변경된 것을 확인할 수 있습니다.","먼저 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","변경 사항이 잘 반영되었는지 확인하기 위해 다시 moreh-smi 명령어를 이용해 현재 사용중인 MoAI Accelerator를 확인합니다.","사용을 위해 8을 입력합니다.","앞서 Llama2 13B Fine-tuning - 시작하기 전에 에서 MoAI Accelerator를 확인했던 것을 기억하시나요? 이제 본격적인 학습 실행을 위해 필요한 가속기를 설정해보겠습니다.","여기서 번호를 입력하여 다른 flavor로 전환할 수 있습니다.","이번 튜토리얼에서는 2048GB 크기의 MoAI Accelerator를 이용하겠습니다.","튜토리얼을 계속 진행하기 위해 인프라 제공자에게 각 flavor에 대응되는 GPU 종류 및 개수를 문의하십시오. 다음 중 하나에 해당하는 flavor를 선택하여 계속 진행하십시오.","현재 사용중인 MoAI Accelerator의 메모리 크기는 512GB입니다."]},{"l":"학습 실행","p":["주어진 train_llama2.py 스크립트를 실행합니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력될 것입니다. 이 로그를 통해 Advanced Parallelism 기능이 올바르게 동작하며 최적의 병렬화 설정이 적용되었음을 확인할 수 있습니다. 이전에 살펴본 PyTorch 스크립트 상에서는 AP 코드 한 줄을 제외한 부분에서 GPU 여러 개를 동시에 사용하기 위한 처리가 전혀 없었음을 참고하십시오.","훈련 로그를 확인해 보면 학습이 정상적으로 이루어지는 것을 확인할 수 있습니다.","학습 도중에 출력되는 throughput은 해당 PyTorch 스크립트를 통해 초당 몇 개의 token을 학습하고 있는지를 의미합니다.","AMD MI250 GPU 16개 사용 시: 약 150,000 tokens/sec","GPU 종류 및 개수에 따른 대략적인 학습 소요 시간은 다음과 같습니다.","AMD MI250 GPU 16개 사용 시: 약 4시간"]},{"l":"학습 중에 가속기 상태 확인","p":["학습 도중에 터미널을 하나 더 띄워서 컨테이너에 접속한 후 moreh-smi 명령을 실행하시면 다음과 같이 MoAI Accelerator의 메모리를 점유하며 학습 스크립트가 실행되는 것을 확인하실 수 있습니다. 실행 로그상에서 초기화 과정이 끝나고 Loss가 출력되는 도중에 확인해 보시기 바랍니다."]}],[{"l":"4. 학습 결과 확인하기","p":["앞 장과 같이 train_llama2.py 스크립트를 실행하면 결과 모델이 llama2_summarization 디렉토리에 저장됩니다. 이는 순수한 PyTorch 모델 파라미터 파일로 MoAI Platform이 아닌 일반 GPU 서버에서도 완벽하게 호환됩니다.","미리 다운로드한 GitHub 레포지토리의 tutorial 디렉토리 아래에 있는 inference_llama2.py 스크립트로 학습된 모델을 테스트해 볼 수 있습니다.","테스트에는 영국 프리미어 리그(EPL) 경기 결과와 관련된 기사 내용이 사용되었습니다.","코드를 실행합니다.","출력값을 확인해보면 Llama2가 프롬프트의 내용을 적절하게 요약한 것을 확인할 수 있습니다."]}],[{"l":"5. GPU 개수 변경하기","p":["앞과 동일한 fine-tuning 작업을 GPU 개수를 바꾸어 다시 실행해 보겠습니다. MoAI Platform은 GPU 자원을 단일 가속기로 추상화하여 제공하며 자동으로 병렬 처리를 수행합니다. 따라서 GPU 개수를 변경하더라도 PyTorch 스크립트를 수정할 필요가 전혀 없습니다."]},{"l":"가속기 Flavor 변경","p":["moreh-switch-model 툴을 사용하여 가속기 flavor를 전환합니다. 가속기 변경 방법은 3. 학습 실행하기 문서를 한번 더 참고해주시기 바랍니다.","AMD MI250 GPU 32개 사용","Moreh의 체험판 컨테이너 사용 시: 선택","KT Cloud의 Hyperscale AI Computing 사용 시: 선택","AMD MI210 GPU 64개 사용","AMD MI300X GPU 16개 사용"]},{"l":"학습 실행","p":["다시 train_llama2.py 스크립트를 실행합니다.","사용 가능한 GPU 메모리가 2배 늘었기 때문에, 배치 사이즈 또한 기존 256 에서 512 로 변경하여 실행시켜 보겠습니다.","학습이 정상적으로 진행된다면 다음과 같은 로그가 출력될 것입니다.","앞서 GPU 개수가 절반이었을 때 실행한 결과와 비교해 동일하게 학습이 이루어지며 throughput이 향상되었음을 확인할 수 있습니다.","AMD MI250 GPU 16 → 32개 사용 시: 약 150,000 tokens/sec → 315,000 tokens/sec"]}],[{"l":"6. 마무리","p":["지금까지 MoAI 플랫폼에서 Llama2 13B 로 텍스트 요약 작업을 할 때 fine-tuning 하는 과정을 살펴보았습니다. Llama와 같은 오픈 소스 대형 언어 모델(LLM)은 요약, 질문 답변 등 다양한 자연어 처리 작업에 활용될 수 있습니다. MoAI 플랫폼을 사용한다면 여러분이 필요한 GPU 수를 코드 변경 없이 손쉽게 설정할 수 있습니다. LLaMA 2와 같은 대형 언어 모델, 미세 조정 기술, MoAI 플랫폼의 가용성 덕분에 누구나 강력한 AI 애플리케이션을 개발할 수 있게 되었습니다. 따라서 이 튜토리얼에서 수행해 본 과정을 바탕으로 여러분만의 데이터로 새로운 모델을 빠르고 쉽게 개발해 보세요.","이 튜토리얼이나 MoAI Platform에 관련하여 궁금한 점이나 문의 사항이 있는 경우 Moreh( support@moreh.io)에 문의하시기 바랍니다."]},{"l":"더 알아보기","p":["MoAI Platform의 자동병렬화 기능, Advanced Parallelization (AP)","Llama3 8B Fine-tuning","Llama3 70B Fine-tuning","Mistral Fine-tuning","GPT Fine-tuning","Qwen Fine-tuning","Baichuan2 Fine-tuning"]}],[{"l":"Supported Documents","p":["Moreh Toolkit 가이드","MoAI Platform에서 Fine-tuning 준비하기","Advanced Parallelization (AP)","LLM Fine-tuning 파라미터 가이드","기술 지원 관련 및 일반적인 궁금한 사항이 있으시면 언제든지 support@moreh.io 로 문의해 주세요."]}],[{"l":"Moreh Toolkit 가이드","p":["Moreh Toolkit은 MoAI Platform 상에서 MoAI Accelerator 를 관리하거나 모니터링할 때 유용한 command line 도구입니다. 이 도구는 사용자에게 세 가지 명령어 ( moreh-smi, moreh-switch-model, update-moreh)를 제공하여 MoAI Accelerator를 효율적으로 관리하고, 설치된 MoAI Platform을 손쉽게 업데이트할 수 있도록 합니다."]},{"l":"주요 기능","p":["Moreh Toolkit의 주요 기능은 다음과 같습니다:","MoAI Accelerator의 모니터링:","moreh-smi 명령어를 사용하여 MoAI Accelerator에 할당된 메모리 사용량 및 프로세스 현황을 실시간으로 확인할 수 있습니다.","MoAI Accelerator 변경:","moreh-switch-model 명령어를 사용하여 MoAI Accelerator를 변경하고 최적의 성능을 얻기 위한 프로세스를 실행할 수 있습니다.","MoAI Platform 버전 설치 및 업데이트:","update-moreh 명령어를 통해 MoAI Platform을 최신 버전으로 설치 및 업데이트하거나 필요시 이전 버전으로 롤백할 수 있습니다."]},{"i":"moai-accelerator-모니터링-moreh-smi","l":"MoAI Accelerator 모니터링: moreh-smi","p":["moreh-smi 는 사용자가 MoAI Accelerator를 관리하고 모니터링할 수 있는 명령어 입니다. MoAI Platform Pytorch가 설치된 conda 환경에서 다음과 같이 실행할 수 있습니다.","현재 MoAI Accelerator를 사용하여 학습을 진행하고 있다면, 다른 터미널 세션을 활용하여 moreh-smi 을 실행할 경우 실행중인 프로세스 정보를 다음과 같은 화면을 보실 수 있습니다. 또한 moreh-smi 을 활용하면 현재 본인의 Job ID 를 확인할 수 있으므로, MoAI Platform 에서 학습 또는 추론에 문제가 생길 경우 해당 Job ID 와 함께 고객지원을 문의하면 더 빠르게 응답을 받으실 수 있습니다.","아래 예시에서 확인할 수 있는 Job ID는 976356입니다."]},{"l":"MoAI Accelerator 의 Multi Accelerator 기능 활용하기","p":["유저가 별도의 세팅을 하지 않을 경우에는 기본적으로 하나의 VM 또는 컨테이너 환경에 하나의 MoAI Accelerator만 존재할 것입니다. 기본적으로 MoAI Accelerator 한 개로는 하나의 프로세스만 실행할 수 있기 때문에, 기본 세팅으로는 하나의 환경에서 하나의 프로세스 실행만 가능합니다.","하지만 경우에 따라서는 하나의 환경에서도 여러 개 MoAI Accelerator를 활용하여 동시에 여러 개의 프로세스로 학습을 실행하고 싶은 경우도 있을 겁니다. (예: 동일한 소스코드이나 하이퍼 파라미터를 변경해서 여러 개의 학습 실험을 동시에 수행하고 싶은 경우) 이런 경우 moreh-smi 에서 하나의 토큰 내에 여러 개의 MoAI Accelerator를 생성하면, 동시에 여러개의 프로세스를 수행할 수 있습니다. 다음 예제를 통해 MoAI Accelerator를 추가, 변경 삭제해보겠습니다."]},{"i":"moai-accelerator-추가하기-moreh-smi-device---add","l":"MoAI Accelerator 추가하기 moreh-smi device --add","p":["먼저 MoAI Accelerator를 추가해보겠습니다. 2개 이상의 MoAI Accelerator를 사용하기 위해서 moreh-smi device --add 커멘드를 입력하면 아래와 같은 인터페이스가 나타납니다.","1~ 13 중 사용할 모델에 해당하는 정수를 입력하면 “Create device success.” 메시지와 함께 입력된 디바이스 번호에 해당하는 MoAI Accelerator가 생성됩니다. 하나의 환경 내에서는 최대 5개 AI가속기를 생성할 수 있습니다. 더 많은 MoAI Accelerator 를 생성하려면 인프라 관리자에게 문의하십시오.","아래 예제에서는 10번 8xLarge.4096GB MoAI Accelerator를 추가해 보겠습니다."]},{"i":"moai-accelerator-기본값-변경하기-moreh-smi-device---switch","l":"MoAI Accelerator 기본값 변경하기: moreh-smi device --switch","p":["moreh-smi device --switch {Device_ID} 는 기본값으로 설정된 MoAI Accelerator를 변경할 수 있는 명령어 입니다.","다음과 같이 사용할 수 있습니다 :","현재 기본값으로 설정된 MoAI Accelerator가 1번 가속기로 변경된 것을 확인할 수 있습니다."]},{"i":"moai-accelerator-삭제하기-moreh-smi-device---rm","l":"MoAI Accelerator 삭제하기: moreh-smi device --rm","p":["이번에는 생성된 디바이스를 moreh-smi device --rm {Device_ID} 커멘드로 특정 디바이스 ID에 해당하는 가속기를 삭제해보겠습니다.","다음과 같이 사용할 수 있습니다 :","위와 같은 커멘드를 입력해서 Device ID가 1인 MoAI Accelerator인 가 삭제되었습니다. 확인을 위해 다시 moreh-smi 를 실행하면 해당 디바이스가 삭제된 것을 확인할 수 있습니다."]},{"i":"그-외의-다양한-옵션-활용하기---help","l":"그 외의 다양한 옵션 활용하기 --help","p":["moreh-smi 는 이외에도 다양한 다양한 옵션을 제공합니다. 다음과 같이 --help 옵션을 활용하면 어떠한 옵션이 제공되는지 확인할 수 있습니다.","moreh-smi -p- MoAI Accelerator 상세 하드웨어 상태 모니터링하기","moreh-smi -t- MoAI Accelerator토큰 정보 확인하기","학습 도중에 프로세스가 꼬이거나 종료가 잘 안되어서 Process Running 과 같은 메시지가 출력된다면 moreh-smi --reset 명령어를 활용하세요."]},{"i":"moai-accelerator-변경하기-moreh-switch-model","l":"MoAI Accelerator 변경하기: moreh-switch-model","p":["moreh-switch-model 는 현재 설정된 MoAI Accelerator의 flavor(가속기 사양)를 변경할 수 있는 툴입니다. MoAI Accelerator의 flavor를 변경함으로써 GPU 메모리를 얼만큼 사용할 것인지 결정합니다.","다음과 같이 사용할 수 있습니다 :","예를 들어, moreh-smi 명령어의 결과가 다음과 같다면 이는 “현재 기본값으로 설정된 MoAI Accelerator는 0번 가속기이며 이 MoAI Accelerator의 유형은 모델”이라는 의미입니다.","moreh-switch-model 명령어를 사용하면 아래와 같은 입력창이 나타납니다.","1~ 13 중 사용할 모델에 해당하는 정수(디바이스 번호)를 입력하면 “ The MoAI Platform AI Accelerator model is successfully switched to {model_id}.” 메시지와 함께 입력된 디바이스 번호에 해당하는 MoAI Accelerator로 변경됩니다.","지금은 3번 로 MoAI Accelerator를 변경해보겠습니다.","변경을 계속하거나 q 또는 Q 를 통해 MoAI Accelerator 변경을 종료할 수 있습니다.","변경이 완료된 후 다시 moreh-smi 를 사용하여 확인한 결과는 다음과 같습니다.","0번 모델 유형의 MoAI Accelerator가 모델 유형으로 변경된 것을 확인할 수 있습니다."]},{"i":"moai-platform-업데이트-하기-update-moreh","l":"MoAI Platform 업데이트 하기: update-moreh","p":["update-moreh 는 conda 환경을 새롭게 생성하고 그 위에 MoAI Platform을 설치하거나, 이미 conda 환경에 설치된 MoAI Platform의 버전을 업데이트할 수 있는 명령어입니다. 다음과 같은 상황에서 update-moreh 를 사용할 수 있습니다.","새롭게 conda 환경을 생성한 경우 아직 MoAI Platform에 필요한 Python 패키지 설치가 필요합니다. 이 경우에는 update-moreh 명령어를 통해서 최신 버전의 MoAI Platform을 간단하게 설치할 수 있습니다.","이미 MoAI Platform이 설치된 conda 환경 내에서도 최신 버전의 MoAI Platform을 사용하고자 할 때, update-moreh 명령어를 단독으로 사용하여 현재 사용 중인 MoAI Platform을 최신버전으로 업데이트 할 수 있습니다.","필요에 따라 특정 버전의 MoAI Platform을 설치해야 할 경우가 있습니다. 이 경우에는 --target 옵션을 사용하여 사용자가 설치하고 싶은 특정 버전을 지정할 수 있습니다.","conda 환경에서 다른 패키지간의 의존성 충돌이 발생하는 문제 등으로 인해 MoAI Platform이 정상적으로 동작하지 않는 경우, conda 환경을 재구성을 해야 할 수 있습니다. 이러한 경우에도 conda 환경 내의 MoAI Platform 복구를 위하여 update-moreh 를 사용할 수 있습니다. 후자의 경우 --force 옵션을 사용하여 환경 재구성이 가능합니다. (—-target 옵션과 같이 사용 가능)"]}],[{"l":"MoAI Platform에서 Fine-tuning 준비하기","p":["MoAI Platform은 다양한 GPU로 구성될 수 있지만, 동일한 인터페이스(CLI)를 통해 사용자에게 일관된 경험을 제공합니다. 모든 사용자가 같은 방식으로 시스템에 접근하여 플랫폼을 사용할 수 있기 때문에 보다 효율적이며 직관적입니다.","MoAI Platform 또한 일반적인 AI 학습 환경과 유사하게 Python 기반의 프로그래밍을 지원합니다. 이에 따라 본 문서에서는 AI 학습을 위한 표준 환경 구성으로서 conda 가상 환경의 설정과 사용 방법을 중심으로 설명합니다."]},{"l":"conda 환경 설정하기","p":["훈련을 시작하기 위해 먼저 conda 환경을 생성합니다.","my-env 에는 사용자가 사용할 환경 이름을 입력합니다.","conda 환경을 활성화합니다.","MoAI Platform은 다양한 PyTorch 버전을 제공하고 있으므로 사용자가 필요한 환경에 맞는 버전을 선택해 설치할 수 있습니다.","moreh-smi 명령어를 입력해 설치된 MoAI Platform의 버전과 사용중인 MoAI Accelerator 정보를 확인할 수 있습니다. 현재 사용중인 MoAI Accelerator는 입니다.","각 모델별로 MoAI Platform에서 권장하는 Fine-tuning 시 최적의 파라미터는 LLM Fine-tuning 파라미터 가이드 를 참고하시기 바랍니다.","moreh-smi, moreh-switch-model 를 비롯한 moreh toolkit의 구체적인 사용 방법에 대해서는 MoAI Platform의 toolkit 사용하기 를 참고하시기 바랍니다."]}],[{"i":"advanced-parallelizationap","l":"Advanced Parallelization(AP)"},{"i":"advanced-parallelizationap-이란","l":"Advanced Parallelization(AP) 이란?","p":["Advanced Parallelization( AP)은 MoAI Platform에서 제공하는 자동 모델 최적화 및 분산 병렬처리 기능입니다. 일반적으로 ML 엔지니어는 거대 모델 학습 시 모델 병렬화를 최적화하기 위해 수많은 시행착오를 겪습니다. 사용 중인 GPU의 메모리 크기를 고려해 여러 병렬화 기법을 직접 적용해 보고, 각 기법에서 활용할 수 있는 옵션 조합들에 대해 성능을 측정해 최적화된 환경 변수를 결정해야 합니다. 이는 숙련된 머신러닝 개발자가 작업하더라도 몇 주에서 몇 달까지 걸릴 수 있는 매우 고된 작업입니다.","MoAI Platform의 AP 기능을 이용하면, 단 한 줄의 코드 추가만으로 복잡한 병렬 처리 및 모델 최적화 작업을 자동으로 수행해 학습에 소요되는 시간과 노력을 획기적으로 줄일 수 있습니다.","학습 throughput: MoAI 플랫폼의 AP 기능 적용한 최적화 vs 숙련된 엔지니어의 한 달 간의 최적화"]},{"i":"병렬화가-반드시-필요한-이유는-무엇일까요","l":"병렬화가 반드시 필요한 이유는 무엇일까요?","p":["간단한 예시로 Llama2 13B 모델을 학습할 때 필요한 GPU 메모리를 계산해 보겠습니다.","Llama2 13B 모델의 파라미터 수는 약 130억 개 입니다. 모델을 로드하는 데 필요한 메모리 크기를 FP16 데이터 타입을 기준으로 계산해보면 약 25 GB입니다. 학습에 필요한 optimizer, gradients 등까지 포함하면 최소 100GB에서 150GB의 메모리가 필요합니다. 따라서 보통의 단일 GPU가 보유한 메모리 크기(80 - 128GB)만으로는 학습이 불가능합니다. 이 부분이 모델 학습에 GPU 병렬 처리가 필수적인 이유입니다.","예를 들어, FSDP(Fully Sharded Data Parallel)나 DeepSpeed를 사용할 때, 개발자는 다양한 병렬화 설정을 수동으로 조정해야 하는 어려움을 겪습니다. 이 때 다음과 같은 매개변수를 신중하게 조정해야 합니다:","파라미터 분할: FSDP는 모델 파라미터를 GPU 간에 어떻게 분할할지 지정해야 합니다. 설정이 잘못되면 최적의 성능을 발휘하지 못하거나 메모리 초과 오류가 발생할 수 있습니다.","옵티마이저 상태 분할: FSDP와 DeepSpeed 모두 효과적인 메모리 사용과 통신 오버헤드를 위해 옵티마이저 상태를 분할해야 하며, 이는 복잡한 설정을 필요로 합니다.","액티베이션 체크포인팅: 메모리를 절약하기 위해 액티베이션 체크포인팅을 활성화가 요구될 수도 있으며, 더 나아가 메모리 절약과 역전파 시 액티베이션을 재계산하는 추가 계산 오버헤드 간의 균형을 맞춰야 합니다.","반면에, MoAI Platform의 AP 기능을 사용하면 사용자는 복잡한 병렬화 작업에 들이던 시간과 노력을 절약하여 다른 중요한 작업에 집중할 수 있습니다. 단 한 줄의 코드만 추가만으로, 플랫폼이 병렬화 작업의 복잡한 부분을 자동으로 처리하여 최적의 성능을 보장합니다.","MoAI Platform의 AP 기능을 사용하면 사용자는 병렬화 설정을 구성하는 번거로운 작업 대신 모델학습이라는 목표에 집중할 수 있습니다.","따라서 사용자는 거대 모델 훈련시 필요한 Data Parallelism이나 Pipeline Parallelism과 같은 병렬화 기법의 최적 매개변수와 환경 변수 조합을 간단히 얻을 수 있습니다."]}],[{"l":"AP 기능 사용하기","p":["AP 기능은 노드 단위로 병렬화를 진행합니다. 따라서 AP를 사용하기 위해서는 멀티 노드 규모의 가속기 사용이 권장됩니다. AP 기능을 사용하기 전에 현재 사용하는 가속기 정보를 점검하시기 바랍니다."]},{"l":"AP 기능 적용 방법","p":["AP 기능은 다음과 같이 import torch 이후에 torch.moreh..option.enable_advanced_parallelization() 한 줄을 추가하여 적용할 수 있습니다."]},{"l":"사용 예시 살펴보기","p":["사용자가 2대 이상의 노드를 사용하는 환경이 준비 되었다면 이제 AP 기능을 사용하기 위한 학습 코드를 만들어 보겠습니다. 이 가이드에서는 Llama2 모델을 활용하여 코드를 세팅합니다. 참고로, Llama2 모델은 커뮤니티 라이센스 동의와 Hugging Face 토큰 정보가 필요합니다. Llama2 1. Fine-tuning 준비하기 를 참고하여 학습 코드를 준비해주세요.","학습 코드가 준비되었다면, MoAI Platform에서 학습을 실행하기 전 아래와 같이 pytorch 환경을 설정합니다. 아래 예시의 경우 PyTorch 1.13.1+cu116 버전을 실행하는 MoAI Platform의 24.2.0 버전이 설치되어 있음을 의미합니다.","Pytorch 환경 설정이 되었다면, Github 레포지토리에서 학습을 위한 코드를 가져옵니다.","quickstart 레포지토리를 클론하여 quickstart/ap-example 디렉토리를 확인해보시면 Moreh에서 미리 준비한 AP기능 test를 위한 text_summarization_for_ap.py 를 확인하실 수 있습니다. 이 코드를 기반으로 AP 기능을 적용해봅시다.","테스트를 위한 학습 구성은 다음과 같습니다. 이를 토대로 테스트를 진행하겠습니다.","Batch Size: 64","Sequence Length: 1024","MoAI Accelerator: 4xLarge"]},{"l":"AP 기능 ON","p":["프로그램의 main 함수 시작 지점에 AP 기능을 켜는 line이 있습니다. 다음과 같이 AP를 적용한 후 학습을 실행합니다.","학습이 시작되면 다음과 같은 로그를 확인할 수 있습니다.","이처럼 단 한 줄의 AP 기능 프로그램을 추가하여 복잡한 분산 병렬처리가 수행되어 학습이 진행된 것을 확인할 수 있습니다. 다음은 사용자가 AP 기능을 사용하지 않을 경우 경험하게 될 상황을 가정하여 설명하겠습니다."]},{"l":"AP 기능 OFF","p":["AP 기능을 사용하지 않았을 때의 상황을 살펴보겠습니다. 이를 확인하기 위해, Python 프로그램의 main 함수 시작 지점에서 AP 기능을 켜는 줄을 주석 처리하여 AP 기능을 끄겠습니다.","그 다음 학습을 진행합니다.","학습이 종료되면 다음과 같은 로그를 확인할 수 있습니다.","위 로그에서 RuntimeError: Error Code 4: OUT_OF_MEMORY 라는 메시지를 볼 수 있습니다. 이것이 바로 앞서 말씀드린 1 device chip의 VRAM인 64GB를 초과하는 데이터를 로드할고 할 때 발생하는 OOM(Out Of Memory) 에러입니다.","MoAI Platform 이 아닌 다른 프레임워크를 사용한다면 현재 사용 중인 GPU의 메모리 크기를 고려해 여러 병렬화 기법을 직접 적용하고 메모리 초과 이슈를 해결하기 위해 많은 시간과 노력을 들여 병렬화 및 최적화 작업을 직접 진행해야 합니다. 그러나 MoAI Platform을 사용하는 사용자는 AP 기능 한 줄만 추가하여 이러한 OOM 문제를 간단하게 해결할 수 있습니다."]}],[{"l":"AP 더 알아보기","p":["AP와 관련된 로그를 조금 더 자세히 들여다보겠습니다.","MoAI Platform은 최적화된 병렬 처리를 위해 다양한 최적화 후보 config들을 생성합니다. 다음 로그는 Compiler Config Generator가 병렬화를 위한 후보 config를 30개로 설정했다는 의미입니다.","그 다음 모든 후보군에 대해서 연산 그래프를 생성합니다.","위 로그를 통해 config 들을 compile 하는 데에 약 6.1초가 소요되었음을 알 수 있습니다.","여기서 다시 가능한 후보 config를 추정합니다.","이로써 총 7개의 가능한 config들이 있다는 것을 확인했습니다.","이제 graph simulator가 각 config에 대한 cost를 계산하고, 계산이 종료되면 최적의 config라고 판단한 1개의 config를 채택합니다.","위 로그는 최종 config 1개가 설정되기까지 cost를 계산하는데 약 0.8초가 소요되었음을 보여줍니다.","이 정보는 advanced_parallelization_selected_config.dump 라는 파일에 기록되며 파이썬 프로그램을 실행한 위치에 생성됩니다. 이제 advanced_parallelization_selected_config.dump 이 어떻게 생겼는지 확인해 봅시다.","이처럼 MoAI Platform에서는 단 한 줄의 프로그램 추가로 수 개의 병렬화 후보를 계산하여 최적의 병렬화 방법을 자동으로 선택할 수 있습니다."]}],[{"l":"LLM Fine-tuning 파라미터 가이드","p":["1,112,135 MiB","1,121,814 MiB","1,147,841 MiB","1,366,564 MiB","1,403,640 MiB","1,541,212 MiB","1,764,955 MiB","1,853,432 MiB","1,899,079 MiB","1,951,344 MiB","100m","1024","128","13,286 TPS","140m","144m","14m","150,406 TPS","15m","16","16m","17m","18,001 TPS","190,433 TPS","190,949 TPS","191,605 TPS","197,489 TPS","19m","2,089,476 MiB","2,845,656 MiB","2048","220m","24.5.0","249m","256","28m","2xlarge","3,460,240 MiB","30m","315,004 TPS","32","381,714 TPS","384,165 TPS","392,573 TPS","394,605 TPS","4xlarge","512","560m","6,841 TPS","62m","699,751 MiB","758,555 MiB","762652 MiB","78,274 TPS","78m","798,760 TPS","81m","866,656 MiB","867,021 MiB","8xlarge","93,291 TPS","95302 TPS","99,873 TPS","Advanced Parallelism is applied","Baichuan2 13B","batch size","Cerebras GPT 13B","Llama2 13B","Llama3 8B","Mistral 7B","MoAI Accelerator","MoAI Accelerator 에 명시된 명칭은 사용자가 이용하는 CSP에 따라 다를 수 있습니다.","MoAI Platform version","Model","Qwen1.5 7B","sequence length","throughput","Training Time","True","vram Usage","이 가이드는 MoAI Platform에서 제공하는 최적의 파라미터이며 사용자 학습시 참고 정보로만 사용해주시기 바랍니다."]}]];